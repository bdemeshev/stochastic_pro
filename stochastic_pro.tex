% arara: xelatex: {shell: yes}
% arara: biber
% arara: xelatex: {shell: yes}
% arara: xelatex: {shell: yes}



%!TeX cleanPatterns = $OUTDIR/$JOB!($OUTEXT|.synctex.gz|.tex|.pdf), /$OUTDIR/_minted-$JOB/
\documentclass[12pt, a4paper]{article}
\usepackage{libertine}

% utf8 is the preferred encoding

 % this magick is to solve problem that appeared after update of texlive 2018 to texlive 2020
 % https://tex.stackexchange.com/questions/511341/the-error-occurred-after-the-last-update
\makeatletter
\def\nobreak{\penalty\@M}
\makeatother


\usepackage{fontspec} % что-то про шрифты? % нужно ли загружать?

\usepackage{polyglossia} % русификация xelatex
\usepackage{csquotes}


\setmainlanguage{english}
\setotherlanguage{russian}

% download "Linux Libertine" fonts:
% http://www.linuxlibertine.org/index.php?id=91&L=1
%\setmainfont{Linux Libertine O} % or Helvetica, Arial, Cambria
% why do we need \newfontfamily:
% http://tex.stackexchange.com/questions/91507/
%\newfontfamily{\cyrillicfonttt}{Linux Libertine O}

\newfontfamily\arabicfont[Script=Arabic]{Scheherazade New}


\usepackage{etoolbox} % provides \AtEndPreamble
% etoolbox causes wrong behavior of tocbasic
\AtEndPreamble{ % ради арабского написания Абу ибн-Сина
  \usepackage{arabxetex} 
 \let\textarabic\relax 
 \let\Arabic\relax 
\setotherlanguages{arabic, english}
}
% комбо из:
% https://tex.stackexchange.com/questions/501897
% https://tex.stackexchange.com/questions/392175/

\usepackage{imakeidx} 
\indexsetup{level=\section}
\makeindex[title=Hashtags]

% \usepackage{etex} % расширение классического tex
% в частности позволяет подгружать гораздо больше пакетов, чем мы и займёмся далее

\usepackage{verbatim} % для многострочных комментариев
\usepackage{makeidx} % для создания предметных указателей

\usepackage{setspace}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathrsfs} % sudo yum install texlive-rsfs
\usepackage{dsfont} % sudo yum install texlive-doublestroke
\usepackage{array, multicol, multirow, bigstrut} % sudo yum install texlive-multirow
\usepackage{indentfirst} % установка отступа в первом абзаце главы


\usepackage{bm}
\usepackage{bbm} % шрифт с двойными буквами
%\usepackage[perpage]{footmisc}

\usepackage{dcolumn} % центрирование по разделителю для apsrtable

% создание гиперссылок в pdf
\usepackage[unicode, colorlinks=true, urlcolor=blue, hyperindex, breaklinks]{hyperref}


\usepackage{microtype} % свешиваем пунктуацию
% теперь знаки пунктуации могут вылезать за правую границу текста, при этом текст выглядит ровнее


\usepackage{textcomp}  % Чтобы в формулах можно было русские буквы писать через \text{}

% размер листа бумаги
%\usepackage[paperwidth=145mm,paperheight=215mm,
%height=182mm,width=113mm,top=20mm,includefoot]%{geometry}
\usepackage[paper=a4paper, top=15mm, bottom=13.5mm, left=16.5mm, right=13.5mm, includefoot]{geometry}

\usepackage{xcolor}

\usepackage{framed} %  \leftbar


% \usepackage{float, longtable}
% \usepackage{soulutf8}

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке

\usepackage{mathtools}
\usepackage{cancel, xspace} % sudo yum install texlive-cancel


\usepackage{numprint} % sudo yum install texlive-numprint
\npthousandsep{,}\npthousandthpartsep{}\npdecimalsign{.}


% \usepackage{subfigure} % для создания нескольких рисунков внутри одного

\usepackage{tikz, pgfplots} % язык для рисования графики из latex'a
\pgfplotsset{compat=1.16}
\usetikzlibrary{trees} % tikz-прибамбас для рисовки деревьев
\usepackage{tikz-qtree} % альтернативный tikz-прибамбас для рисовки деревьев
\usetikzlibrary{arrows} % tikz-прибамбас для рисовки стрелочек подлиннее

\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет Последний день Помпеи}
% \listoftodos --- печатает все поставленные \todo'шки



\usepackage{booktabs} %  красивые таблицы
% заповеди из докупентации:
% 1. Не используйте вертикальные линни
% 2. Не используйте двойные линии
% 3. Единицы измерения - в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"

\usepackage{physics}
% \usepackage{minted} % moved to listings to simplify development
\usepackage{listings}
\lstset{%
basicstyle=\fontfamily{lmtt}\bfseries,
keywordstyle=\fontfamily{lmtt}\bfseries
}
% \usepackage{julia-mono-listings}
% TODO: установить?
\usepackage{answers}




\usepackage[bibencoding=auto, backend=biber, sorting=none, style=alphabetic]{biblatex}

\addbibresource{stochastic_pro.bib}

\setcounter{tocdepth}{1} % в оглавление оставляем уровень 1

\usepackage[titles]{tocloft} % альтернатива tocbasic для настройки toc
% если нужен subfigure, то у tocloft можно добавить опцию subfigure
\renewcommand{\cftbeforesecskip}{0.7pt} % поправка интервала между строками для section в toc
\renewcommand{\cftsecdotsep}{\cftdotsep} % добавляем точечки

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
% \setlist[enumerate, 1]{label=\asbuk*),ref=\asbuk*} % цифра рядом с enumerate = уровень нумерации
\setlist[enumerate, 1]{label=\alph*),ref=\alph*} % цифра рядом с enumerate = уровень нумерации


%%%%%%%%%%%%%%%%%%%%%%%  ПАРАМЕТРЫ  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setstretch{1}                          % Межстрочный интервал
\flushbottom                            % Эта команда заставляет LaTeX чуть растягивать строки, чтобы получить идеально прямоугольную страницу
\righthyphenmin=2                       % Разрешение переноса двух и более символов
%\pagestyle{plain}                       % Нумерация страниц снизу по центру.
\widowpenalty=300                     % Небольшое наказание за вдовствующую строку (одна строка абзаца на этой странице, остальное --- на следующей)
\clubpenalty=3000                     % Приличное наказание за сиротствующую строку (омерзительно висящая одинокая строка в начале страницы)
\setlength{\parindent}{1.5em}           % Красная строка.
%\captiondelim{. }
\setlength{\topsep}{0pt}
\emergencystretch=2em

% делаем короче интервал в списках
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}



\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\sgn}{sign}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\amn}{arg\,min}
\DeclareMathOperator*{\amx}{arg\,max}


\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\sCorr}{sCorr}
\DeclareMathOperator{\sCov}{sCov}
\DeclareMathOperator{\sVar}{sVar}

\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\Med}{Med}


\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\e}{\varepsilon}


\newcommand{\cF}{\mathcal{F}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}

\newcommand{\cN}{\mathcal{N}}
\newcommand{\dNorm}{\mathcal{N}}
\newcommand{\dN}{\mathcal{N}}
\newcommand{\dLN}{\mathcal{LN}}

\newcommand{\dBern}{\mathrm{Bern}}
\newcommand{\dPois}{\mathrm{Pois}}
\newcommand{\dBin}{\mathrm{Bin}}
\newcommand{\dMult}{\mathrm{Mult}}
\newcommand{\dGeom}{\mathrm{Geom}}
\newcommand{\dNHGeom}{\mathrm{NHGeom}}
\newcommand{\dHGeom}{\mathrm{HGeom}}
\newcommand{\dDUnif}{\mathrm{DUnif}}
\newcommand{\dFS}{\mathrm{FS}}
\newcommand{\dNBin}{\mathrm{NBin}}

\newcommand{\dTri}{\mathrm{Triangle}}
\newcommand{\dUnif}{\mathrm{Unif}}
\newcommand{\dCauchy}{\mathrm{Cauchy}}
\newcommand{\dExpo}{\mathrm{Expo}}
\newcommand{\dBeta}{\mathrm{Beta}}
\newcommand{\dGamma}{\mathrm{Gamma}}
\newcommand{\dWei}{\mathrm{Wei}}
\newcommand{\dLogistic}{\mathrm{Logistic}}
\newcommand{\dRayleigh}{\mathrm{Rayleigh}}
\newcommand{\dPareto}{\mathrm{Pareto}}


% вместо горизонтальной делаем косую черточку в нестрогих неравенствах
\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}


\newcommand{\wv}{\textrm{word2vec}}
\newcommand \hVar{\widehat{\Var}}
\newcommand \hCorr{\widehat{\Corr}}
\newcommand \hCov{\widehat{\Cov}}


\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}

\newcommand{\Lin}{\mathcal{L}in}
\newcommand{\Linp}{\Lin^{\perp}}

\title{Stochastic Processes problems}
\author{\url{https://github.com/bdemeshev/stochastic_pro}}
\date{\today}


%\newtheorem{problem}{Задача}
%\numberwithin{problem}{section}

\Newassociation{sol}{solution}{solution_file}
% sol --- имя окружения внутри задач
% solution --- имя окружения внутри solution_file
% solution_file --- имя файла в который будет идти запись решений
% можно изменить далее по ходу
\Opensolutionfile{solution_file}[all_solutions]
% в квадратных скобках фактическое имя файла



% магия для автоматических гиперссылок задача-решение
\newlist{myenum}{enumerate}{3}
% \newcounter{problem}[chapter] % нумерация задач внутри глав
\newcounter{problem}[section]

\newenvironment{problem}%
{%
\refstepcounter{problem}%
%  hyperlink to solution
     \hypertarget{problem:{\thesection.\theproblem}}{} % нумерация внутри глав
     % \hypertarget{problem:{\theproblem}}{}
     \Writetofile{solution_file}{\protect\hypertarget{soln:\thesection.\theproblem}{}}
     %\Writetofile{solution_file}{\protect\hypertarget{soln:\theproblem}{}}
     \begin{myenum}[label=\bfseries\protect\hyperlink{soln:\thesection.\theproblem}{\thesection.\theproblem},ref=\thesection.\theproblem]
     % \begin{myenum}[label=\bfseries\protect\hyperlink{soln:\theproblem}{\theproblem},ref=\theproblem]
     \item%
    }%
    {%
    \end{myenum}}
% для гиперссылок обратно надо переопределять окружение
% это происходит непосредственно перед подключением файла с решениями





\begin{document}

\maketitle % ставим сюда название, автора и время создания

% здесь нужна прикольная картинка

\newpage
\tableofcontents{}

\newpage


\section{First step analysis}

\begin{problem}
Biden and Trump alternately throw a fair dice infinite number of times. 
Biden throws first. 
The person who obtains the first $6$ wins the game. 

\begin{enumerate}
  \item What is the probability that Biden will win?
  \item What is expected number of turns?
  \item What is variance of the number of turns?
  \item What is expected number of turns given that Biden won?
  \item Find the transition matrix of this four state Markov chain. 
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item $\P(B) = 6/11$, first step equation for $p = \P(B)$ is
  $p = 1/6 + (5/6)^2 p$ or $p = 1/6 + 5/6 \cdot (1 - p)$.
  \item $\E(N) = 6$, first step equation for $m = \E(N)$ is $m = 1/6 + 5/6 (m + 1)$.
  \item $\E(N^2) = 66$, $\Var(N) = 30$, first step equation is $\E(N^2) = 1/6 + 5/6 \E((N + 1)^2)$.
  \item $\E(N \mid B) = 61/11$. Start by replacing uncoditional probabilities on the tree by conditional ones. 
  First step equation for $\mu = \E(N \mid B)$ is $\mu = 11/36 + 25/36 (\mu + 2)$.
  \item $\begin{pmatrix}
    0 & 5/6 & 1/6 & 0 \\
    5/6 & 0 & 0 & 1/6 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
  \end{pmatrix}$
\end{enumerate}

\end{sol}
\end{problem}



\begin{problem}
  Elon throws an unfair coin until ``head'' appears. 
  The probability of ``head'' is $p \in (0;1)$. 
  Let $N$ be the total number of throws. 
  \begin{enumerate}
    \item Find $\E(N)$, $\Var(N)$, $\E(N^3)$, $\E(\exp(tN))$.
    \item What is the probability than $N$ will be even?
  \end{enumerate}

  \begin{sol}
    \begin{enumerate}
      \item $\E(N) = 1/p$, $\Var(N) = $, $\E(N^3) = $, $\E(\exp(tN)) = $
      \item $a = \P(N \in 2 \cdot \NN)$, $a = (1-p)(1-a)$, $a = (1 - p) / (2 - p)$.
    \end{enumerate}
    
  \end{sol}
\end{problem}
  


\begin{problem}
  Alice and Bob throw a fair coin until the sequence $HTT$ or $THT$ appears.
  Alice wins if $HTT$ appears first, Bob wins if $THT$ appears first. 
  \begin{enumerate}
    \item Find the probability that Alice wins.
    \item Find the expected value and variance of the total number of throws. 
    \item Using any open source software find the probability that Alice wins for all possible combinations 
    of three coins sequences for Alice and Bob. 
    \item Now Alice and Bob play the following game. 
    Alice chooses her three coins winning sequence first. Next Bob, knowing the choice of Alice, chooses his three coins winning sequence. 
    Than they throw a fair coin until either of their sequences appears. 
    What is the best strategy for Alice? For Bob? What is the probability that Alice wins this game?
  \end{enumerate}
  
  \begin{sol}
  
  \end{sol}
\end{problem}

\begin{problem}
You throw a dice unbounded number of times. 
If it shows $1$, $2$ or $3$ then the corresponding amount of dollars 
is added in the pot. 
It it shows $4$ or $5$ the game stops and you get the pot with money. 
If it shows $6$ the game ends and you get nothing. 
Initially the pot is empty. 

\begin{enumerate}
  \item What is probability that the game will end by $6$?
  \item What is expected duration of the game?
  \item What is your expected payoff?
  \item What is your payoff variance?
  \item Consider variation-A of the game. 
  Rules are the same, but initially the pot contains $100$ dollars. 
  How will the answers to questions (a)-(d) change?
  \item Consider variation-B of the game. 
  Initially the pot is empty. One rule is changed. 
  If the dice shows $5$ the content of the pot is burned and the game continues. 
  How will the answers to questions (a)-(d) change?
\end{enumerate}

  \begin{sol}
  Let's denote the throws by $(X_t)$ and the number of throws by $T$.
  Thus the last throw is $X_T$. 
    \begin{enumerate}
      \item $\P(X_T = 6) = 1/3$ as we have three possible endings. 
      One may also sum the probability geometric serie or use first step analysis.
      \item $\E(T) = 0.5 + 0.5 (\E(T) + 1)$;
      \item Let $\mu = \E(S)$ and $\gamma = \P(X_T \in \{4, 5\})$.
      \[
      \mu = \frac{3}{6}\cdot 0 + \frac{1}{6}(\mu + 1\cdot \gamma) + \frac{1}{6}(\mu + 2\cdot \gamma) + \frac{1}{6}(\mu + 3\cdot \gamma)
      \]
      \item 
      \item 
      \item 
      \[
      \mu_B = \frac{2}{6}\cdot 0 + \frac{1}{6}\mu_B + \frac{1}{6}(\mu + 1\cdot \beta) + \frac{1}{6}(\mu + 2\cdot \beta) + \frac{1}{6}(\mu + 3\cdot \beta),
      \]
      with $\beta = 1/3$.
    \end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
Boris Johnson throws a fair coin until $1$ appears or until he says ``quit''.
His payoff is the value of the last throw. 
Boris optimizes his expected payoff. 
If many strategies gives the same expected payoff 
he chooses the strategy that minimizes the expected duration of the game. 

\begin{enumerate}
  \item What is the optimal strategy and the corresponding expected payoff?
  \item What is the expected duration?
  \item How the answers to points (a) and (b) will change
  if Boris should pay $0.3$ dollars for each throw?
  \end{enumerate}

  \begin{sol}
  
  \end{sol}
\end{problem}


\begin{problem}
  Winnie-the-Pooh starts wandering from the point $x=1$.
  Every minute he moves one unit left or one right with equal probabilities.

  Let $T$ be the random moment of time when he reaches $x=0$.

  \begin{enumerate}
   \item Find the generating function $g(u) = \E(u^T)$.
   \item Extract all probabilities $\P(T = k)$ from the function $g(u)$.
  \end{enumerate}

  \begin{sol}
  
  \end{sol}
\end{problem}


\begin{problem}
  
  \begin{sol}
  
  \end{sol}
\end{problem}

\begin{problem}
  
  \begin{sol}
  
  \end{sol}
\end{problem}


\section{Markov chains}

\begin{problem}
HSE student lives in two states: "sleep" and "study" and tries to change the state every 1 hour. 
After the sleep state the student continues sleeping with probability equal to 0.25, 
otherwise a student starts studying. 
If the student is studying, the probabilities to continue studying and to start sleeping are equal.

\begin{enumerate}
\item Write down the transition matrix of this Markov chain.
\item Draw the graph representation.
\item What is the probability that a Sleeping Student will be a Studying Student after 1 hour? After 2 hours?
\item We know that initially student is sleeping with probability $p=\frac{2}{3}$. 
Find the probabilities of sleep and study states after 1 and 2 hours. 
\item Find the probabilities of sleep and study states after 20 and 100 hours (do it with \textbf{matrix} operations and any soft). 
Is there any difference and why?  
\end{enumerate}

\begin{sol}
  
\end{sol}

\end{problem}

\begin{problem}
HSE student has three states: pre-coffee, with-coffee and over-coffee. 
He goes to Jeffrey's each break seeking for a cup of coffee. 
The line is usually too long, so probability to stay pre-coffee is equal to 60\% and to be over-coffee — is zero. 
Caffeinated students can stay in lines longer, so for with-coffee student the probability to become over-coffee is 20\% and to become pre-coffee — 30\%. 
Over-coffee student runs to coffeeshop very fast and able to stay over-coffeed with $p = 0.70$ 
and can suddenly become pre-coffee with $p = 0.10$. 

\begin{enumerate}
\item Draw the graph representation of this Markov chain.
\item Write the transition matrix of this Markov chain.
\item What is the probability that morning pre-coffee student will be with-coffee after 1 break? After 3 breaks?
\item What is the probability that morning pre-coffee student will be with-coffee after 1 break? After 3 breaks? After 200 breaks?
\end{enumerate}

\begin{sol}
  
\end{sol}
\end{problem}


\begin{problem}
Unteachable students in NOTHSE University try to pass the exams. 
Students cheat successfully and pass the exams with probability 10\%. 
In the case of a failure students are allowed to infinite number of retakes. 
All students are unteachable so the amount of knowledge is always the same and doesn't depend of the number of retakes.

\begin{enumerate}
\item Draw the graph representation of this Markov chain.
\item What is the probability to graduate using no more than 5 retakes? 
\item What is the probability to graduate eventually? 
\item Use \textbf{first step analysis} to find the average number of retakes per student in this University.
\end{enumerate}

\begin{sol}
  
\end{sol}

\end{problem}

\begin{problem}
Every month the real estate Galina agent has two options: to increase her commission and to ask an owner to increase the rent. 
If the agent has increased the commission, on the next step she increases the commission again with probability $5/8$. 
If she has asked the owner, she decides to increase the commission with probability equal to $3/4$ on the next step.

\begin{enumerate}
\item Write the transition matrix of this Markov chain.
\item Draw the graph representation.
\item Use \textbf{first step analysis} to find how many steps the agent does between asking the owner to increase the price.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
Alice and Bob toss a coin, writing down the results.
 If the last 3 tosses are Head, Head and Tail, Alice wins. 
 If the last 3 tosses are Tail, Head and Head, Bob wins.

\begin{enumerate}
\item Is it easy to work with matrix representation in this case?
\item Draw the graph representation. Who is more likely to win the game?
\item Use \textbf{first step analysis} to find the probability of Alice's win.
\item Find the probability that the game ends in exactly 4 tosses.
\item Find the expected value and variance of the total number of coin throws in the game.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
HSE student has an unusually caring granny who cooks one pie with probability $0.7$ every weekend. 
Granny's pies are so tasty that HSE student can't resist and he gains 1 kilo for each pie eaten. 
Without pies the student with more than 70 kilos weight looses 1 kilo per week, yeah, he has a lot of studies!
At the beginning of the study year student's weight is $W_0 = 70$ kilos.

Let $W_t$ be the weight of the student $t$ weeks later. 

\begin{enumerate}
\item Find the probability $\P(W_3 \geq 71)$ and expected value $\E(W_3)$.
\item Find the limit weight after infinitely many study weeks $\lim_{t\to\infty} W_t$.
\item Explain whether the chain $(W_t)$ has a stationary distribution.
\end{enumerate}

\begin{sol}
\begin{enumerate}
    \item 
    \item $\E(W_t) \to + \infty$;
    \item No stationary distribution. For stationary distribution $\E(W_t)$ can't tend to infinity. 

\end{enumerate}

\end{sol}
\end{problem}


\begin{problem}
The fair price of Sborbank in discrete stock market is somewhere between 100 and 101 rubles. 
If the price is equal to 100, then the price grows up by 1 ruble with probability $\frac{9}{10}$, 
otherwise it goes down by 1 ruble. 
If the price is greater then 100, it grows by 1 ruble with probability $\frac{1}{3}$ or declines by 1 ruble. 
If the price is lower than 100, it grows by 1 ruble with probability $\frac{2}{3}$ or declines by 1 ruble. 

\begin{enumerate}
\item Draw the graph representation of the corresponding Markov chain.
\item Do you think this chain has some stationary distribution?
\item Find the average time for the stock price to fall from 102 rubles to 98 rubles. 

Hint: you may to decompose the long path into smaller ones and to use the first step analysis.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}



\section{Classification of states}

\begin{problem}
We randomly wander on the graph choosing at each moment of time one of the possible directions.
If probabilities are not given we choose equiprobably.   

\begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
  \node [circle, draw] (A) {$a$};
  \node [circle, draw] (B) [right of=A] {$b$};
  \node [circle, draw] (C) [right of=B] {$c$};
  \node [circle, draw] (D) [below of=A] {$d$};
  \node [circle, draw] (E) [right of=D] {$e$};
  \node [circle, draw] (F) [right of=E] {$f$};

  \path (A) edge (D);
  \path (B) edge (A);
  \path (B) edge (C);
  \path (D) edge (B);
  \path (C) edge (F);
  \path (B) edge (F);
  \path (E) edge [bend left] (F);
  \path (F) edge [bend left] (E);



  % \path (one) edge [bend left] node [above] {$0.1$} (two);
  %\path (two) edge [bend left] node [below]{$0.2$} (one);
  %\path (two) edge [loop right] node {} (two);
  %\path (one) edge [loop left] node {} (one);
\end{tikzpicture}


\begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
  \node [circle, draw] (A) {$a$};
  \node [circle, draw] (B) [right of=A] {$b$};
  \node [circle, draw] (C) [below of=B] {$c$};
  \node [circle, draw] (D) [below of=A] {$d$};
  \node [circle, draw] (E) [right of=B] {$e$};

  \path (A) edge [bend left] (B);
  \path (B) edge [bend left] (A);
  \path (B) edge [bend left] (C);
  %\path (A) edge (C);
  \path (C) edge [bend left] (B);
  \path (C) edge [bend left] (D);
  \path (D) edge [bend left] (C);
  \path (D) edge [bend left] (A);
  \path (A) edge [bend left] (D);

  \path (B) edge (E);
  \path (E) edge [loop right] (E);
\end{tikzpicture}


\begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
  \node [circle, draw] (A) {$a$};
  \node [circle, draw] (B) [right of=A] {$b$};
  \node [circle, draw] (C) [right of=B] {$c$};
  \node [circle, draw] (D) [below of=A] {$d$};
  \node [circle, draw] (E) [right of=D] {$e$};
  \node [circle, draw] (F) [right of=E] {$f$};
  \node [circle, draw] (G) [right of=C] {$g$};


  \path (A) edge (D);
  \path (D) edge (E);
  \path (E) edge (A);
  \path (B) edge (E);
  \path (B) edge (C);
  \path (C) edge (F);
  \path (C) edge (G);
  \path (F) edge (B);


  \path (D) edge [loop left] (D);
  \path (E) edge [loop right] (E);
  \path (B) edge [loop left] (B);
  \path (G) edge [loop below] (G);
\end{tikzpicture}


\begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
  \node (A) {\dots};
  \node [circle, draw] (B) [right of=A] {$-1$};
  \node [circle, draw] (C) [right of=B] {$0$};
  \node [circle, draw] (D) [right of=C] {$1$};
  \node (E) [right of=D] {\dots};

  \path (A) edge [bend left] node [above] {$0.1$} (B);
  \path (B) edge [bend left] node [below] {$0.9$} (A);
  \path (B) edge [bend left] node [above] {$0.1$} (C);
  \path (C) edge [bend left] node [below] {$0.9$} (B);
  \path (C) edge [bend left] node [above] {$0.1$} (D);
  \path (D) edge [bend left] node [below] {$0.9$} (C);
  \path (D) edge [bend left] node [above] {$0.1$} (E);
  \path (E) edge [bend left] node [below] {$0.9$} (D);
\end{tikzpicture}



\begin{enumerate}
  \item Split each Markov chain into communicating classes. 
  \item Find the period of every state. 
  \item Classify each state as transient, null-recurrent and positive recurrent.
  \item For positive recurrent states find the expected return time.
  \item Find all stationary distributions. 
\end{enumerate}


  \begin{sol}
    
  \end{sol}
\end{problem}


\begin{problem}
  A Knight randomly wanders on the chessboard. 
  At each step he randomly chooses one of the possible Knight-moves with equal probabilities. 
  
\begin{enumerate}
    \item Find the stationary distribution. 
    \item Find the expected return time for every square.
    \item Find the period of every square. 
\end{enumerate}
  
\begin{sol}
      
\end{sol}
\end{problem}
  



\section{Generating functions}

\begin{problem}
The MGF (moment generating function) of the random variable $X$ is give by $M(t) = 0.3 \exp(2t) + 0.2 \exp(3t) + 0.5 \exp(7t)$.

Recover the distribution of the random variable $X$.
  \begin{sol}

  \end{sol}
\end{problem}



\begin{problem}
The random variable $Y$ takes values $1$, $2$ or $3$ with equal probabilities.

Find the MGF of the random variable $Y$.
  \begin{sol}
  $M(t) = (\exp(t) + \exp(2t) + \exp(3t))/3$.
  \end{sol}
\end{problem}


\begin{problem}
The MGF of the random variable $W$ has a Taylor expansion that starts with $M(t) = 1 + 2t + 7t^2 + 20t^3 + \ldots$.

Find $\E(W)$, $\Var(W)$, $\E(W^3)$.

  \begin{sol}
  $\E(W) = 2$, $\Var(W) = 7\cdot 2 - 2^2$, $\E(W^3) = 20 \cdot 3!$.
  \end{sol}
\end{problem}


\begin{problem}
The random variable $X$ takes non-negative integer values.
The generating function $g(u) = \E(u^X)$ has a Taylor expansion that
starts with $g(u) = 0.1 + 0.2 u + 0.15 u^2 + \ldots$.

Find probabilities $\P(X = 0)$, $\P(X = 1)$, $\P(X = 2)$.

  \begin{sol}
  $\P(X = 0) = 0.1$, $\P(X = 1) = 0.2$, $\P(X = 2) = 0.15$.
  \end{sol}
\end{problem}




\begin{problem}
  Random variables $X_i$ are mutually independent and $X_i$ has Gamma distribution $\dGamma(\alpha_i, \beta_i)$.

  I sum up the random number $N$ of terms,
  \[
   S = X_1 + X_2 + \ldots + X_N.
  \]
  The number $N$ has Poisson distribution $\dPois(\lambda)$ and is independent of the sequence $(X_i)$.

  \begin{enumerate}
   \item Find the MGF of $S$. You may the MGF formula for Gamma distribution as known.
   \item Find $\E(S)$ and $\Var(S)$.
  \end{enumerate}

  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  The random variable $X$ take non-negative integer values.
  Its moment generating function is equal to $M(t) = (2 - \exp(t))^{-7}$.

  \begin{enumerate}
   \item Find the probability generating function $g(u) = \E(u^X)$.
   \item Find $\E(X)$, $\Var(X)$, $\P(X = 0)$, $\P(X = 1)$, $\P(X = 2)$.
   \item Find $\P(X = k)$.
  \end{enumerate}


  \begin{sol}
      $g(u) = g(\exp(t)) = \E(\exp(tX)) = M(t)$
  \end{sol}
\end{problem}

\section{Limits}

\begin{problem}
Polina loves sweet chestnuts. 
She has infinite sequence of baskets before her.
In the basket number~$n$ there are $n$~chestnuts in total.
Unfortunately only one chestnut in every basket is a sweet one. 

She picks chestnuts one by one at random from all the buskets sequentially. 
First she picks the unique chestnut from the basket number one, 
than she picks in a random order two chestnuts from the basket number two and so on. 

The random variable $S_t$ indicates whether the chestnut number $t$ was a sweet one. 

\begin{enumerate}
  \item Find $\lim S_t$ or prove that the limit does not exist.
  \item Find $\plim S_t$ or prove that the limit does not exist.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Let $(X_n)$ be independent, each variable $X_n$ has exponential distribution with rate $\lambda_n = n$.
\begin{enumerate}
  \item Find the probability limit $\plim X_n$ or prove that it does not exist.
\end{enumerate}
Let $(Y_n)$ be independent, each variable $Y_n$ has exponential distribution with rate $\lambda_n = n / (n + 1)$.
\begin{enumerate}[resume]
  \item Find the probability limit $\plim Y_n$ or prove that it does not exist.
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item $\plim X_n = 0$;
  \item $\plim Y_n$ does not exist.
\end{enumerate}

\end{sol}
\end{problem}


\begin{problem}
Let $(X_n)$ be independent normally distributed $\cN(5; 10)$.

\begin{enumerate}
  \item Find the probability limit 
  \[
  \plim \frac{X_1 + X_2 + \dots + X_n}{7n}.
  \]
  \item Find the probability limit 
  \[
  \plim \frac{X_1^2 + X_2^2 + \dots + X_n^2}{7n}.
  \]
  \item Find the probability limit 
  \[
  \plim \ln(X_1^2 + X_2^2 + \dots + X_n^2) - \ln n.
  \]
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item $\plim \frac{X_1 + X_2 + \dots + X_n}{7n} = 5/7$;
  \item $\plim \frac{X_1^2 + X_2^2 + \dots + X_n^2}{7n} = 5$;
  \item $\plim \ln(X_1^2 + X_2^2 + \dots + X_n^2) - \ln n = \ln 35$.
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
  Let $(X_n)$ be independent uniform on $[0; 1]$.
  Let $Y_n = X_n^2 + X_n^3$.

  
  \begin{enumerate}
    \item Find the probability limit $\plim V_n$ for 
    \[
    V_n = \max\{Y_1, Y_2, \dots, Y_n\}.
    \]
    \item Find the probability limit $\plim W_n$ for 
    \[
    W_n = \max\{X_1 + Y_1, X_1 + Y_2, \dots, X_1 + Y_n\}.
    \]
  \end{enumerate}
  \begin{sol}
  \begin{enumerate}
    \item $\plim \max\{Y_1, Y_2, \dots, Y_n\} = 2$;
    \item $\plim \max\{X_1 + Y_1, X_1 + Y_2, \dots, X_1 + Y_n\} = X_1 + 2$.
  \end{enumerate}
  \end{sol}
  \end{problem}

\begin{problem}
Consider the random variable $X$ and the sequence of random variables $Y_n$ with $\E(Y_n) = \frac{1}{n}$ 
and $\Var(Y_n) = \frac{\sigma^2}{n}$. 
Let $W_n = X + Y_n$.

\begin{enumerate}
        \item Find the probability limit $\plim Y_n$;
        \item Find the probability limit $\plim W_n$.
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\plim Y_n = 0$;
    \item $\plim W_n = X$.
  \end{enumerate}
  \end{sol}
\end{problem}
  

\begin{problem}
  The random variables $X_i$ are independent and uniformly distributed on $[0; 1]$.  
  Let $Y_n = \min{X_1, \ldots X_n}$. 

  \begin{enumerate}
        \item Find the almost sure limit of $Y_n$;
        \item Find the probability limit of $Y_n$;
        \item Find the limiting distribution of $Y_n$.
  \end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\P(\lim Y_n = 0 )=1$;
    \item $\plim Y_n = 0$;
    \item Limiting distribution is a constant $0$.
\end{enumerate}
\end{sol}

\end{problem}


\begin{problem}
Let $X$ and $Y$ be independent and uniformly distributed on $[0; 1]$.
Let $V_n = n^2 Y \cdot I(X \leq 1/n)$ and $W_n = Y \cdot I(X > 1/n)$. 
  \begin{enumerate}
        \item Find $\plim V_n$ and $\plim W_n$.
        \item Does $(V_n)$ converge in mean squared?
        \item Does $(W_n)$ converge in mean squared?
  \end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\plim V_n = 0$, $\plim W_n = Y$;
    \item The sequence $V_n$ does not converge in mean squared;
    \item  $W_n$ converges to $Y$ in mean squared.
\end{enumerate}
\end{sol}

\end{problem}


\section{Conditional expected value without sigma-algebras}

\begin{problem}
We randomly uniformly select a point inside triangle $A = (6, 0)$, $B = (0, 2)$ and $O = (0, 0)$.
Let $(X, Y)$ be coordinates of this random point.

\begin{enumerate}
  \item Find conditional expected values $\E(Y \mid X)$ and $\E(X \mid Y)$.
  \item Find conditional variances $\Var(Y \mid X)$ and $\Var(X \mid Y)$.
\end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item $\E(Y \mid X) = 1 - X/6$ and $\E(X \mid Y) = 3 - 1.5 X$.
      \item $\Var(Y \mid X) = $, $\Var(X \mid Y) = $.
    \end{enumerate}        
  \end{sol}
\end{problem}


\begin{problem}
The pair of random variables $X$ and $Y$ has joint probability density
\[
f(x, y) = \begin{cases}
  x + y, \text{ if } x\in [0;1], y\in [0;1]; \\
  0, \text{ otherwise.}
\end{cases}
\]  

\begin{enumerate}
  \item Find the marginal densities $f_X(x)$ and $f_Y(y)$.
  \item Find the conditional densities $f(x \mid y)$ and $f(y \mid x)$.
  \item Find the conditional expected values $\E(Y \mid X)$ and $\E(X \mid Y)$.
  \item Find the conditional variances $\Var(Y \mid X)$ and $\Var(X \mid Y)$.
\end{enumerate}

  \begin{sol}
\begin{enumerate}
  \item 
  \[
  f(x) = \begin{cases}
    x + 0.5, \text{ if } x\in [0;1]; \\
    0, \text{ otherwise.}
  \end{cases}
  \]
  \item 
\[
f(x, y) = \begin{cases}
  (x + y) / (x + 0.5), \text{ if } x\in [0;1], y\in [0;1]; \\
  0, \text{ otherwise.}
\end{cases}
\]  
\item 
\[
\E(Y \mid X) = \frac{0.5X + 1/3}{X + 0.5}.
\]  
\item 

\end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
The random variables $X$ and $Y$ are independend with Poisson distribution with rate $\lambda = 1$.
Let $S = X + Y$.

\begin{enumerate}
  \item Find conditional probabilities $\P(X = x \mid S = s)$ and $\P(Y = y \mid S = s)$.
  \item Find conditional expected values $\E(X \mid S)$ and $\E(Y \mid S)$.
  \item Find conditional variances $\Var(X \mid S)$ and $\Var(Y \mid S)$.
  \item How the answers will change if $X \sim \dPois(\lambda_x)$ and $Y \sim \dPois(\lambda_y)$?
\end{enumerate}

  \begin{sol}
    \begin{enumerate}
      \item 
      \item $\E(X \mid S) = \E(Y \mid S) = S/2$;
      \item 
      \item  
    \end{enumerate}
    
  \end{sol}
\end{problem}
  



\begin{problem}
Let $X$ and $Y$ be independent and exponentially distributed with rate $\lambda = 1$ and $S = X + Y$.

\begin{enumerate}
  \item Find conditional densities $f(x \mid s)$ and $f(y \mid s)$.
  \item Find conditional expected values $\E(X \mid S)$ and $\E(Y \mid S)$.
  \item Find conditional variances $\Var(X \mid S)$ and $\Var(Y \mid S)$.
  \item Find $\Cov(X, Y \mid S)$ and $\Corr(X, Y \mid S)$.
  \item How the answers will change if $X \sim \dExpo(\lambda_x)$ and $Y \sim \dExpo(\lambda_y)$?
\end{enumerate}

  \begin{sol}
    \begin{enumerate}
      \item $X \mid S \sim \dUnif[0; S]$, $Y \mid S \sim \dUnif[0; S]$.
      \item $\E(X \mid S) = \E(Y \mid S) = S/2$;
      \item $\Var(X \mid S) = \Var(Y \mid S) = S^2 / 12$;
      \item  
    \end{enumerate}
    
  \end{sol}
\end{problem}


\begin{problem}
The random variable $X$ has Poisson distribution with rate $\lambda = 1$.
The random variable $Y$ has uniform distribution on $[1; 2]$. 
Random variables $X$ and $Y$ are independent. 

Find $\E(XY \mid X)$, $\Var(XY + X^3 \mid X)$, $\Cov(X, Y \mid X)$, $\Cov(XY, X^2Y \mid X)$.

\begin{sol}
    $\E(XY \mid X) = X\E(Y) = X/2$, $\Var(XY  + X^3\mid X) = X^2 \Var(Y) = X^2 /12$, $\Cov(X, Y \mid X) = 0$, $\Cov(XY, X^2Y \mid X) = X^3 \Var(Y) = X^3 / 12$
\end{sol}
\end{problem}


\section{Sigma-algebras and measurability}

\begin{leftbar}
Sigma-algebra generated by discrete random variable $X$, $\sigma(X)$ — the list of all events that can be stated using $X$.

Sigma-algebra generated by arbitrary random variable $X$, $\sigma(X)$ — the smallest list of events that satisfies two properties:
\begin{itemize}
  \item The list contains all events of the form $\{X \leq t\}$, that means one can compare $X$ with any number;
  \item If one takes countably many events from this list and does logical operations (union, complement, intersection) then one will obtain an event from the list.
\end{itemize}
\end{leftbar}


\begin{problem}
The random variable $X$ takes values $1$, $2$ and $-2$ with equal probabilities.

\begin{enumerate}
  \item Find the sigma-algebra $\sigma(X)$.
  \item How the answer will change if one modifies probability distribution of $X$?
  \item Find the sigma-algebra $\sigma(\abs{X})$.
  \item Foma knows $\abs{X}$ and Yeryoma knows $X^2$.
  What can one say about sigma-algebras that model their knowledge?
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item
\item Sigma-algebras do not depend on probabilities.
\item $\sigma(\abs{X}) = \{\emptyset, \Omega, \{\abs{X} = 2\}, \{X= 1\}\}$.
\item $\sigma(\abs{X}) = \sigma(X^2)$;
\end{enumerate}

\end{sol}
\end{problem}


\begin{problem}
Experiment may end by one of the six outcomes:

\begin{tabular}{*{4}{c}}
\toprule
& $X=-1$ & $X=0$ & $X=1$ \\
\midrule
$Y=0$ & 0.1 & 0.2 & 0.3  \\
$Y=1$ & 0.2 & 0.1 & 0.1  \\
\bottomrule
\end{tabular}

\begin{enumerate}
  \item Find expicitely the sigma-algebras $\sigma(X)$, $\sigma(Y)$, $\sigma(X \cdot Y)$, $\sigma(X^2)$, $\sigma(2X+3)$.
  \item How many elements are there in $\sigma(X, Y)$, $\sigma(X + Y)$, $\sigma(X, Y, X+Y)$?
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item
  \item $\card \sigma(X, Y) = 2^6$, $\card \sigma(X + Y) = 2^4$, $\card \sigma(X, Y, X+Y) = 2^6$.
\end{enumerate}

\end{sol}
\end{problem}




\begin{problem}
Let's look at the number of possible elements in a sigma-algebra.
\begin{enumerate}
  \item The random variable $X$ has five possible values.
How many events are there in $\sigma(X)$?
\item Can a sigma-algebra contain exactly $1000$ events? Exactly $1024$ events?
\end{enumerate}
Maria throws a coin $100$ times and remembers very well all the tosses.
\begin{enumerate}[resume]
  \item How many elementary outcomes are there in the probability space $\Omega$?
 \item How many events are there in a sigma-algebra that models Maria's knowledge?
\end{enumerate}



\begin{sol}
\begin{enumerate}
  \item $2^5$;
  \item Only $2^k$ or infinity;
  \item $2^{100}$;
  \item $2^{2^{100}}$.
\end{enumerate}
\end{sol}
\end{problem}





\begin{problem}
How sigma-algebras $\sigma(X)$ and $\sigma(f(X))$ are related? When they are equal?
\begin{sol}
In general $\sigma(f(X)) \subseteq \sigma(X)$; If $f$ is a bijection then $\sigma(f(X)) = \sigma(X)$.
\end{sol}
\end{problem}





\begin{problem}
How many different $\sigma$-algebras can be created using the set of outcomes $\Omega$ has three elements? And if $\Omega$ has four elements?

\begin{sol}
In the finite case sigma-algebra corresponds to partitions.
We get five sigma-algebras on a set of three elements and $15$ sigma-algebras on a set of four elements. 
These numbers are known as Bell numbers.
\end{sol}
\end{problem}



\begin{problem}
Provide an example of algebra that is not a $\sigma$-algebra.

\begin{sol}
Let $\Omega = \NN$, $\cA$ contains all finite sets and sets with finite complement.
\end{sol}
\end{problem}

\begin{problem}
Prove a statement or provide a counter-example:
\begin{enumerate}
  \item The intersection of two sigma-algebras is a sigma-algebra.
  \item If the intersection of two sigma-algebras is a sigma-algebra then one of them  is contained in the other one.
  \item The union of two sigma-algebras is a sigma-algebra.
  \item If the union of two sigma-algebras is a sigma-algebra then one of them  is contained in the other one.
\end{enumerate}


\begin{sol}
\begin{enumerate}
  \item The intersection of two sigma-algebras is always a sigma-algebra.
  \item The intersection of two sigma-algebras is always a sigma-algebra.
  \item The union of two sigma-algebras is not always a sigma-algebra.
  \item 
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
Let $\cF$ be some $\sigma$-algebra of subsets of $\Omega$ and $B\subseteq\Omega$. 
Consider the collection of sets $\cH=\{A: A\subseteq B \text{ or } B^{c}\subseteq A\}$.

Is $\cH$ a $\sigma$-algebra?

\begin{sol}
Yes. This is convinient do draw $\Omega$ as a segment. With «пескари» $A\subseteq B$ and «sharks» $A \supseteq B^{c}$.
\end{sol}
\end{problem}


\begin{problem}
Будем обозначать количество элементов множества с помощью $\card A$.
Рассмотрим подмножества натуральных чисел, $A \subseteq \mathbb{N}$.
Определим для подмножества плотность Чезаро (Cesaro density),
\[
\gamma(A)=\lim_{n\to \infty}\frac{\card (A\cap \{1,2,3, \ldots,n\})}{n}
\]
в тех случаях, когда этот предел существует.

Плотность Чезаро показывает, какую «долю» от всех натуральных чисел составляет указанное подмножество.
Обозначим с помощью $\cH$ все подмножества, имеющие плотность Чезаро.

\begin{enumerate}
\item Чему равна плотность Чезаро у нечётных чисел?
\item Приведите пример множества, у которого не определена доля Чезаро.
\item Верно ли, что у натуральных чисел, в записи которых присутствует
хотя бы одна единица, есть доля? Если да, то чему она равна?
\item Верно ли, что у натуральных чисел, в записи которых присутствует
ровно одна единица, есть доля? Если да, то чему она равна?
\item Верно ли, что $\cH$ — алгебра? Сигма-алгебра?
\end{enumerate}


\begin{sol}
Разобьем натуральный ряд на пары соседних чисел. Можно
так подобрать множества $A$ и $B$, что в каждом из них из каждой
пары взято только одно число.
Поэтому $\gamma(A)=\gamma(B)=\frac{1}{2}$. Подобрав
совпадение-несовпадение в паре, можно сделать так, что
$\gamma(A\cap B)$ не существует.
\end{sol}
\end{problem}


\begin{problem}
We throw a fair dice.
Let $Y$ be the indicator of a even score and $Z$ be the indicator of score bigger than $2$.
\begin{enumerate}
\item Find the sigma-algebra $\sigma(Z)$.
\item Find the sigma algebra $\sigma(Y\cdot Z)$.
\item How many elements are there in $\sigma(Y,Z)$?
\item How are related the $\sigma$-algebras $\sigma(Y\cdot Z)$ and $\sigma(Y,Z)$?
\end{enumerate}


\begin{sol}
  \begin{enumerate}
    \item $\sigma(Z) = \{\{Z =1\}, \{Z = 0\}, \Omega, \emptyset \}$.
    \item $\sigma(YZ) = \{\{YZ =1\}, \{YZ = 0\}, \Omega, \emptyset \}$.    
    \item $2^4$;
    \item $\sigma(Y\cdot Z) \subseteq \sigma(Y,Z)$.
    \end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
We throw a coin infinitely many times. 
Let $X_{n}$ be the indicator that the coin landed on Head at toss number $n$.
Consider a pack of $ \sigma$-algebras: $\cF_{n}:=\sigma(X_1, X_2, \ldots, X_n)$, $\cH_{n}:=\sigma(X_{n}, X_{n+1}, X_{n+2}, \ldots)$.

\begin{enumerate}
\item For each case provide two examples of $\sigma$-algebras that contain the corresponding event
\begin{enumerate}
\item $\{X_{37}>0 \}$;
\item $\{X_{37}>X_{2024} \}$;
\item $\{ X_{37}>X_{2024}>X_{12} \}$;
\end{enumerate}
  
\item Simplify experessions: $\cF_{11}\cap \cF_{25}$, $\cF_{11}\cup \cF_{25}$, $\cH_{11}\cap \cH_{25} $, $\cH_{11}\cup \cH_{25}$.

\item For each case provide two non-trivial examples (different from $\Omega$ and $\emptyset$) of an event $A$ such that

\begin{enumerate}
\item $A\in \cF_{2024}$;
\item $A\notin \cF_{2025}$;
\item $A \in \cH_{n}$ for all possible $n$;
\end{enumerate}


\end{enumerate}


\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
Правда ли равносильны три набора требований к списку множеств $\cF$?

Тариф «Классический»:
\begin{enumerate}
  \item $\Omega \in \cF$;
  \item Если $A\in \cF$, то $A^c \in \cF$;
  \item Если $A_1, A_2, A_3, \ldots, \in \cF$, то $\cup A_i \in \cF$.
\end{enumerate}


Тариф «Перевёрнутый»:
\begin{enumerate}
  \item $\emptyset \in \cF$;
  \item Если $A\in \cF$, то $A^c \in \cF$;
  \item Если $A_1, A_2, A_3, \ldots, \in \cF$, то $\cap A_i \in \cF$.
\end{enumerate}


Тариф «Хочу всё»:
\begin{enumerate}
  \item $\Omega \in \cF$, $\emptyset \in \cF$;
  \item Если $A\in \cF$ и $B\in \cF$, то $A \backslash B \in \cF$;
  \item Если $A_1, A_2, A_3, \ldots, \in \cF$, то $\cup A_i \in \cF$ и $\cap A_i \in \cF$.
\end{enumerate}


\begin{sol}
Yes!
\end{sol}
\end{problem}



\begin{problem}
Рассмотрим $\Omega=[0;1]$ и набор множества $\cF$ таких,
что либо каждое множество не более, чем счётно, либо дополнение к нему не более, чем счётно.

\begin{enumerate}
\item Верно ли, что $\cF$ — алгебра? $\sigma$-алгебра?
\item Придумайте $B\subset\Omega$, такое что $B \notin\cF$.
\end{enumerate}

\begin{sol}
Например, $B$ — Канторово множество, или, гораздо проще,
$B=[0;0,5]$. Оно само более чем счетно и дополнение к нему более
чем счетно.

Набор $\cF$ действительно $\sigma$-алгебра.
$\emptyset$ лежит в $\cF$, так как имеет ноль элементов.

Если $A$ не более чем счетно, то $A^{c}$ лежит в $\cF$,
так как дополнение к $A^{c}$ содержит не более чем счетное
число элементов.

Если дополнение к $A$ не более чем счетно, то $A^{c}$ лежит
в $\cF$, так как содержит не более чем счетное число элементов.

Проверяем счетное объединение $\bigcup_{i} A_{i}$.
Если среди $A_{i}$ встречаются только не более чем счетные, то и
их объединение — не более чем счетно.
Если среди $A_{i}$ встретилось хотя бы одно множество с не более
чем счетным дополнением, то $\bigcup_{i} A_{i}$ тоже будет
обладать не более чем счетным дополнением, так как объединение не
может быть меньше ни одного из объединяемых множеств.
\end{sol}
\end{problem}



\begin{problem}
В лесу есть три вида грибов: рыжики, лисички и мухоморы. Попадаются они равновероятно и независимо друг от друга. Маша нашла 100 грибов. Пусть $R$ — количество рыжиков, $L$ — количество лисичек, а $M$ — количество мухоморов среди найденных грибов.
\begin{enumerate}
\item Сколько элементов $ \sigma(R)$?
\item Сколько элементов $ \sigma(R,M)$?
\item Измерима ли $L$ относительно $ \sigma(R)$?
\item Измерима ли $L$ относительно $ \sigma(R,M)$?
\item Измерима ли $L$ относительно $ \sigma(R+M)$?
\item Измерима ли $L$ относительно $ \sigma(R-M)$?
\end{enumerate}

\begin{sol}
$ 2^{101} $, $2^{101\cdot 51}$,
\end{sol}
\end{problem}



\begin{problem}
Сейчас либо солнечно, либо дождь, либо пасмурно без дождя.
Соответственно, множество $\Omega$ состоит из трёх исходов, $\Omega=\{\text{солнечно},\text{дождь},\text{пасмурно}\}$.
Джеймс Бонд пойман и привязан к стулу с завязанными глазами, но он может на слух отличать, идёт ли дождь.
\begin{enumerate}
\item Как выглядит $\sigma$-алгебра событий, которые различает агент 007?
\item Как выглядит минимальная $\sigma$-алгебра, содержащая событие $A=\{\text{не видно солнце}\}$?
\item Сколько различных $\sigma$-алгебр можно придумать для данного $\Omega$?
\end{enumerate}


\begin{sol}
$\cF=\{\emptyset, \Omega, \{\text{дождь}\}, \{\text{солнечно}, \text{пасмурно}\}$. Всего есть $1+1+3=5$ $\sigma$-алгебр.
\end{sol}
\end{problem}

  


\section{Sigma-algebras and conditional expected value}


\begin{problem}
At time moment $t = 0$ in the casiono there are countably many players with perfect memory.
Let's number them as Miss First, Mister Second, etc. 


Time is discrete. 
Random variables $X_t$ are independent and take values $+1$ or $-1$ with equal probabilities.
At each moment of time $t > 0$ everybode gets $X_t$ roubles and than the player number $t$ leaves the casiono.

The cumulative sum $S_t = X_1 + \dots + X_t$ reaches its first local maximum at the random time $T$.
At time $T+1$ the dealer calls his friend Black Jack and says «It's time!»
They have agreed beforehand on the call time. 

Black Jack chases the player number $T$ and steals all his information before the police can intervent.  
Let's describe the information of Black Jack by sigma-algebra $\cF_{J}$ and the information of every player $t$ at the last moment in casino by $\cF_t$.

\begin{enumerate}
  \item Which sigma-algebras contain the event $\{T = 10\}$?
  \item Provide an example of two events from $\cF_J$ that do not enter in neither $\cF_t$.
  \item Find conditional expected values $\E(T \mid \cF_J)$, $\E(X_T \mid \cF_J)$, $\E(X_{T+1} \mid \cF_J)$.
  \item Find conditional expected values $\E(S_{T - 1} \mid \cF_J)$, $\E(S_T \mid \cF_J)$, $\E(S_{T+1} \mid \cF_J)$, $\E(S_{T+2} \mid \cF_J)$.
\end{enumerate}
Let's define $Y_{T - k}$ as 
\[
Y_{T - k} = \begin{cases}
  X_{T - k}, \text{ if } T - k > 0; \\
  0, \text{ otherwise.} 
\end{cases}
\]
\begin{enumerate}[resume]
  \item Find $\E(Y_{T - 10} \mid \cF_J)$.
  \item Find conditional expected values $\E(X_T \mid \cF_{10})$, $\E(X_{T+1} \mid \cF_{10})$.
  \item Find $\E(T \mid \cF_{10})$ and $\E(S_T \mid \cF_{10})$.
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item $\cF_J$, $\cF_{11}$, $\cF_{12}$, \dots
    \item $\{T \text{ is divisible by } 2\}$, $\{T \geq 3, X_{T-2} = 1\}$.
    \item $\E(T \mid \cF_J) = T$, $\E(X_{T} \mid \cF_J) = 1$, $\E(X_{T+1} \mid \cF_J) = -1$.
    \item $\E(S_T \mid \cF_J) = S_T$, $\E(S_{T+1} \mid \cF_J) = S_T - 1$, $\E(S_{T+2} \mid \cF_J) = S_T - 1$.
    \item $\E(Y_{T - k} \mid \cF_J) = Y_{T- k }$.
    \item $\E(X_T \mid \cF_{10}) = 1$, $\E(X_{T+1} \mid \cF_{10}) = -1$.
    \item $\E(T \mid \cF_{10}) = ... $, $\E(S_T \mid \cF_{10}) = ...$.  
\end{enumerate}
    
\end{sol}
\end{problem}


\begin{problem}
Bad police officers operate in groups of $1$, $2$ or $3$ people with probabilities $0.5$, $0.2$ and $0.3$. 
If you cross the road in the wrong place, they will catch you and demand a bribe $X$ of $1$, $5$ or 10 thousand rubles respectively. 

For each of the following cases write down the $\sigma$-algebra $\cF$ that models your information and calculate $\E(X \mid \cF)$.

  \begin{enumerate}
    \item you can see how many officers are going to stop you;
    \item they are sitting in the car and you don't know their number;
    \item it is dark and you can only say if it is one policeman or more than one. 
    \end{enumerate}
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
HSE student rolled the dice once. 
Find the $\sigma$-algebras that model the following situations:
\begin{enumerate}
    \item she only knows that the dice was rolled once;
    \item she knows the result of the roll;
    \item she observes the result of the roll but she is able to count only up to two.
\end{enumerate}
\begin{sol}
  Here $\Omega = \{1, 2, 3, 4, 5, 6\}$.
  \begin{enumerate}
    \item $\cF = \{\emptyset, \Omega\}$;
    \item $\cF = 2^{\Omega}$, this notation means «all subsets of $\Omega$».
    \item $\cF = \sigma(\{1\}, \{2\})$, eight events in total;
  \end{enumerate}

\end{sol}
\end{problem}

\section{Martinales}

\begin{leftbar}
\begin{itemize}
  \item \emph{Natural filtration} of a process $(X_n)$ is given by $\cF_n = \sigma(X_1, X_2, \dots, X_n)$.
  \item A process $(X_n)$ is a \emph{martingale} if $\E(X_{n + k} \mid X_n, X_{n-1}, \dots, X_1) = X_{n}$ for all $k \geq 1$.
  \item A process $(X_n)$ is \emph{adapted} to filtration $(\cF_n)$ if every random variable $X_n$ is measurable wrt to sigma-algebra $\cF_n$.
  \item A process $(X_n)$ is a \emph{martingale wrt to filtration} $(\cF_n)$ if $\E(X_{n + k} \mid \cF_n) = X_{n}$.
\end{itemize}
\end{leftbar}


\begin{problem}
Consider the sequence $(X_t)$ of independent identically distributed random variables with mean $\E(X_t) = 2$ and variance $\Var(X_t) = 3$.
Let's work with natural filtration $\cF_t = \sigma(X_1, X_2, \dots, X_t)$.

On the base of $(X_t)$ let's create more sequences: $S_t = X_1 + X_2 + \dots + X_t$, $W_t = S_t - 2t$ and $Y_t = W_t^2 - 3t$.

\begin{enumerate}
  \item Is $(X_t)$ a martingale with respect to $(\cF_t)$?
  \item Is $(S_t)$ a martingale with respect to $(\cF_t)$?
  \item Is $(W_t)$ a martingale with respect to $(\cF_t)$?
  \item Is $(Y_t)$ a martingale with respect to $(\cF_t)$?
  \item Is $(W_t)$ a martingale with respect to $(\cF_{t-1})$?
  \item Is $(W_t)$ a martingale with respect to $(\cF_{t+1})$?
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $(X_t)$ is not a martingale with respect to $(\cF_t)$;
    \item $(S_t)$ is not a martingale with respect to $(\cF_t)$;
    \item $(W_t)$ is a martingale with respect to $(\cF_t)$;
    \item $(Y_t)$ is a martingale with respect to $(\cF_t)$;
    \item $(W_t)$ is not a martingale with respect to $(\cF_{t-1})$;
    \item $(W_t)$ is not a martingale with respect to $(\cF_{t+1})$;
  \end{enumerate}  
\end{sol}
\end{problem}

\begin{problem}
Vasiliy has found three non-random infinite sequences in his garage: $a_n = n$, $b_n = -n$ and $c_n = 0$.
He randomly selects one of these sequences with equal probabilities and hence obtain a sequence of random variables $(X_n)$.
\begin{enumerate}
  \item What is the distribution of $X_7$?
  \item Find $\E(X_n)$ and $\Var(X_n)$.
  \item Is $(X_n)$ a Markov chain?
  \item Is $(X_n)$ a martingale?
  \item Explicitely find the $\sigma$-algebra $\sigma(X_1, X_2, X_3, \dots, X_{1000})$.
  \item Find the probability that limit of $(X_n)$ exists.
  \item Does $\plim X_n$ exist?
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item What is the distribution of $X_7$?
    \item $\E(X_n) = 0$ and $\Var(X_n) = $.
    \item The process $(X_n)$ is a Markov chain!
    \item The process $(X_n)$ is not a martingale.
    \item $\sigma(X_1, X_2, X_3, \dots, X_{1000}) = \sigma(X_1)$
    \item $\P(\lim X_n \text{ exists}) = 1/3$.
    \item $\plim X_n$ does not exist.
  \end{enumerate}  
\end{sol}
\end{problem}

\begin{problem}
Consider the sequence $(X_t)$ of independent identically distributed random variables
that take values $0$ or $1$ with equal probabilities.
Let's work with natural filtration $\cF_t = \sigma(X_1, X_2, \dots, X_t)$.

On the base of $(X_t)$ let's create more sequences: $S_t = X_1 + X_2 + \dots + X_t$, $W_t = S_t - at$, $M_t = \exp(bS_t)$.

\begin{enumerate}
  \item Is $(X_t)$ a martingale?
  \item Is $(S_t)$ a martingale?
  \item For which values of $a$ the process $(W_t)$ is a martingale?
  \item For which values of $b$ the process $(M_t)$ is a martingale?
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $(X_t)$ is not a martingale;
    \item $(S_t)$ is not a martingale;
    \item $a = 0.5$;
    \item $b = 0$ and $b = ...$
  \end{enumerate}  
\end{sol}
\end{problem}
  


\begin{problem}
Consider a well-mixed standard deck of $52$ cards. 
James Bond in an elegant outfit\footnote{Sponsors are wellcome to contact us for product placement!} 
opens cards one by one. 
Let the sigma-algebra $(\cF_n)$ model his information 
and $(X_n)$ be the proportion of Queens in the closed part of the deck after opening $n$ cards.

\begin{enumerate}
  \item Find the marginal distribution of $X_0$, $X_1$ and $X_{51}$.
  \item Find the joint distribution of $X_{50}$ and $X_{51}$.
  \item Is $(X_n)$ a martingale with respect to $(\cF_n)$?
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item $X_0 = 4/52$; $X_1$ is $4/51$ with probability $48/52$ or $3/51$ with probability $4/52$;
     $X_{51}$ is $1$ with probability $4/52$ and $0$ with probability $48/52$.
    \item 
    \item The process $(X_n)$ is a martingale with respect to $(\cF_n)$.
  \end{enumerate}
  
\end{sol}
\end{problem}

\begin{problem}
If possible create a martingale $(X_n)$ such that simulteneously $\P(X_n = 0 \text{ infinitely often}) = 1$ and
$\P(X_n = 1 \text{ infinitely often}) = 1$.
\begin{sol}
It is possible. 
\end{sol}
\end{problem}

\begin{problem}
At time $t=0$ there is one black and one white ball in the vase. 
At each moment of time we take out randomly one ball from the vase and put back two balls of the same color. 
Let $(W_t)$ be the proportion of white balls in the vase after $t$ extractions and $(Q_t)$ be the number of times when white ball was extracted.
\begin{enumerate}
  \item What is the distribution of $W_1$? Of $W_2$?
  \item Is $(W_t)$ a martingale?
  \item Consider a fixed parameter $p \in (0;1)$ and the process $M_t = (t + 1) C_t^{Q_t} p^{Q_t}(1-p)^{t - Q_t}$.
  Is $(M_t)$ a martingale?
  \item What is the limiting distribution of $(W_t)$?
  % \item What is the limiting distribution of $(Q_t / t)$? (need to think)
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item $W_1$ is equal to $1/3$ or $2/3$ with equal probabilities; $W_2$ is equal to $1/4$, $2/4$, $3/4$
    \item $(W_t)$ is a martingale;
    \item $(M_t)$ is a martingale;
    \item The limiting distribution of $(W_t)$ is uniform on $[0;1]$;
    % \item What is the limiting distribution of $(Q_t / t)$? (need to think)
  \end{enumerate}
  
\end{sol}
\end{problem}
  



\begin{problem}
Consider non-random sequence of numbers $(a_n)$. 
How can this sequence be a martingale?
\begin{sol}
Only constant non-random sequences are martingales.
\end{sol}
\end{problem}



\begin{problem}
Let $(M_n)$ be a martingale and $a<b<c<d$. 
\begin{enumerate}
  \item Find covariance $\Cov(M_d - M_c, M_b - M_a)$.
  \item Are $(M_d - M_c)$ and $(M_b - M_a)$ independent?
\end{enumerate}
\begin{sol}
$\Cov(M_d - M_c, M_b - M_a) = 0$;
\end{sol}
\end{problem}

\begin{problem}
Let $(M_t)$ be a process adapted to filtration $(\cF_t)$.

Is it true that in discrete time conditions 
\[
  \E(M_{t+1} \mid \cF_t) = M_t
\]
and
\[
\E(M_{t+k} \mid \cF_t) = M_t \text{ for all } k\geq 1
\]
are equivalent?
\begin{sol}
Yes. 
\end{sol}
\end{problem}

\begin{problem}
Initial wealth of a player is equal to $W_0 = 1$.
At each moment of time she can bet any proportion of her wellfare on the toss of a coin. 
If she guesses wrong she loses her bet. 
If she guesses right she gets profit equal to hear bet. 
The coin is not fair lands on head with probability $0.8$. 

\begin{enumerate}
  \item Find the bet that maximises one period log interest rate $\E(\ln (W_{t+1} / W_t) \mid \cF_t)$.
  \item Assume that the player maximises one period log interest rate every time. 
  Find a constant $a$ such that $\ln W_n - an$ is a martingale. 
\end{enumerate}


\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
For each case provide an example of a process.
\begin{enumerate}
  \item $(X_n)$ is a Markov chain and a martingale.
  \item $(X_n)$ is a Markov chain but not a martingale.
  \item $(X_n)$ is a martingale but not a Markov chain.
  \item $(X_n)$ is neither a Markov chain nor a martingale.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}


\begin{problem}

\begin{sol}

\end{sol}
\end{problem}

\subsection{Stopping time}

\begin{problem}
Famous «ABRACADABRA» problem. 

A monkey types randomly letters on a typewriter choosing each time one of the $26$ letters with equal probabilities.  
Let $T$ be the number of keypresses required to write the word «ABRACADABRA» for the first time. 

\begin{enumerate}
  \item Organise a casino to calculate $\E(T)$.
  \item Organise a casino to calculate $\E(T^2)$ and hence $\Var(T)$.
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\E(T) = 26^{11} + 26^4 + 26$.
    \item 
  \end{enumerate}  
\end{sol}
\end{problem}


  

\section{Poisson process}

\begin{leftbar}
  The process $(N_t)$ is called \emph{Poisson process} with intensity $\lambda$ if 
  \begin{itemize}
    \item $N_0 = 0$;
    \item Increments are independent:
    If $t_1 < t_2 < t_3 < \dots < t_k$ then random increments $N(t_2) - N(t_1)$, $N(t_3) - N(t-2)$, \dots{ } are independent.
    \item Increments have Poisson distribution:
    \[
    N_b - N_a \sim \dPois(\lambda (b-a));
    \]
  \end{itemize}
  The process $(N_t)$ is called \emph{Poisson process} with intensity $\lambda$ if 
  \begin{itemize}
    \item $N_0 = 0$;
    \item Increments are independent;
    \item Increments are stationary:
    
    The distribution of $N_b - N_a$ depends only on $(b - a)$.
    \item Probability of observing two or more points in a small interval is negligible:
    \[
    \P(N_{t + \Delta} - N_t > 1) = o(\Delta).
    \]
  \end{itemize}
\end{leftbar}


\begin{problem}
Two cashiers Alice and Bob simulteneously started to service their clients.
The service times $X_a$ and $X_b$ are independent and exponentially distributed with rates $\lambda_a = 1$ and $\lambda_b = 2$.

\begin{enumerate}
  \item Find the probability $\P(X_a < X_b)$.
  \item Find the density of $S = X_a + X_b$.
  \item Find the density of $L = \min\{X_a, X_b\}$.
  \item Find the density of $R = \max\{X_a, X_b\}$.
  \item Solve all the previous points for general rates $\lambda_a$ and $\lambda_b$.
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item $\P(X_a < X_b) = \lambda_a/(\lambda_a + \lambda_b)$.
    There are two possible solutions: double integral and first step analysis.
    \item 
    \item 
    \item 
  \end{enumerate}  
\end{sol}
\end{problem}


\begin{problem}
Let $X_t$ and $Y_t$ be two independent Poisson processes. 
Is it true that $S_t = X_t + Y_t$ is also a Poisson process?
\begin{sol}
Yes. 
\end{sol}
\end{problem}

\begin{problem}
Hedgehogs are scattered in a big forest according Poisson process with rate $\lambda = 1$ per $100$ squared meters. 

What should be the edge of a square such that the probability of finding a hedgehog there is $0.7$?
\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Let $N_t$ be a Poisson process with rate $\lambda$.
\begin{enumerate}
  \item Is the process $A_t = N_t - \lambda t$ a martingale?
  \item Is the process $B_t = A_t^2 - \lambda t$ a martingale?
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $A_t = N_t - \lambda t$ is a martingale;
    \item $B_t = A_t^2 - \lambda t$ is a martingale;
  \end{enumerate}  
\end{sol}
\end{problem}


\begin{problem}
  Students arrive in the Grusha café according to the Poisson arrival process $(X_t)$ with constant rate $\lambda$.
  The probability of no visitors during 5 minutes is 0.05.
  \begin{enumerate}
    \item   Find the value of $\lambda$.
    \item  Find the variance and expected number of arrivals between 5 pm and 8 pm.
    \item What is the probability of exactly 5 arrivals between 5 pm and 8 pm?
  \end{enumerate}
\begin{sol}

\end{sol}
\end{problem}
  


\begin{problem}
Masha receives on average 10 sms per minute. 
Sms arrival is well described by the Poisson process.
\begin{enumerate}
  \item  What is the probability that Masha receives exactly 10 sms in the next 40 seconds?
  \item Masha just received an sms. What is the probability that she will wait more that 2.5 seconds before the
  next one?
  \item  Find the covariance between the number of sms in the first 3 minutes and the number of sms in the first
  10 minutes.
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}
  


\begin{problem}
Taxis arrive to the station according to the Poisson process with rate 1 per 5 minutes.
Let $Y_t$ be the number of taxis that will arrive between 0 and $t$ minutes.
\begin{enumerate}
  \item  Sketch the expected value of $Y_t$ as a function of $t$.
\item Sketch the probability $\P(Y_t = Y_{60})$ as a function of $t$.
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}



\begin{problem}
A company gets fines for non-removal of quadrobics video content.
What is the probability that the total amount will exceed two undecillion roubles in 1000 days for each case?

\begin{enumerate}
  \item Fines arrive according to Poisson process with rate $1$ fine per day and each fine has the size $10^{33}$ roubles. 
  Fines are summing up without additional penalties.

  \item Initial fine is $10^5$ roubles but it doubles according to Poisson process with rate $1$ doubling per $10$ days. 
\end{enumerate}

\begin{sol}
Here we may approximate Poisson distribution by normal distribution, $\cN(\lambda t, \lambda t)$.
\end{sol}
\end{problem}
  



\begin{problem}

\begin{sol}

\end{sol}
\end{problem}
  
  


\begin{problem}
Customers order coffee according to Poisson process with rate $1$ cup per minute.
The owner will close the shop if no one orders a coffee in $7$ minutes.

Let $X$ be the closure time. 

Find $\E(X)$ and $\Var(X)$.
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
The arrival of buses at a given stop follows Poisson process with rate $3$. 
The arrival of taxis at same stop follows independent Poisson process with rate $5$. 

\begin{enumerate}
  \item What is the probability that two or more taxis will arrive before next bus?
  \item What is the probability that exactly two taxis will arrive before next bus?
\end{enumerate} 
\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Customers order coffee according to Poisson process with rate $1$ cup per minute.
Let $N_t$ be the number of orders up to time $t$.

Find the probability $\P(N_t \text{ is even})$.
\begin{sol}
Let's denote $a(t) = \P(N_t \text{ is even})$.
\[
a(t+\Delta)=a(t)(1-\Delta)+(1-a(t))\Delta + o(\Delta) 
\]
Hence we get a differential equation  $a'(t)=1-2a(t)$ with $a(0)=1$.
The solution is $a(t)=(1+\exp(-2t))/2$.
\end{sol}
\end{problem}
  


\begin{problem}
Prove that two definitions of Poisson process are equivalent.

Definition A. The process $(N_t)$ is called \emph{Poisson process} with intensity $\lambda$ if 
  \begin{itemize}
    \item $N_0 = 0$;
    \item Increments are independent:
    If $t_1 < t_2 < t_3 < \dots < t_k$ then random increments $N(t_2) - N(t_1)$, $N(t_3) - N(t-2)$, \dots{ } are independent.
    \item Increments have Poisson distribution:
    \[
    N_b - N_a \sim \dPois(\lambda (b-a));
    \]
  \end{itemize}

  Definition B. The process $(N_t)$ is called \emph{Poisson process} with intensity $\lambda$ if 
  \begin{itemize}
    \item $N_0 = 0$;
    \item Increments are independent;
    \item Increments are stationary:
    
    The distribution of $N_b - N_a$ depends only on $(b - a)$.
    \item Probability of observing two or more points in a small interval is negligible:
    \[
    \P(N_{t + \Delta} - N_t > 1) = o(\Delta).
    \]
  \end{itemize}
\begin{sol}

\end{sol}
\end{problem}
  


\Closesolutionfile{solution_file}


% для гиперссылок на условия
% http://tex.stackexchange.com/questions/45415
\renewenvironment{solution}[1]{%
         % add some glue
         \vskip .5cm plus 2cm minus 0.1cm%
         {\bfseries \hyperlink{problem:#1}{#1.}}%
}%
{%
}%

\section{Solutions}
\input{all_solutions}


\printindex


\section{Sources of wisdom}

\nocite{buzun2015stochastic}

\nocite{buzun2015stochastic}

\printbibliography[heading=none]


\end{document}
