% arara: xelatex: {shell: yes}
% arara: biber
% arara: xelatex: {shell: yes}
% arara: xelatex: {shell: yes}



%!TeX cleanPatterns = $OUTDIR/$JOB!($OUTEXT|.synctex.gz|.tex|.pdf), /$OUTDIR/_minted-$JOB/
\documentclass[12pt, a4paper]{article}
\usepackage{libertine}

% utf8 is the preferred encoding

 % this magick is to solve problem that appeared after update of texlive 2018 to texlive 2020
 % https://tex.stackexchange.com/questions/511341/the-error-occurred-after-the-last-update
\makeatletter
\def\nobreak{\penalty\@M}
\makeatother


\usepackage{fontspec} % что-то про шрифты? % нужно ли загружать?

\usepackage{polyglossia} % русификация xelatex
\usepackage{csquotes}


\setmainlanguage{english}
\setotherlanguage{russian}

% download "Linux Libertine" fonts:
% http://www.linuxlibertine.org/index.php?id=91&L=1
%\setmainfont{Linux Libertine O} % or Helvetica, Arial, Cambria
% why do we need \newfontfamily:
% http://tex.stackexchange.com/questions/91507/
%\newfontfamily{\cyrillicfonttt}{Linux Libertine O}

\newfontfamily\arabicfont[Script=Arabic]{Scheherazade New}


\usepackage{etoolbox} % provides \AtEndPreamble
% etoolbox causes wrong behavior of tocbasic
\AtEndPreamble{ % ради арабского написания Абу ибн-Сина
  \usepackage{arabxetex} 
 \let\textarabic\relax 
 \let\Arabic\relax 
\setotherlanguages{arabic, english}
}
% комбо из:
% https://tex.stackexchange.com/questions/501897
% https://tex.stackexchange.com/questions/392175/

\usepackage{imakeidx} 
\indexsetup{level=\section}
\makeindex[title=Hashtags]

% \usepackage{etex} % расширение классического tex
% в частности позволяет подгружать гораздо больше пакетов, чем мы и займёмся далее

\usepackage{verbatim} % для многострочных комментариев
\usepackage{makeidx} % для создания предметных указателей

\usepackage{setspace}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathrsfs} % sudo yum install texlive-rsfs
\usepackage{dsfont} % sudo yum install texlive-doublestroke
\usepackage{array, multicol, multirow, bigstrut} % sudo yum install texlive-multirow
\usepackage{indentfirst} % установка отступа в первом абзаце главы


\usepackage{bm}
\usepackage{bbm} % шрифт с двойными буквами
%\usepackage[perpage]{footmisc}

\usepackage{dcolumn} % центрирование по разделителю для apsrtable

% создание гиперссылок в pdf
\usepackage[unicode, colorlinks=true, urlcolor=blue, hyperindex, breaklinks]{hyperref}


\usepackage{microtype} % свешиваем пунктуацию
% теперь знаки пунктуации могут вылезать за правую границу текста, при этом текст выглядит ровнее


\usepackage{textcomp}  % Чтобы в формулах можно было русские буквы писать через \text{}

% размер листа бумаги
%\usepackage[paperwidth=145mm,paperheight=215mm,
%height=182mm,width=113mm,top=20mm,includefoot]%{geometry}
\usepackage[paper=a4paper, top=15mm, bottom=13.5mm, left=16.5mm, right=13.5mm, includefoot]{geometry}

\usepackage{xcolor}

\usepackage{framed} %  \leftbar


% \usepackage{float, longtable}
% \usepackage{soulutf8}

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке

\usepackage{mathtools}
\usepackage{cancel, xspace} % sudo yum install texlive-cancel


\usepackage{numprint} % sudo yum install texlive-numprint
\npthousandsep{,}\npthousandthpartsep{}\npdecimalsign{.}


% \usepackage{subfigure} % для создания нескольких рисунков внутри одного

\usepackage{tikz, pgfplots} % язык для рисования графики из latex'a
\pgfplotsset{compat=1.16}
\usetikzlibrary{trees} % tikz-прибамбас для рисовки деревьев
\usepackage{tikz-qtree} % альтернативный tikz-прибамбас для рисовки деревьев
\usetikzlibrary{arrows} % tikz-прибамбас для рисовки стрелочек подлиннее

\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет Последний день Помпеи}
% \listoftodos --- печатает все поставленные \todo'шки



\usepackage{booktabs} %  красивые таблицы
% заповеди из докупентации:
% 1. Не используйте вертикальные линни
% 2. Не используйте двойные линии
% 3. Единицы измерения - в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"

\usepackage{physics}
% \usepackage{minted} % moved to listings to simplify development
\usepackage{listings}
\lstset{%
basicstyle=\fontfamily{lmtt}\bfseries,
keywordstyle=\fontfamily{lmtt}\bfseries
}
% \usepackage{julia-mono-listings}
% TODO: установить?
\usepackage{answers}




\usepackage[bibencoding=auto, backend=biber, sorting=none, style=alphabetic]{biblatex}

\addbibresource{stochastic_pro.bib}

\setcounter{tocdepth}{1} % в оглавление оставляем уровень 1

\usepackage[titles]{tocloft} % альтернатива tocbasic для настройки toc
% если нужен subfigure, то у tocloft можно добавить опцию subfigure
\renewcommand{\cftbeforesecskip}{0.7pt} % поправка интервала между строками для section в toc
\renewcommand{\cftsecdotsep}{\cftdotsep} % добавляем точечки

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
% \setlist[enumerate, 1]{label=\asbuk*),ref=\asbuk*} % цифра рядом с enumerate = уровень нумерации
\setlist[enumerate, 1]{label=\alph*),ref=\alph*} % цифра рядом с enumerate = уровень нумерации


%%%%%%%%%%%%%%%%%%%%%%%  ПАРАМЕТРЫ  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setstretch{1}                          % Межстрочный интервал
\flushbottom                            % Эта команда заставляет LaTeX чуть растягивать строки, чтобы получить идеально прямоугольную страницу
\righthyphenmin=2                       % Разрешение переноса двух и более символов
%\pagestyle{plain}                       % Нумерация страниц снизу по центру.
\widowpenalty=300                     % Небольшое наказание за вдовствующую строку (одна строка абзаца на этой странице, остальное --- на следующей)
\clubpenalty=3000                     % Приличное наказание за сиротствующую строку (омерзительно висящая одинокая строка в начале страницы)
\setlength{\parindent}{1.5em}           % Красная строка.
%\captiondelim{. }
\setlength{\topsep}{0pt}
\emergencystretch=2em

% делаем короче интервал в списках
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}



\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\sgn}{sign}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\amn}{arg\,min}
\DeclareMathOperator*{\amx}{arg\,max}


\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\sCorr}{sCorr}
\DeclareMathOperator{\sCov}{sCov}
\DeclareMathOperator{\sVar}{sVar}

\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\Med}{Med}


\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\e}{\varepsilon}


\newcommand{\cF}{\mathcal{F}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}

\newcommand{\cN}{\mathcal{N}}
\newcommand{\dNorm}{\mathcal{N}}
\newcommand{\dN}{\mathcal{N}}
\newcommand{\dLN}{\mathcal{LN}}

\newcommand{\dBern}{\mathrm{Bern}}
\newcommand{\dPois}{\mathrm{Pois}}
\newcommand{\dBin}{\mathrm{Bin}}
\newcommand{\dMult}{\mathrm{Mult}}
\newcommand{\dGeom}{\mathrm{Geom}}
\newcommand{\dNHGeom}{\mathrm{NHGeom}}
\newcommand{\dHGeom}{\mathrm{HGeom}}
\newcommand{\dDUnif}{\mathrm{DUnif}}
\newcommand{\dFS}{\mathrm{FS}}
\newcommand{\dNBin}{\mathrm{NBin}}

\newcommand{\dTri}{\mathrm{Triangle}}
\newcommand{\dUnif}{\mathrm{Unif}}
\newcommand{\dCauchy}{\mathrm{Cauchy}}
\newcommand{\dExpo}{\mathrm{Expo}}
\newcommand{\dBeta}{\mathrm{Beta}}
\newcommand{\dGamma}{\mathrm{Gamma}}
\newcommand{\dWei}{\mathrm{Wei}}
\newcommand{\dLogistic}{\mathrm{Logistic}}
\newcommand{\dRayleigh}{\mathrm{Rayleigh}}
\newcommand{\dPareto}{\mathrm{Pareto}}


% вместо горизонтальной делаем косую черточку в нестрогих неравенствах
\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}


\newcommand{\wv}{\textrm{word2vec}}
\newcommand \hVar{\widehat{\Var}}
\newcommand \hCorr{\widehat{\Corr}}
\newcommand \hCov{\widehat{\Cov}}


\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}

\newcommand{\Lin}{\mathcal{L}in}
\newcommand{\Linp}{\Lin^{\perp}}

\title{Stochastic Processes problems}
\author{\url{https://github.com/bdemeshev/stochastic_pro}}
\date{\today}


%\newtheorem{problem}{Задача}
%\numberwithin{problem}{section}

\Newassociation{sol}{solution}{solution_file}
% sol --- имя окружения внутри задач
% solution --- имя окружения внутри solution_file
% solution_file --- имя файла в который будет идти запись решений
% можно изменить далее по ходу
\Opensolutionfile{solution_file}[all_solutions]
% в квадратных скобках фактическое имя файла



% магия для автоматических гиперссылок задача-решение
\newlist{myenum}{enumerate}{3}
% \newcounter{problem}[chapter] % нумерация задач внутри глав
\newcounter{problem}[section]

\newenvironment{problem}%
{%
\refstepcounter{problem}%
%  hyperlink to solution
     \hypertarget{problem:{\thesection.\theproblem}}{} % нумерация внутри глав
     % \hypertarget{problem:{\theproblem}}{}
     \Writetofile{solution_file}{\protect\hypertarget{soln:\thesection.\theproblem}{}}
     %\Writetofile{solution_file}{\protect\hypertarget{soln:\theproblem}{}}
     \begin{myenum}[label=\bfseries\protect\hyperlink{soln:\thesection.\theproblem}{\thesection.\theproblem},ref=\thesection.\theproblem]
     % \begin{myenum}[label=\bfseries\protect\hyperlink{soln:\theproblem}{\theproblem},ref=\theproblem]
     \item%
    }%
    {%
    \end{myenum}}
% для гиперссылок обратно надо переопределять окружение
% это происходит непосредственно перед подключением файла с решениями





\begin{document}

\maketitle % ставим сюда название, автора и время создания

% здесь нужна прикольная картинка

\newpage
\tableofcontents{}

\newpage


\section{First step analysis}

\begin{problem}
Biden and Trump alternately throw a fair dice infinite number of times. 
Biden throws first. 
The person who obtains the first $6$ wins the game. 

\begin{enumerate}
  \item What is the probability that Biden will win?
  \item What is expected number of turns?
  \item What is variance of the number of turns?
  \item What is expected number of turns given that Biden won?
  \item Find the transition matrix of this four state Markov chain. 
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item $\P(B) = 6/11$, first step equation for $p = \P(B)$ is
  $p = 1/6 + (5/6)^2 p$ or $p = 1/6 + 5/6 \cdot (1 - p)$.
  \item $\E(N) = 6$, first step equation for $m = \E(N)$ is $m = 1/6 + 5/6 (m + 1)$.
  \item $\E(N^2) = 66$, $\Var(N) = 30$, first step equation is $\E(N^2) = 1/6 + 5/6 \E((N + 1)^2)$.
  \item $\E(N \mid B) = 61/11$. Start by replacing uncoditional probabilities on the tree by conditional ones. 
  First step equation for $\mu = \E(N \mid B)$ is $\mu = 11/36 + 25/36 (\mu + 2)$.
  \item $\begin{pmatrix}
    0 & 5/6 & 1/6 & 0 \\
    5/6 & 0 & 0 & 1/6 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
  \end{pmatrix}$
\end{enumerate}

\end{sol}
\end{problem}



\begin{problem}
  Elon throws an unfair coin until ``head'' appears. 
  The probability of ``head'' is $p \in (0;1)$. 
  Let $N$ be the total number of throws. 
  \begin{enumerate}
    \item Find $\E(N)$, $\Var(N)$, $\E(N^3)$, $\E(\exp(tN))$.
    \item What is the probability than $N$ will be even?
  \end{enumerate}

  \begin{sol}
    \begin{enumerate}
      \item $\E(N) = 1/p$, $\Var(N) = $, $\E(N^3) = $, $\E(\exp(tN)) = $
      \item $a = \P(N \in 2 \cdot \NN)$, $a = (1-p)(1-a)$, $a = (1 - p) / (2 - p)$.
    \end{enumerate}
    
  \end{sol}
\end{problem}
  


\begin{problem}
  Alice and Bob throw a fair coin until the sequence $HTT$ or $THT$ appears.
  Alice wins if $HTT$ appears first, Bob wins if $THT$ appears first. 
  \begin{enumerate}
    \item Find the probability that Alice wins.
    \item Find the expected value and variance of the total number of throws. 
    \item Using any open source software find the probability that Alice wins for all possible combinations 
    of three coins sequences for Alice and Bob. 
    \item Now Alice and Bob play the following game. 
    Alice chooses her three coins winning sequence first. Next Bob, knowing the choice of Alice, chooses his three coins winning sequence. 
    Than they throw a fair coin until either of their sequences appears. 
    What is the best strategy for Alice? For Bob? What is the probability that Alice wins this game?
  \end{enumerate}
  
  \begin{sol}
  
  \end{sol}
\end{problem}

\begin{problem}
You throw a dice unbounded number of times. 
If it shows $1$, $2$ or $3$ then the corresponding amount of dollars 
is added in the pot. 
It it shows $4$ or $5$ the game stops and you get the pot with money. 
If it shows $6$ the game ends and you get nothing. 
Initially the pot is empty. 

\begin{enumerate}
  \item What is probability that the game will end by $6$?
  \item What is expected duration of the game?
  \item What is your expected payoff?
  \item What is your payoff variance?
  \item Consider variation-A of the game. 
  Rules are the same, but initially the pot contains $100$ dollars. 
  How will the answers to questions (a)-(d) change?
  \item Consider variation-B of the game. 
  Initially the pot is empty. One rule is changed. 
  If the dice shows $5$ the content of the pot is burned and the game continues. 
  How will the answers to questions (a)-(d) change?
\end{enumerate}

  \begin{sol}
  Let's denote the throws by $(X_t)$ and the number of throws by $T$.
  Thus the last throw is $X_T$. 
    \begin{enumerate}
      \item $\P(X_T = 6) = 1/3$ as we have three possible endings. 
      One may also sum the probability geometric serie or use first step analysis.
      \item $\E(T) = 0.5 + 0.5 (\E(T) + 1)$;
      \item Let $\mu = \E(S)$ and $\gamma = \P(X_T \in \{4, 5\})$.
      \[
      \mu = \frac{3}{6}\cdot 0 + \frac{1}{6}(\mu + 1\cdot \gamma) + \frac{1}{6}(\mu + 2\cdot \gamma) + \frac{1}{6}(\mu + 3\cdot \gamma)
      \]
      \item 
      \item 
      \item 
      \[
      \mu_B = \frac{2}{6}\cdot 0 + \frac{1}{6}\mu_B + \frac{1}{6}(\mu + 1\cdot \beta) + \frac{1}{6}(\mu + 2\cdot \beta) + \frac{1}{6}(\mu + 3\cdot \beta),
      \]
      with $\beta = 1/3$.
    \end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
Boris Johnson throws a fair coin until $1$ appears or until he says ``quit''.
His payoff is the value of the last throw. 
Boris optimizes his expected payoff. 
If many strategies gives the same expected payoff 
he chooses the strategy that minimizes the expected duration of the game. 

\begin{enumerate}
  \item What is the optimal strategy and the corresponding expected payoff?
  \item What is the expected duration?
  \item How the answers to points (a) and (b) will change
  if Boris should pay $0.3$ dollars for each throw?
  \end{enumerate}

  \begin{sol}
  
  \end{sol}
\end{problem}


\begin{problem}
  Winnie-the-Pooh starts wandering from the point $x=1$.
  Every minute he moves one unit left or one right with equal probabilities.

  Let $T$ be the random moment of time when he reaches $x=0$.

  \begin{enumerate}
   \item Find the generating function $g(u) = \E(u^T)$.
   \item Extract all probabilities $\P(T = k)$ from the function $g(u)$.
  \end{enumerate}

  \begin{sol}
  
  \end{sol}
\end{problem}


\begin{problem}
Gleb Zheglov catches one criminal every day. 
With probability $0.2$ the catched criminal is replaced by $w$ new criminals. 
Initially there are $n$ criminals in the town. 

What is the expected time to the ultimate crime eradication in the town?

\begin{enumerate}
  \item (4 points) Solve the problem for $w=1$ and $n=1$.
  \item (6 points) Solve the problem for arbitrary $w$ and $n$.
\end{enumerate}
  \begin{sol}
  
  \end{sol}
\end{problem}

\begin{problem}
  Consider infinite ladder with steps numbered from $0$ to infinity. 
  I start at step $0$. Every day with probability $u$ I go one step up.
  With probability $d$ I go one step down. With probability $1-u-d$ I stay on the same step.

  If I am at step $0$ then I stay there with probability $1-u$ because it's impossible to go down. 

  Consider the case $d>u$. 
  
  What is the probability that I will be at step $0$ after $10^{1000}$ days?
  \begin{sol}
  
  \end{sol}
\end{problem}

\begin{problem}
  I throw a fair die until the sequence 626 appears. Let $N$ be the number of throws.
  \begin{enumerate}
      \item What is the expected value $\E(N)$?
      \item Write down the system of linear equations for the moment generating function of $N$. You don't need to solve it!
  \end{enumerate}
  \begin{sol}
  
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
  
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
  
  \end{sol}
\end{problem}




\begin{problem}
  \begin{sol}
  
  \end{sol}
\end{problem}



\begin{problem}
  \begin{sol}
  
  \end{sol}
\end{problem}



\begin{problem}
  \begin{sol}
  
  \end{sol}
\end{problem}






\section{Markov chains}

\begin{problem}
HSE student lives in two states: "sleep" and "study" and tries to change the state every 1 hour. 
After the sleep state the student continues sleeping with probability equal to 0.25, 
otherwise a student starts studying. 
If the student is studying, the probabilities to continue studying and to start sleeping are equal.

\begin{enumerate}
\item Write down the transition matrix of this Markov chain.
\item Draw the graph representation.
\item What is the probability that a Sleeping Student will be a Studying Student after 1 hour? After 2 hours?
\item We know that initially student is sleeping with probability $p=\frac{2}{3}$. 
Find the probabilities of sleep and study states after 1 and 2 hours. 
\item Find the probabilities of sleep and study states after 20 and 100 hours (do it with \textbf{matrix} operations and any soft). 
Is there any difference and why?  
\end{enumerate}

\begin{sol}
  
\end{sol}

\end{problem}

\begin{problem}
HSE student has three states: pre-coffee, with-coffee and over-coffee. 
He goes to Jeffrey's each break seeking for a cup of coffee. 
The line is usually too long, so probability to stay pre-coffee is equal to 60\% and to be over-coffee — is zero. 
Caffeinated students can stay in lines longer, so for with-coffee student the probability to become over-coffee is 20\% and to become pre-coffee — 30\%. 
Over-coffee student runs to coffeeshop very fast and able to stay over-coffeed with $p = 0.70$ 
and can suddenly become pre-coffee with $p = 0.10$. 

\begin{enumerate}
\item Draw the graph representation of this Markov chain.
\item Write the transition matrix of this Markov chain.
\item What is the probability that morning pre-coffee student will be with-coffee after 1 break? After 3 breaks?
\item What is the probability that morning pre-coffee student will be with-coffee after 1 break? After 3 breaks? After 200 breaks?
\end{enumerate}

\begin{sol}
  
\end{sol}
\end{problem}


\begin{problem}
Unteachable students in NOTHSE University try to pass the exams. 
Students cheat successfully and pass the exams with probability 10\%. 
In the case of a failure students are allowed to infinite number of retakes. 
All students are unteachable so the amount of knowledge is always the same and doesn't depend of the number of retakes.

\begin{enumerate}
\item Draw the graph representation of this Markov chain.
\item What is the probability to graduate using no more than 5 retakes? 
\item What is the probability to graduate eventually? 
\item Use \textbf{first step analysis} to find the average number of retakes per student in this University.
\end{enumerate}

\begin{sol}
  
\end{sol}

\end{problem}

\begin{problem}
Every month the real estate Galina agent has two options: to increase her commission and to ask an owner to increase the rent. 
If the agent has increased the commission, on the next step she increases the commission again with probability $5/8$. 
If she has asked the owner, she decides to increase the commission with probability equal to $3/4$ on the next step.

\begin{enumerate}
\item Write the transition matrix of this Markov chain.
\item Draw the graph representation.
\item Use \textbf{first step analysis} to find how many steps the agent does between asking the owner to increase the price.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
Alice and Bob toss a coin, writing down the results.
 If the last 3 tosses are Head, Head and Tail, Alice wins. 
 If the last 3 tosses are Tail, Head and Head, Bob wins.

\begin{enumerate}
\item Is it easy to work with matrix representation in this case?
\item Draw the graph representation. Who is more likely to win the game?
\item Use \textbf{first step analysis} to find the probability of Alice's win.
\item Find the probability that the game ends in exactly 4 tosses.
\item Find the expected value and variance of the total number of coin throws in the game.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
HSE student has an unusually caring granny who cooks one pie with probability $0.7$ every weekend. 
Granny's pies are so tasty that HSE student can't resist and he gains 1 kilo for each pie eaten. 
Without pies the student with more than 70 kilos weight looses 1 kilo per week, yeah, he has a lot of studies!
At the beginning of the study year student's weight is $W_0 = 70$ kilos.

Let $W_t$ be the weight of the student $t$ weeks later. 

\begin{enumerate}
\item Find the probability $\P(W_3 \geq 71)$ and expected value $\E(W_3)$.
\item Find the limit weight after infinitely many study weeks $\lim_{t\to\infty} W_t$.
\item Explain whether the chain $(W_t)$ has a stationary distribution.
\end{enumerate}

\begin{sol}
\begin{enumerate}
    \item 
    \item $\E(W_t) \to + \infty$;
    \item No stationary distribution. For stationary distribution $\E(W_t)$ can't tend to infinity. 

\end{enumerate}

\end{sol}
\end{problem}


\begin{problem}
The fair price of Sborbank in discrete stock market is somewhere between 100 and 101 rubles. 
If the price is equal to 100, then the price grows up by 1 ruble with probability $\frac{9}{10}$, 
otherwise it goes down by 1 ruble. 
If the price is greater then 100, it grows by 1 ruble with probability $\frac{1}{3}$ or declines by 1 ruble. 
If the price is lower than 100, it grows by 1 ruble with probability $\frac{2}{3}$ or declines by 1 ruble. 

\begin{enumerate}
\item Draw the graph representation of the corresponding Markov chain.
\item Do you think this chain has some stationary distribution?
\item Find the average time for the stock price to fall from 102 rubles to 98 rubles. 

Hint: you may to decompose the long path into smaller ones and to use the first step analysis.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
The hedgehog Melissa starts at the vertex $A$ of a triangle $\Delta ABC$.
Each minute she randomly moves to an adjacent vertex with probabilities $\P(A \to B) = 0.7$, 
$\P(A \to C) = 0.3$, $\P(B \to C) = \P(B \to A) = 0.5$,  $\P(C \to B) = \P(C \to A) = 0.5$.

\begin{enumerate}
  \item What is the probability that she will be in vertex $B$ after 3 steps?
  \item Write down the transition matrix of this Markov chain. 
  \item What proportion of time Melissa will spend in each state in the long run?
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}
  


\begin{problem}
A Hedgehog starts at the point $x=2$ on the real line. 
Every minute he moves one step left with probability $0.3$ or one step right with probability $0.7$.
There are two exceptions from this rule: the absorbing point $x=0$ and the reflecting barrier at $x=3$.

If the Hedgehog reaches the absorbing point $x=0$ then he stops moving and stays there. 
If the Hedgehog reaches the reflecting barrier $x=3$ then his next move will be one step left with probability $1$.

\begin{enumerate}
\item {[2]} Write the transition matrix of this Markov chain. 
\item {[3]} What is the probability that Hedgehog will be at $x=1$ after exactly 3 steps?
\item {[5]} What is the expected time to reach the absorbing state?
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Vampire Petr drinks blood of a new victim every day. 
Unfortunately 20\% of the population are vaccinated against vampires. 
If more than one victim of the last three victims are vaccinated then Petr will be instantaneously cured and will return to the normal life. 

For simplicity let's assume that the last three victims were not vaccinated. 

\begin{enumerate}
    \item What is the probability that vampire Petr will be cured in the next three days?
    \item How many victims will be bitten by vampire Petr on average?
\end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}
  


\begin{problem}
  A hedgehog moves at random on the vertices $A$, $B$, $C$ and $D$ of a regular tetrahedron (тетраэдр).
  She start at the vertex $A$ and every minute changes her position to one of the adjacent vertices with probability $1/3$
  independently of past moves. 
  
  \begin{enumerate}
      \item Write down the transition matrix of this Markov chain. 
      \item What is the expected time of the first return to the starting vertex $A$?
  \end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}



\section{Classification of states}

\begin{problem}
We randomly wander on the graph choosing at each moment of time one of the possible directions.
If probabilities are not given we choose equiprobably.   

\begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
  \node [circle, draw] (A) {$a$};
  \node [circle, draw] (B) [right of=A] {$b$};
  \node [circle, draw] (C) [right of=B] {$c$};
  \node [circle, draw] (D) [below of=A] {$d$};
  \node [circle, draw] (E) [right of=D] {$e$};
  \node [circle, draw] (F) [right of=E] {$f$};

  \path (A) edge (D);
  \path (B) edge (A);
  \path (B) edge (C);
  \path (D) edge (B);
  \path (C) edge (F);
  \path (B) edge (F);
  \path (E) edge [bend left] (F);
  \path (F) edge [bend left] (E);



  % \path (one) edge [bend left] node [above] {$0.1$} (two);
  %\path (two) edge [bend left] node [below]{$0.2$} (one);
  %\path (two) edge [loop right] node {} (two);
  %\path (one) edge [loop left] node {} (one);
\end{tikzpicture}


\begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
  \node [circle, draw] (A) {$a$};
  \node [circle, draw] (B) [right of=A] {$b$};
  \node [circle, draw] (C) [below of=B] {$c$};
  \node [circle, draw] (D) [below of=A] {$d$};
  \node [circle, draw] (E) [right of=B] {$e$};

  \path (A) edge [bend left] (B);
  \path (B) edge [bend left] (A);
  \path (B) edge [bend left] (C);
  %\path (A) edge (C);
  \path (C) edge [bend left] (B);
  \path (C) edge [bend left] (D);
  \path (D) edge [bend left] (C);
  \path (D) edge [bend left] (A);
  \path (A) edge [bend left] (D);

  \path (B) edge (E);
  \path (E) edge [loop right] (E);
\end{tikzpicture}


\begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
  \node [circle, draw] (A) {$a$};
  \node [circle, draw] (B) [right of=A] {$b$};
  \node [circle, draw] (C) [right of=B] {$c$};
  \node [circle, draw] (D) [below of=A] {$d$};
  \node [circle, draw] (E) [right of=D] {$e$};
  \node [circle, draw] (F) [right of=E] {$f$};
  \node [circle, draw] (G) [right of=C] {$g$};


  \path (A) edge (D);
  \path (D) edge (E);
  \path (E) edge (A);
  \path (B) edge (E);
  \path (B) edge (C);
  \path (C) edge (F);
  \path (C) edge (G);
  \path (F) edge (B);


  \path (D) edge [loop left] (D);
  \path (E) edge [loop right] (E);
  \path (B) edge [loop left] (B);
  \path (G) edge [loop below] (G);
\end{tikzpicture}


\begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
  \node (A) {\dots};
  \node [circle, draw] (B) [right of=A] {$-1$};
  \node [circle, draw] (C) [right of=B] {$0$};
  \node [circle, draw] (D) [right of=C] {$1$};
  \node (E) [right of=D] {\dots};

  \path (A) edge [bend left] node [above] {$0.1$} (B);
  \path (B) edge [bend left] node [below] {$0.9$} (A);
  \path (B) edge [bend left] node [above] {$0.1$} (C);
  \path (C) edge [bend left] node [below] {$0.9$} (B);
  \path (C) edge [bend left] node [above] {$0.1$} (D);
  \path (D) edge [bend left] node [below] {$0.9$} (C);
  \path (D) edge [bend left] node [above] {$0.1$} (E);
  \path (E) edge [bend left] node [below] {$0.9$} (D);
\end{tikzpicture}



\begin{enumerate}
  \item Split each Markov chain into communicating classes. 
  \item Find the period of every state. 
  \item Classify each state as transient, null-recurrent and positive recurrent.
  \item For positive recurrent states find the expected return time.
  \item Find all stationary distributions. 
\end{enumerate}


  \begin{sol}
    
  \end{sol}
\end{problem}


\begin{problem}
  A Knight randomly wanders on the chessboard. 
  At each step he randomly chooses one of the possible Knight-moves with equal probabilities. 
  
\begin{enumerate}
    \item Find the stationary distribution. 
    \item Find the expected return time for every square.
    \item Find the period of every square. 
\end{enumerate}
  
\begin{sol}
      
\end{sol}
\end{problem}
  
\begin{problem}
Consider the Markov chain with the transition matrix
\[
  P = \begin{pmatrix}
    0.2 & 0.2 & 0 & 0.6 \\
    0.3 & 0.3 & 0.4 & 0 \\
    0 & 0 & 0.1 & 0.9 \\
    0 & 0 & 0.8 & 0.2 \\
  \end{pmatrix}.
\]

\begin{enumerate}
  \item (3 points) Split the chain in classes and classify them into closed or not closed.
  \item (2 points) Classify the states into recurrent or transient.
  \item (5 points) A Hedgehog starts in the state one and moves 
  randomly between states according to the transition matrix.

  What is the approximate probability that the Hedgehog will be in the 
  state four after $10^{2021}$ moves?
\end{enumerate}

\begin{sol}
      
\end{sol}
\end{problem}



\begin{problem}
  Consider the Markov chain with the transition matrix
\[
  P = \begin{pmatrix}
    0.2 & 0.2 & 0 & 0.6 & 0 \\
    0.3 & 0.3 & 0.4 & 0 & 0\\
    0 & 0 & 0.3 & 0.7 & 0 \\
    0 & 0 & 0.8 & 0.2 & 0 \\
    0 & 0 & 0 & 0 & 1 \\
  \end{pmatrix}.
\]

\begin{enumerate}
  \item (3 points) Split the chain in classes and classify them into closed or not closed.
  \item (2 points) Classify the states into recurrent or transient.
  \item (5 points) A Hedgehog starts in the state one and moves 
  randomly between states according to the transition matrix.

  What is the approximate probability that the Hedgehog will be in the 
  state four after $10^{2021}$ moves?
\end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
      
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
      
  \end{sol}
\end{problem}



\section{Generating functions}

\begin{problem}
The MGF (moment generating function) of the random variable $X$ is give by $M(t) = 0.3 \exp(2t) + 0.2 \exp(3t) + 0.5 \exp(7t)$.

Recover the distribution of the random variable $X$.
  \begin{sol}

  \end{sol}
\end{problem}



\begin{problem}
The random variable $Y$ takes values $1$, $2$ or $3$ with equal probabilities.

Find the MGF of the random variable $Y$.
  \begin{sol}
  $M(t) = (\exp(t) + \exp(2t) + \exp(3t))/3$.
  \end{sol}
\end{problem}


\begin{problem}
The MGF of the random variable $W$ has a Taylor expansion that starts with $M(t) = 1 + 2t + 7t^2 + 20t^3 + \ldots$.

Find $\E(W)$, $\Var(W)$, $\E(W^3)$.

  \begin{sol}
  $\E(W) = 2$, $\Var(W) = 7\cdot 2 - 2^2$, $\E(W^3) = 20 \cdot 3!$.
  \end{sol}
\end{problem}


\begin{problem}
The random variable $X$ takes non-negative integer values.
The generating function $g(u) = \E(u^X)$ has a Taylor expansion that
starts with $g(u) = 0.1 + 0.2 u + 0.15 u^2 + \ldots$.

Find probabilities $\P(X = 0)$, $\P(X = 1)$, $\P(X = 2)$.

  \begin{sol}
  $\P(X = 0) = 0.1$, $\P(X = 1) = 0.2$, $\P(X = 2) = 0.15$.
  \end{sol}
\end{problem}




\begin{problem}
  Random variables $X_i$ are mutually independent and $X_i$ has Gamma distribution $\dGamma(\alpha_i, \beta_i)$.

  I sum up the random number $N$ of terms,
  \[
   S = X_1 + X_2 + \ldots + X_N.
  \]
  The number $N$ has Poisson distribution $\dPois(\lambda)$ and is independent of the sequence $(X_i)$.

  \begin{enumerate}
   \item Find the MGF of $S$. You may the MGF formula for Gamma distribution as known.
   \item Find $\E(S)$ and $\Var(S)$.
  \end{enumerate}

  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  The random variable $X$ take non-negative integer values.
  Its moment generating function is equal to $M(t) = (2 - \exp(t))^{-7}$.

  \begin{enumerate}
   \item Find the probability generating function $g(u) = \E(u^X)$.
   \item Find $\E(X)$, $\Var(X)$, $\P(X = 0)$, $\P(X = 1)$, $\P(X = 2)$.
   \item Find $\P(X = k)$.
  \end{enumerate}


  \begin{sol}
      $g(u) = g(\exp(t)) = \E(\exp(tX)) = M(t)$
  \end{sol}
\end{problem}


\begin{problem}
The number of players $N$ who will win the lottery
is a random variable with probability mass function $\P(N = k) = 7\cdot 0.3^k / 3$ for $k\geq 1$.
Each player will get a random prize $X_i \sim U[0;1]$.
All random variables are independent. 
Let $S$ be the sum of all the prizes. 

\begin{enumerate}
  \item Find $\E(S \mid N)$ and conditional moment generating function $M_{S\mid N}(u)$.
  \item Find the unconditional moment generating function $M_S(u)$.
  \item What is the probabilistic meaning of $M_S''(0) - (M_S'(0))^2$? 
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
Prince Myshkin throws a fair coin until two consecutive heads appear. 
Let $N$ be the number of throws. 
  
Find the moment generating function of $N$. 
  \begin{sol}
    
  \end{sol}
\end{problem}

\begin{problem}
  The moment generating function of a random variable $X$ is $1/(1-2t)$.
  \begin{enumerate}
      \item Find the moment generating function of $2X$.
      \item Find the moment generating function of $X + Y$ where $X$ and $Y$ are independent and identically distributed.
      \item Do you remember the sum of geometric progression? Find $\E(X^{2021})$.
  \end{enumerate}
  \begin{sol}
    
  \end{sol}
\end{problem}

\begin{problem}
  The ultimate goal of this exercise is to prove the good upper bound for tail probability of a normal distribution: 
  if $X\sim \cN(0; \sigma^2)$ then $\P(X > c) \leq \exp(-c^2/2\sigma^2)$.
  
  Here are the guiding hints (you free to use not use them): 
  
  \begin{enumerate}
    \item State the MGF of $X$. You may derive it or simply write it if you remember.
    \item Consider $Y = \exp(uX)$. Using Markov inequality provide the upper bound for $\P(Y > \exp(uc))$.
    \item Prove that $\P(X > c) \leq MGF_X(u)\exp(-uc)$ for any $u$.
    \item Find the value of $u$ that makes the upper bound as tight as possible. 
  \end{enumerate}
  \begin{sol}
    
  \end{sol}
\end{problem}

\begin{problem}
  I have an unfair coin with probability of heads equal to $h \in (0;1)$.
  \begin{enumerate}
      \item Let $N$ be the number of tails before the first head. Find the MGF of $N$.
      \item Let $S$ be the number of tails before $k$ heads (not necessary consecutive). Find the MGF of $S$.
      \item What is the limit of $MGF_S(t)$ when $k \to \infty$ and $k \times h \to 0.5$? What is the name of the corresponding distribution?
  \end{enumerate}
  \begin{sol}
    
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}



\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}



\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}





\section{Limits}

\begin{problem}
Polina loves sweet chestnuts. 
She has infinite sequence of baskets before her.
In the basket number~$n$ there are $n$~chestnuts in total.
Unfortunately only one chestnut in every basket is a sweet one. 

She picks chestnuts one by one at random from all the buskets sequentially. 
First she picks the unique chestnut from the basket number one, 
than she picks in a random order two chestnuts from the basket number two and so on. 

The random variable $S_t$ indicates whether the chestnut number $t$ was a sweet one. 

\begin{enumerate}
  \item Find $\lim S_t$ or prove that the limit does not exist.
  \item Find $\plim S_t$ or prove that the limit does not exist.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Let $(X_n)$ be independent, each variable $X_n$ has exponential distribution with rate $\lambda_n = n$.
\begin{enumerate}
  \item Find the probability limit $\plim X_n$ or prove that it does not exist.
\end{enumerate}
Let $(Y_n)$ be independent, each variable $Y_n$ has exponential distribution with rate $\lambda_n = n / (n + 1)$.
\begin{enumerate}[resume]
  \item Find the probability limit $\plim Y_n$ or prove that it does not exist.
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item $\plim X_n = 0$;
  \item $\plim Y_n$ does not exist.
\end{enumerate}

\end{sol}
\end{problem}


\begin{problem}
Let $(X_n)$ be independent normally distributed $\cN(5; 10)$.

\begin{enumerate}
  \item Find the probability limit 
  \[
  \plim \frac{X_1 + X_2 + \dots + X_n}{7n}.
  \]
  \item Find the probability limit 
  \[
  \plim \frac{X_1^2 + X_2^2 + \dots + X_n^2}{7n}.
  \]
  \item Find the probability limit 
  \[
  \plim \ln(X_1^2 + X_2^2 + \dots + X_n^2) - \ln n.
  \]
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item $\plim \frac{X_1 + X_2 + \dots + X_n}{7n} = 5/7$;
  \item $\plim \frac{X_1^2 + X_2^2 + \dots + X_n^2}{7n} = 5$;
  \item $\plim \ln(X_1^2 + X_2^2 + \dots + X_n^2) - \ln n = \ln 35$.
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
  Let $(X_n)$ be independent uniform on $[0; 1]$.
  Let $Y_n = X_n^2 + X_n^3$.

  
  \begin{enumerate}
    \item Find the probability limit $\plim V_n$ for 
    \[
    V_n = \max\{Y_1, Y_2, \dots, Y_n\}.
    \]
    \item Find the probability limit $\plim W_n$ for 
    \[
    W_n = \max\{X_1 + Y_1, X_1 + Y_2, \dots, X_1 + Y_n\}.
    \]
  \end{enumerate}
  \begin{sol}
  \begin{enumerate}
    \item $\plim \max\{Y_1, Y_2, \dots, Y_n\} = 2$;
    \item $\plim \max\{X_1 + Y_1, X_1 + Y_2, \dots, X_1 + Y_n\} = X_1 + 2$.
  \end{enumerate}
  \end{sol}
  \end{problem}

\begin{problem}
Consider the random variable $X$ and the sequence of random variables $Y_n$ with $\E(Y_n) = \frac{1}{n}$ 
and $\Var(Y_n) = \frac{\sigma^2}{n}$. 
Let $W_n = X + Y_n$.

\begin{enumerate}
        \item Find the probability limit $\plim Y_n$;
        \item Find the probability limit $\plim W_n$.
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\plim Y_n = 0$;
    \item $\plim W_n = X$.
  \end{enumerate}
  \end{sol}
\end{problem}
  

\begin{problem}
  The random variables $X_i$ are independent and uniformly distributed on $[0; 1]$.  
  Let $Y_n = \min{X_1, \ldots X_n}$. 

  \begin{enumerate}
        \item Find the almost sure limit of $Y_n$;
        \item Find the probability limit of $Y_n$;
        \item Find the limiting distribution of $Y_n$.
  \end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\P(\lim Y_n = 0 )=1$;
    \item $\plim Y_n = 0$;
    \item Limiting distribution is a constant $0$.
\end{enumerate}
\end{sol}

\end{problem}


\begin{problem}
Let $X$ and $Y$ be independent and uniformly distributed on $[0; 1]$.
Let $V_n = n^2 Y \cdot I(X \leq 1/n)$ and $W_n = Y \cdot I(X > 1/n)$. 
  \begin{enumerate}
        \item Find $\plim V_n$ and $\plim W_n$.
        \item Does $(V_n)$ converge in mean squared?
        \item Does $(W_n)$ converge in mean squared?
  \end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\plim V_n = 0$, $\plim W_n = Y$;
    \item The sequence $V_n$ does not converge in mean squared;
    \item  $W_n$ converges to $Y$ in mean squared.
\end{enumerate}
\end{sol}

\end{problem}


\begin{problem}
  \begin{enumerate}
    \item As a warm-up find the limit
    \[
    \lim_{n\to\infty} \frac{2n^2 + 3n + 6}{5n^2 + 2n + 9}.
    \]
  \end{enumerate}
  Now consider the sequence with parameters:
  \[
  X_n = \frac{L n^2 + 3n + 6}{R n^2 + 2n + 9}
  \]
  \begin{enumerate}[resume]
    \item For each value of parameters $L$ and $R$ find the limit $\lim X_n$.
    \item Find the almost surely limit of $X_n$ if $L$ and $R$ are independent and $\dUnif[0;1]$.
    Does the pointwise limit exist?
    \item Random variables $L$ and $Q$ be independent and take values $0$ or $1$ with equal probabilities.
    Let $R = L + Q$. Find the almost surely limit of $X_n$ in terms of $L$ and $R$.
    Does the pointwise limit exist?
  \end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $2/5$;
    \item $L/R$ or $+\infty$ or $-\infty$ or $3/2$.
    \item Almost surely limit is $L/R$, pointwise limit does not exist.
    \item Pointwise and almost surely limits are $\frac{2}{5} I(R>0) + \frac{3}{2} I(R=0)$.
  \end{enumerate}
\end{sol}
\end{problem}

\begin{problem}
Consider the sequence $Y_n = U^n$ with parameter $U$.

\begin{enumerate}
  \item Find the ordinary limit of $Y_n$ for all values of $U$ for which the sequence converges.
  \item Find the almost surely limit of $Y_n$ if $U\sim \dUnif[0;1]$.
  \item What is the probability that $Y_n$ converges if $U \sim \dUnif[0;2]$?
  \item What is the probability that $Y_n$ converges if $U$ takes values $+1$ or $-1$ with equal probabilities?
  \item Does $Y_n$ converges in distribution if $U$ takes values $+1$ or $-1$ with equal probabilities?
\end{enumerate}
\begin{sol}
\begin{enumerate}
  \item $0$, $1$, $+\infty$ or does not exist. 
  \item Almost surely limit is $0$;
  \item $1/2$;
  \item $\P(Y_n \text{ converges }) = 0$;
  \item Yes, as every $Y_n$ has the same distribution. 
\end{enumerate}
\end{sol}
\end{problem}

\begin{problem}
The random variables $X_i$ are independend and uniformly distributed on $[0;2]$.
Find the probability limit
\[
\plim_{n\to\infty}  \max \left\{ \frac{\sum_{i=1}^{10} X_i}{n}, \frac{\sum_{i=1}^n X^3_i}{n+1} \right\}.
\]
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
The random variables $X_i$ are independend and uniformly distributed on $[0;1]$.
Find the probability limit
\[
\plim_{n\to\infty}  \max \left\{ \frac{\sum_{i=1}^n X_i}{n}, \frac{2\sum_{i=1}^n X^2_i}{n} \right\}.
\]
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
The random variables $X_i$ are independend and uniformly distributed on $[0;2]$.
Find 
\[
  \plim_{n\to\infty}  \frac{(X_1 - \bar X)^3 + (X_2 - \bar X)^3 + \ldots + (X_n - \bar X)^3}{n + 2022}.
\]
\begin{sol}
\end{sol}
\end{problem}
  
\begin{problem}
Consider the stochastic process $(X_n)$, where $X_0$ is uniform on $[0;2]$ and
$X_n = (1 + X_{n-1}) / 2$.

\begin{enumerate}
  \item Find $\E(X_n)$ and $\Var(X_n)$.
  \item Find the probability limit $\plim X_n$.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}
  
\begin{problem}
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
\begin{sol}
\end{sol}
\end{problem}



\section{Conditional expected value without sigma-algebras}

\begin{problem}
We randomly uniformly select a point inside triangle $A = (6, 0)$, $B = (0, 2)$ and $O = (0, 0)$.
Let $(X, Y)$ be coordinates of this random point.

\begin{enumerate}
  \item Find conditional expected values $\E(Y \mid X)$ and $\E(X \mid Y)$.
  \item Find conditional variances $\Var(Y \mid X)$ and $\Var(X \mid Y)$.
\end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item $\E(Y \mid X) = 1 - X/6$ and $\E(X \mid Y) = 3 - 1.5 X$.
      \item $\Var(Y \mid X) = $, $\Var(X \mid Y) = $.
    \end{enumerate}        
  \end{sol}
\end{problem}


\begin{problem}
The pair of random variables $X$ and $Y$ has joint probability density
\[
f(x, y) = \begin{cases}
  x + y, \text{ if } x\in [0;1], y\in [0;1]; \\
  0, \text{ otherwise.}
\end{cases}
\]  

\begin{enumerate}
  \item Find the marginal densities $f_X(x)$ and $f_Y(y)$.
  \item Find the conditional densities $f(x \mid y)$ and $f(y \mid x)$.
  \item Find the conditional expected values $\E(Y \mid X)$ and $\E(X \mid Y)$.
  \item Find the conditional variances $\Var(Y \mid X)$ and $\Var(X \mid Y)$.
\end{enumerate}

  \begin{sol}
\begin{enumerate}
  \item 
  \[
  f(x) = \begin{cases}
    x + 0.5, \text{ if } x\in [0;1]; \\
    0, \text{ otherwise.}
  \end{cases}
  \]
  \item 
\[
f(x, y) = \begin{cases}
  (x + y) / (x + 0.5), \text{ if } x\in [0;1], y\in [0;1]; \\
  0, \text{ otherwise.}
\end{cases}
\]  
\item 
\[
\E(Y \mid X) = \frac{0.5X + 1/3}{X + 0.5}.
\]  
\item 

\end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
The random variables $X$ and $Y$ are independend with Poisson distribution with rate $\lambda = 1$.
Let $S = X + Y$.

\begin{enumerate}
  \item Find conditional probabilities $\P(X = x \mid S = s)$ and $\P(Y = y \mid S = s)$.
  \item Find conditional expected values $\E(X \mid S)$ and $\E(Y \mid S)$.
  \item Find conditional variances $\Var(X \mid S)$ and $\Var(Y \mid S)$.
  \item How the answers will change if $X \sim \dPois(\lambda_x)$ and $Y \sim \dPois(\lambda_y)$?
\end{enumerate}

  \begin{sol}
    \begin{enumerate}
      \item 
      \item $\E(X \mid S) = \E(Y \mid S) = S/2$;
      \item 
      \item  
    \end{enumerate}
    
  \end{sol}
\end{problem}
  



\begin{problem}
Let $X$ and $Y$ be independent and exponentially distributed with rate $\lambda = 1$ and $S = X + Y$.

\begin{enumerate}
  \item Find conditional densities $f(x \mid s)$ and $f(y \mid s)$.
  \item Find conditional expected values $\E(X \mid S)$ and $\E(Y \mid S)$.
  \item Find conditional variances $\Var(X \mid S)$ and $\Var(Y \mid S)$.
  \item Find $\Cov(X, Y \mid S)$ and $\Corr(X, Y \mid S)$.
  \item How the answers will change if $X \sim \dExpo(\lambda_x)$ and $Y \sim \dExpo(\lambda_y)$?
\end{enumerate}

  \begin{sol}
    \begin{enumerate}
      \item $X \mid S \sim \dUnif[0; S]$, $Y \mid S \sim \dUnif[0; S]$.
      \item $\E(X \mid S) = \E(Y \mid S) = S/2$;
      \item $\Var(X \mid S) = \Var(Y \mid S) = S^2 / 12$;
      \item  
    \end{enumerate}
    
  \end{sol}
\end{problem}


\begin{problem}
The random variable $X$ has Poisson distribution with rate $\lambda = 1$.
The random variable $Y$ has uniform distribution on $[1; 2]$. 
Random variables $X$ and $Y$ are independent. 

Find $\E(XY \mid X)$, $\Var(XY + X^3 \mid X)$, $\Cov(X, Y \mid X)$, $\Cov(XY, X^2Y \mid X)$.

\begin{sol}
    $\E(XY \mid X) = X\E(Y) = X/2$, $\Var(XY  + X^3\mid X) = X^2 \Var(Y) = X^2 /12$, $\Cov(X, Y \mid X) = 0$, $\Cov(XY, X^2Y \mid X) = X^3 \Var(Y) = X^3 / 12$
\end{sol}
\end{problem}


\begin{problem}
The random variables $X_1$ and $X_2$ are independent and normally distributed, 
$X_1 \sim \cN(1;1)$, $X_2 \sim \cN(2;2)$. 
I choose $X_1$ with probability $0.3$ and $X_2$ with probability $0.7$ without knowing their values.

Casino pays me the value $Y$ that is equal to the chosen random variable. 

Let the indicator $I$ be equal to $1$ if I choose $X_1$ and $0$ otherwise. 

\begin{enumerate}
  \item Express $Y$ in terms of $X_1$, $X_2$ and $I$.
  \item Find $\E(Y \mid I)$, $\Var(Y \mid I)$.
  \item Find $\E(Y)$ and $\Var(Y)$. 
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
A Hedgehog in the fog starts in $(0, 0)$ at $t=0$ and moves randomly with equal probabilities in four directions (north, south, east, west) by one unit every minute. 

Let $X_t$ and $Y_t$ be his coordinates after $t$ minutes and $S_t = X_t + Y_t$.

\begin{enumerate}
    \item Find $\E(X_2 \mid S_2)$;
    \item Find $\Var(X_2 \mid S_2)$.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
\begin{sol}
\end{sol}
\end{problem}
      



\section{Sigma-algebras and measurability}

\begin{leftbar}
Sigma-algebra generated by discrete random variable $X$, $\sigma(X)$ — the list of all events that can be stated using $X$.

Sigma-algebra generated by arbitrary random variable $X$, $\sigma(X)$ — the smallest list of events that satisfies two properties:
\begin{itemize}
  \item The list contains all events of the form $\{X \leq t\}$, that means one can compare $X$ with any number;
  \item If one takes countably many events from this list and does logical operations (union, complement, intersection) then one will obtain an event from the list.
\end{itemize}
\end{leftbar}


\begin{problem}
The random variable $X$ takes values $1$, $2$ and $-2$ with equal probabilities.

\begin{enumerate}
  \item Find the sigma-algebra $\sigma(X)$.
  \item How the answer will change if one modifies probability distribution of $X$?
  \item Find the sigma-algebra $\sigma(\abs{X})$.
  \item Foma knows $\abs{X}$ and Yeryoma knows $X^2$.
  What can one say about sigma-algebras that model their knowledge?
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item
\item Sigma-algebras do not depend on probabilities.
\item $\sigma(\abs{X}) = \{\emptyset, \Omega, \{\abs{X} = 2\}, \{X= 1\}\}$.
\item $\sigma(\abs{X}) = \sigma(X^2)$;
\end{enumerate}

\end{sol}
\end{problem}


\begin{problem}
Experiment may end by one of the six outcomes:

\begin{tabular}{*{4}{c}}
\toprule
& $X=-1$ & $X=0$ & $X=1$ \\
\midrule
$Y=0$ & 0.1 & 0.2 & 0.3  \\
$Y=1$ & 0.2 & 0.1 & 0.1  \\
\bottomrule
\end{tabular}

\begin{enumerate}
  \item Find expicitely the sigma-algebras $\sigma(X)$, $\sigma(Y)$, $\sigma(X \cdot Y)$, $\sigma(X^2)$, $\sigma(2X+3)$.
  \item How many elements are there in $\sigma(X, Y)$, $\sigma(X + Y)$, $\sigma(X, Y, X+Y)$?
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item
  \item $\card \sigma(X, Y) = 2^6$, $\card \sigma(X + Y) = 2^4$, $\card \sigma(X, Y, X+Y) = 2^6$.
\end{enumerate}

\end{sol}
\end{problem}




\begin{problem}
Let's look at the number of possible elements in a sigma-algebra.
\begin{enumerate}
  \item The random variable $X$ has five possible values.
How many events are there in $\sigma(X)$?
\item Can a sigma-algebra contain exactly $1000$ events? Exactly $1024$ events?
\end{enumerate}
Maria throws a coin $100$ times and remembers very well all the tosses.
\begin{enumerate}[resume]
  \item How many elementary outcomes are there in the probability space $\Omega$?
 \item How many events are there in a sigma-algebra that models Maria's knowledge?
\end{enumerate}



\begin{sol}
\begin{enumerate}
  \item $2^5$;
  \item Only $2^k$ or infinity;
  \item $2^{100}$;
  \item $2^{2^{100}}$.
\end{enumerate}
\end{sol}
\end{problem}





\begin{problem}
How sigma-algebras $\sigma(X)$ and $\sigma(f(X))$ are related? When they are equal?
\begin{sol}
In general $\sigma(f(X)) \subseteq \sigma(X)$; If $f$ is a bijection then $\sigma(f(X)) = \sigma(X)$.
\end{sol}
\end{problem}





\begin{problem}
How many different $\sigma$-algebras can be created using the set of outcomes $\Omega$ has three elements? And if $\Omega$ has four elements?

\begin{sol}
In the finite case sigma-algebra corresponds to partitions.
We get five sigma-algebras on a set of three elements and $15$ sigma-algebras on a set of four elements. 
These numbers are known as Bell numbers.
\end{sol}
\end{problem}



\begin{problem}
Provide an example of algebra that is not a $\sigma$-algebra.

\begin{sol}
Let $\Omega = \NN$, $\cA$ contains all finite sets and sets with finite complement.
\end{sol}
\end{problem}

\begin{problem}
Prove a statement or provide a counter-example:
\begin{enumerate}
  \item The intersection of two sigma-algebras is a sigma-algebra.
  \item If the intersection of two sigma-algebras is a sigma-algebra then one of them  is contained in the other one.
  \item The union of two sigma-algebras is a sigma-algebra.
  \item If the union of two sigma-algebras is a sigma-algebra then one of them  is contained in the other one.
\end{enumerate}


\begin{sol}
\begin{enumerate}
  \item The intersection of two sigma-algebras is always a sigma-algebra.
  \item The intersection of two sigma-algebras is always a sigma-algebra.
  \item The union of two sigma-algebras is not always a sigma-algebra.
  \item 
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
Let $\cF$ be some $\sigma$-algebra of subsets of $\Omega$ and $B\subseteq\Omega$. 
Consider the collection of sets $\cH=\{A: A\subseteq B \text{ or } B^{c}\subseteq A\}$.

Is $\cH$ a $\sigma$-algebra?

\begin{sol}
Yes. This is convinient do draw $\Omega$ as a segment. With «пескари» $A\subseteq B$ and «sharks» $A \supseteq B^{c}$.
\end{sol}
\end{problem}


\begin{problem}
Будем обозначать количество элементов множества с помощью $\card A$.
Рассмотрим подмножества натуральных чисел, $A \subseteq \mathbb{N}$.
Определим для подмножества плотность Чезаро (Cesaro density),
\[
\gamma(A)=\lim_{n\to \infty}\frac{\card (A\cap \{1,2,3, \ldots,n\})}{n}
\]
в тех случаях, когда этот предел существует.

Плотность Чезаро показывает, какую «долю» от всех натуральных чисел составляет указанное подмножество.
Обозначим с помощью $\cH$ все подмножества, имеющие плотность Чезаро.

\begin{enumerate}
\item Чему равна плотность Чезаро у нечётных чисел?
\item Приведите пример множества, у которого не определена доля Чезаро.
\item Верно ли, что у натуральных чисел, в записи которых присутствует
хотя бы одна единица, есть доля? Если да, то чему она равна?
\item Верно ли, что у натуральных чисел, в записи которых присутствует
ровно одна единица, есть доля? Если да, то чему она равна?
\item Верно ли, что $\cH$ — алгебра? Сигма-алгебра?
\end{enumerate}


\begin{sol}
Разобьем натуральный ряд на пары соседних чисел. Можно
так подобрать множества $A$ и $B$, что в каждом из них из каждой
пары взято только одно число.
Поэтому $\gamma(A)=\gamma(B)=\frac{1}{2}$. Подобрав
совпадение-несовпадение в паре, можно сделать так, что
$\gamma(A\cap B)$ не существует.
\end{sol}
\end{problem}


\begin{problem}
We throw a fair dice.
Let $Y$ be the indicator of a even score and $Z$ be the indicator of score bigger than $2$.
\begin{enumerate}
\item Find the sigma-algebra $\sigma(Z)$.
\item Find the sigma algebra $\sigma(Y\cdot Z)$.
\item How many elements are there in $\sigma(Y,Z)$?
\item How are related the $\sigma$-algebras $\sigma(Y\cdot Z)$ and $\sigma(Y,Z)$?
\end{enumerate}


\begin{sol}
  \begin{enumerate}
    \item $\sigma(Z) = \{\{Z =1\}, \{Z = 0\}, \Omega, \emptyset \}$.
    \item $\sigma(YZ) = \{\{YZ =1\}, \{YZ = 0\}, \Omega, \emptyset \}$.    
    \item $2^4$;
    \item $\sigma(Y\cdot Z) \subseteq \sigma(Y,Z)$.
    \end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
We throw a coin infinitely many times. 
Let $X_{n}$ be the indicator that the coin landed on Head at toss number $n$.
Consider a pack of $ \sigma$-algebras: $\cF_{n}:=\sigma(X_1, X_2, \ldots, X_n)$, $\cH_{n}:=\sigma(X_{n}, X_{n+1}, X_{n+2}, \ldots)$.

\begin{enumerate}
\item For each case provide two examples of $\sigma$-algebras that contain the corresponding event
\begin{enumerate}
\item $\{X_{37}>0 \}$;
\item $\{X_{37}>X_{2024} \}$;
\item $\{ X_{37}>X_{2024}>X_{12} \}$;
\end{enumerate}
  
\item Simplify experessions: $\cF_{11}\cap \cF_{25}$, $\cF_{11}\cup \cF_{25}$, $\cH_{11}\cap \cH_{25} $, $\cH_{11}\cup \cH_{25}$.

\item For each case provide two non-trivial examples (different from $\Omega$ and $\emptyset$) of an event $A$ such that

\begin{enumerate}
\item $A\in \cF_{2024}$;
\item $A\notin \cF_{2025}$;
\item $A \in \cH_{n}$ for all possible $n$;
\end{enumerate}


\end{enumerate}


\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
Правда ли равносильны три набора требований к списку множеств $\cF$?

Тариф «Классический»:
\begin{enumerate}
  \item $\Omega \in \cF$;
  \item Если $A\in \cF$, то $A^c \in \cF$;
  \item Если $A_1, A_2, A_3, \ldots, \in \cF$, то $\cup A_i \in \cF$.
\end{enumerate}


Тариф «Перевёрнутый»:
\begin{enumerate}
  \item $\emptyset \in \cF$;
  \item Если $A\in \cF$, то $A^c \in \cF$;
  \item Если $A_1, A_2, A_3, \ldots, \in \cF$, то $\cap A_i \in \cF$.
\end{enumerate}


Тариф «Хочу всё»:
\begin{enumerate}
  \item $\Omega \in \cF$, $\emptyset \in \cF$;
  \item Если $A\in \cF$ и $B\in \cF$, то $A \backslash B \in \cF$;
  \item Если $A_1, A_2, A_3, \ldots, \in \cF$, то $\cup A_i \in \cF$ и $\cap A_i \in \cF$.
\end{enumerate}


\begin{sol}
Yes!
\end{sol}
\end{problem}



\begin{problem}
Рассмотрим $\Omega=[0;1]$ и набор множества $\cF$ таких,
что либо каждое множество не более, чем счётно, либо дополнение к нему не более, чем счётно.

\begin{enumerate}
\item Верно ли, что $\cF$ — алгебра? $\sigma$-алгебра?
\item Придумайте $B\subset\Omega$, такое что $B \notin\cF$.
\end{enumerate}

\begin{sol}
Например, $B$ — Канторово множество, или, гораздо проще,
$B=[0;0,5]$. Оно само более чем счетно и дополнение к нему более
чем счетно.

Набор $\cF$ действительно $\sigma$-алгебра.
$\emptyset$ лежит в $\cF$, так как имеет ноль элементов.

Если $A$ не более чем счетно, то $A^{c}$ лежит в $\cF$,
так как дополнение к $A^{c}$ содержит не более чем счетное
число элементов.

Если дополнение к $A$ не более чем счетно, то $A^{c}$ лежит
в $\cF$, так как содержит не более чем счетное число элементов.

Проверяем счетное объединение $\bigcup_{i} A_{i}$.
Если среди $A_{i}$ встречаются только не более чем счетные, то и
их объединение — не более чем счетно.
Если среди $A_{i}$ встретилось хотя бы одно множество с не более
чем счетным дополнением, то $\bigcup_{i} A_{i}$ тоже будет
обладать не более чем счетным дополнением, так как объединение не
может быть меньше ни одного из объединяемых множеств.
\end{sol}
\end{problem}



\begin{problem}
В лесу есть три вида грибов: рыжики, лисички и мухоморы. Попадаются они равновероятно и независимо друг от друга. Маша нашла 100 грибов. Пусть $R$ — количество рыжиков, $L$ — количество лисичек, а $M$ — количество мухоморов среди найденных грибов.
\begin{enumerate}
\item Сколько элементов $ \sigma(R)$?
\item Сколько элементов $ \sigma(R,M)$?
\item Измерима ли $L$ относительно $ \sigma(R)$?
\item Измерима ли $L$ относительно $ \sigma(R,M)$?
\item Измерима ли $L$ относительно $ \sigma(R+M)$?
\item Измерима ли $L$ относительно $ \sigma(R-M)$?
\end{enumerate}

\begin{sol}
$ 2^{101} $, $2^{101\cdot 51}$,
\end{sol}
\end{problem}



\begin{problem}
Сейчас либо солнечно, либо дождь, либо пасмурно без дождя.
Соответственно, множество $\Omega$ состоит из трёх исходов, $\Omega=\{\text{солнечно},\text{дождь},\text{пасмурно}\}$.
Джеймс Бонд пойман и привязан к стулу с завязанными глазами, но он может на слух отличать, идёт ли дождь.
\begin{enumerate}
\item Как выглядит $\sigma$-алгебра событий, которые различает агент 007?
\item Как выглядит минимальная $\sigma$-алгебра, содержащая событие $A=\{\text{не видно солнце}\}$?
\item Сколько различных $\sigma$-алгебр можно придумать для данного $\Omega$?
\end{enumerate}
\begin{sol}
$\cF=\{\emptyset, \Omega, \{\text{дождь}\}, \{\text{солнечно}, \text{пасмурно}\}$. Всего есть $1+1+3=5$ $\sigma$-алгебр.
\end{sol}
\end{problem}

  

\begin{problem}
The random variables $X_i$ are independent and they take values $+1$ or $-1$ with equal probability. 
\begin{enumerate}
\item {[3]} Explicitely list all the events in sigma-algebra $\sigma(X_1 \cdot X_2)$.
\item {[3]} Pavel says that he knows only whether $X_1$ and $X_3$ are equal. 
How will you describe his knowledge with sigma-algebra?
\item {[4]} How many events are in the sigma-algebra $\sigma(X_1, X_1 + X_2, X_1 + X_2 + X_3)$?
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}
  

\begin{problem}
Vincenzo Peruggia makes attempts to steal the Mona Lisa painting until the first 
success. 
Each attempt is successful with probability $0.1$.

Let $X$ be the number of attempts and $Z = \min\{X, 5\}$.

\begin{enumerate}
  \item (5 points) How many events are in sigma-algebras $\sigma(Z)$ and $\sigma(X)$?
  \item (5 points) If possible provide an example of events $A$ and $B$ such that: $A\in \sigma(Z)$ but $A\not\in\sigma(X)$; $B\in \sigma(X)$ but $B\not\in\sigma(Z)$.
  \item (10 points) Find $\E(Z \mid X)$ and $\E(X \mid Z)$.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}
  

\begin{problem}
  Variables $X_1$, $X_2$, \ldots $X_{100}$ are independent and identically distributed
with mean $1$ and variance $2$. Each $X_i$ has only three possible values: 0, 1, and 2. 

\begin{enumerate}
  \item (5 points) How many events are in sigma-algebras $\sigma(X_1, X_2)$ and $\sigma(X_1 - X_2)$?
  \item (5 points) If possible provide an example of events $A$ and $B$ such that: $A\in \sigma(X_1, X_2)$ but $A\not\in\sigma(X_1 - X_2)$; $B\in \sigma(X_1 - X_2)$ but $B\not\in\sigma(X_1, X_2)$.
  \item (10 points) Find $\E(X_1 + \ldots + X_{100} \mid X_1 + \ldots + X_{50})$ and $\E(X_1 + \ldots + X_{50} \mid X_1 + \ldots + X_{100})$.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}
  
\begin{problem}
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
\begin{sol}
\end{sol}
\end{problem}
      



\section{Sigma-algebras and conditional expected value}


\begin{problem}
At time moment $t = 0$ in the casiono there are countably many players with perfect memory.
Let's number them as Miss First, Mister Second, etc. 


Time is discrete. 
Random variables $X_t$ are independent and take values $+1$ or $-1$ with equal probabilities.
At each moment of time $t > 0$ everybode gets $X_t$ roubles and than the player number $t$ leaves the casiono.

The cumulative sum $S_t = X_1 + \dots + X_t$ reaches its first local maximum at the random time $T$.
At time $T+1$ the dealer calls his friend Black Jack and says «It's time!»
They have agreed beforehand on the call time. 

Black Jack chases the player number $T$ and steals all his information before the police can intervent.  
Let's describe the information of Black Jack by sigma-algebra $\cF_{J}$ and the information of every player $t$ at the last moment in casino by $\cF_t$.

\begin{enumerate}
  \item Which sigma-algebras contain the event $\{T = 10\}$?
  \item Provide an example of two events from $\cF_J$ that do not enter in neither $\cF_t$.
  \item Find conditional expected values $\E(T \mid \cF_J)$, $\E(X_T \mid \cF_J)$, $\E(X_{T+1} \mid \cF_J)$.
  \item Find conditional expected values $\E(S_{T - 1} \mid \cF_J)$, $\E(S_T \mid \cF_J)$, $\E(S_{T+1} \mid \cF_J)$, $\E(S_{T+2} \mid \cF_J)$.
\end{enumerate}
Let's define $Y_{T - k}$ as 
\[
Y_{T - k} = \begin{cases}
  X_{T - k}, \text{ if } T - k > 0; \\
  0, \text{ otherwise.} 
\end{cases}
\]
\begin{enumerate}[resume]
  \item Find $\E(Y_{T - 10} \mid \cF_J)$.
  \item Find conditional expected values $\E(X_T \mid \cF_{10})$, $\E(X_{T+1} \mid \cF_{10})$.
  \item Find $\E(T \mid \cF_{10})$ and $\E(S_T \mid \cF_{10})$.
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item $\cF_J$, $\cF_{11}$, $\cF_{12}$, \dots
    \item $\{T \text{ is divisible by } 2\}$, $\{T \geq 3, X_{T-2} = 1\}$.
    \item $\E(T \mid \cF_J) = T$, $\E(X_{T} \mid \cF_J) = 1$, $\E(X_{T+1} \mid \cF_J) = -1$.
    \item $\E(S_T \mid \cF_J) = S_T$, $\E(S_{T+1} \mid \cF_J) = S_T - 1$, $\E(S_{T+2} \mid \cF_J) = S_T - 1$.
    \item $\E(Y_{T - k} \mid \cF_J) = Y_{T- k }$.
    \item $\E(X_T \mid \cF_{10}) = 1$, $\E(X_{T+1} \mid \cF_{10}) = -1$.
    \item $\E(T \mid \cF_{10}) = ... $, $\E(S_T \mid \cF_{10}) = ...$.  
\end{enumerate}
    
\end{sol}
\end{problem}


\begin{problem}
Bad police officers operate in groups of $1$, $2$ or $3$ people with probabilities $0.5$, $0.2$ and $0.3$. 
If you cross the road in the wrong place, they will catch you and demand a bribe $X$ of $1$, $5$ or 10 thousand rubles respectively. 

For each of the following cases write down the $\sigma$-algebra $\cF$ that models your information and calculate $\E(X \mid \cF)$.

  \begin{enumerate}
    \item you can see how many officers are going to stop you;
    \item they are sitting in the car and you don't know their number;
    \item it is dark and you can only say if it is one policeman or more than one. 
    \end{enumerate}
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
HSE student rolled the dice once. 
Find the $\sigma$-algebras that model the following situations:
\begin{enumerate}
    \item she only knows that the dice was rolled once;
    \item she knows the result of the roll;
    \item she observes the result of the roll but she is able to count only up to two.
\end{enumerate}
\begin{sol}
  Here $\Omega = \{1, 2, 3, 4, 5, 6\}$.
  \begin{enumerate}
    \item $\cF = \{\emptyset, \Omega\}$;
    \item $\cF = 2^{\Omega}$, this notation means «all subsets of $\Omega$».
    \item $\cF = \sigma(\{1\}, \{2\})$, eight events in total;
  \end{enumerate}

\end{sol}
\end{problem}

\section{Martinales}

\begin{leftbar}
\begin{itemize}
  \item \emph{Natural filtration} of a process $(X_n)$ is given by $\cF_n = \sigma(X_1, X_2, \dots, X_n)$.
  \item A process $(X_n)$ is a \emph{martingale} if $\E(X_{n + k} \mid X_n, X_{n-1}, \dots, X_1) = X_{n}$ for all $k \geq 1$.
  \item A process $(X_n)$ is \emph{adapted} to filtration $(\cF_n)$ if every random variable $X_n$ is measurable wrt to sigma-algebra $\cF_n$.
  \item A process $(X_n)$ is a \emph{martingale wrt to filtration} $(\cF_n)$ if $\E(X_{n + k} \mid \cF_n) = X_{n}$.
\end{itemize}
\end{leftbar}


\begin{problem}
Consider the sequence $(X_t)$ of independent identically distributed random variables with mean $\E(X_t) = 2$ and variance $\Var(X_t) = 3$.
Let's work with natural filtration $\cF_t = \sigma(X_1, X_2, \dots, X_t)$.

On the base of $(X_t)$ let's create more sequences: $S_t = X_1 + X_2 + \dots + X_t$, $W_t = S_t - 2t$ and $Y_t = W_t^2 - 3t$.

\begin{enumerate}
  \item Is $(X_t)$ a martingale with respect to $(\cF_t)$?
  \item Is $(S_t)$ a martingale with respect to $(\cF_t)$?
  \item Is $(W_t)$ a martingale with respect to $(\cF_t)$?
  \item Is $(Y_t)$ a martingale with respect to $(\cF_t)$?
  \item Is $(W_t)$ a martingale with respect to $(\cF_{t-1})$?
  \item Is $(W_t)$ a martingale with respect to $(\cF_{t+1})$?
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $(X_t)$ is not a martingale with respect to $(\cF_t)$;
    \item $(S_t)$ is not a martingale with respect to $(\cF_t)$;
    \item $(W_t)$ is a martingale with respect to $(\cF_t)$;
    \item $(Y_t)$ is a martingale with respect to $(\cF_t)$;
    \item $(W_t)$ is not a martingale with respect to $(\cF_{t-1})$;
    \item $(W_t)$ is not a martingale with respect to $(\cF_{t+1})$;
  \end{enumerate}  
\end{sol}
\end{problem}

\begin{problem}
Vasiliy has found three non-random infinite sequences in his garage: $a_n = n$, $b_n = -n$ and $c_n = 0$.
He randomly selects one of these sequences with equal probabilities and hence obtain a sequence of random variables $(X_n)$.
\begin{enumerate}
  \item What is the distribution of $X_7$?
  \item Find $\E(X_n)$ and $\Var(X_n)$.
  \item Is $(X_n)$ a Markov chain?
  \item Is $(X_n)$ a martingale?
  \item Explicitely find the $\sigma$-algebra $\sigma(X_1, X_2, X_3, \dots, X_{1000})$.
  \item Find the probability that limit of $(X_n)$ exists.
  \item Does $\plim X_n$ exist?
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item What is the distribution of $X_7$?
    \item $\E(X_n) = 0$ and $\Var(X_n) = $.
    \item The process $(X_n)$ is a Markov chain!
    \item The process $(X_n)$ is not a martingale.
    \item $\sigma(X_1, X_2, X_3, \dots, X_{1000}) = \sigma(X_1)$
    \item $\P(\lim X_n \text{ exists}) = 1/3$.
    \item $\plim X_n$ does not exist.
  \end{enumerate}  
\end{sol}
\end{problem}

\begin{problem}
Consider the sequence $(X_t)$ of independent identically distributed random variables
that take values $0$ or $1$ with equal probabilities.
Let's work with natural filtration $\cF_t = \sigma(X_1, X_2, \dots, X_t)$.

On the base of $(X_t)$ let's create more sequences: $S_t = X_1 + X_2 + \dots + X_t$, $W_t = S_t - at$, $M_t = \exp(bS_t)$.

\begin{enumerate}
  \item Is $(X_t)$ a martingale?
  \item Is $(S_t)$ a martingale?
  \item For which values of $a$ the process $(W_t)$ is a martingale?
  \item For which values of $b$ the process $(M_t)$ is a martingale?
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $(X_t)$ is not a martingale;
    \item $(S_t)$ is not a martingale;
    \item $a = 0.5$;
    \item $b = 0$ and $b = ...$
  \end{enumerate}  
\end{sol}
\end{problem}
  


\begin{problem}
Consider a well-mixed standard deck of $52$ cards. 
James Bond in an elegant outfit\footnote{Sponsors are wellcome to contact us for product placement!} 
opens cards one by one. 
Let the sigma-algebra $(\cF_n)$ model his information 
and $(X_n)$ be the proportion of Queens in the closed part of the deck after opening $n$ cards.

\begin{enumerate}
  \item Find the marginal distribution of $X_0$, $X_1$ and $X_{51}$.
  \item Find the joint distribution of $X_{50}$ and $X_{51}$.
  \item Is $(X_n)$ a martingale with respect to $(\cF_n)$?
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item $X_0 = 4/52$; $X_1$ is $4/51$ with probability $48/52$ or $3/51$ with probability $4/52$;
     $X_{51}$ is $1$ with probability $4/52$ and $0$ with probability $48/52$.
    \item 
    \item 
    With probability $X_n$ James Bond will pick up a Queen and the current number of closed Queens $X_n(52 - n)$ will decrease by $1$.
    With probability $(1 - X_n)$ James Bond will pick up a card different from Queen and the number of closed Queens $X_n(52 - n)$ will stay the same. 
    \[
    \E(X_{n+1} \mid \cF_n) = X_n \left( \frac{X_n(52 - n) - 1}{52 - n  - 1} \right) + (1 - X_n) \left( \frac{X_n(52 - n)}{52 - n - 1} \right) = \dots = X_n.
    \]
    Hence the process $(X_n)$ is a martingale with respect to $(\cF_n)$.
  \end{enumerate}
  
\end{sol}
\end{problem}

\begin{problem}
If possible create a martingale $(X_n)$ such that simulteneously $\P(X_n = 0 \text{ infinitely often}) = 1$ and
$\P(X_n = 1 \text{ infinitely often}) = 1$.
\begin{sol}
It is possible. 
\end{sol}
\end{problem}

\begin{problem}
At time $t=0$ there is one black and one white ball in the vase. 
At each moment of time we take out randomly one ball from the vase and put back two balls of the same color. 
Let $(W_t)$ be the proportion of white balls in the vase after $t$ extractions and $(Q_t)$ be the number of times when white ball was extracted.
\begin{enumerate}
  \item What is the distribution of $W_1$? Of $W_2$?
  \item Is $(W_t)$ a martingale?
  \item Consider a fixed parameter $p \in (0;1)$ and the process $M_t = (t + 1) C_t^{Q_t} p^{Q_t}(1-p)^{t - Q_t}$.
  Is $(M_t)$ a martingale?
  \item What is the limiting distribution of $(W_t)$?
  % \item What is the limiting distribution of $(Q_t / t)$? (need to think)
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item $W_1$ is equal to $1/3$ or $2/3$ with equal probabilities; $W_2$ is equal to $1/4$, $2/4$, $3/4$
    \item $(W_t)$ is a martingale;
    \item $(M_t)$ is a martingale;
    \item The limiting distribution of $(W_t)$ is uniform on $[0;1]$;
    % \item What is the limiting distribution of $(Q_t / t)$? (need to think)
  \end{enumerate}
  
\end{sol}
\end{problem}
  



\begin{problem}
Consider non-random sequence of numbers $(a_n)$. 
How can this sequence be a martingale?
\begin{sol}
Only constant non-random sequences are martingales.
\end{sol}
\end{problem}



\begin{problem}
Let $(M_n)$ be a martingale and $a<b<c<d$. 
\begin{enumerate}
  \item Find covariance $\Cov(M_d - M_c, M_b - M_a)$.
  \item Are $(M_d - M_c)$ and $(M_b - M_a)$ independent?
\end{enumerate}
\begin{sol}
$\Cov(M_d - M_c, M_b - M_a) = 0$;
\end{sol}
\end{problem}

\begin{problem}
Let $(M_t)$ be a process adapted to filtration $(\cF_t)$.

Is it true that in discrete time conditions 
\[
  \E(M_{t+1} \mid \cF_t) = M_t
\]
and
\[
\E(M_{t+k} \mid \cF_t) = M_t \text{ for all } k\geq 1
\]
are equivalent?
\begin{sol}
Yes. 
\end{sol}
\end{problem}

\begin{problem}
Initial wealth of a player is equal to $W_0 = 1$.
At each moment of time she can bet any proportion of her wellfare on the toss of a coin. 
If she guesses wrong she loses her bet. 
If she guesses right she gets profit equal to hear bet. 
The coin is not fair lands on head with probability $0.8$. 

\begin{enumerate}
  \item Find the bet that maximises one period log interest rate $\E(\ln (W_{t+1} / W_t) \mid \cF_t)$.
  \item Assume that the player maximises one period log interest rate every time. 
  Find a constant $a$ such that $\ln W_n - an$ is a martingale. 
\end{enumerate}


\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
For each case provide an example of a process.
\begin{enumerate}
  \item $(X_n)$ is a Markov chain and a martingale.
  \item $(X_n)$ is a Markov chain but not a martingale.
  \item $(X_n)$ is a martingale but not a Markov chain.
  \item $(X_n)$ is neither a Markov chain nor a martingale.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Let $(X_n)$ be a simple symmetric random walk and $(\cF_n)$ its natural filtration. 

Find a deterministic (non-random) sequence $a_n$ such that $M_n = X_n^3 + a_n X_n$ is a martingale with respect to $(\cF_n)$.
\begin{sol}
For example, $a_n = -3n$, but one may add any constant. 
\end{sol}
\end{problem}

\begin{problem}
The random variables $X_i$ are independent and they take values $+1$ or $-1$ with equal probability. 

\begin{enumerate}
\item {[3]} Find $\E(X_3 \mid X_1, X_2)$, $\E(X_3 \mid X_1 + X_3)$.
\item {[3]} Find $\Var(X_3 \mid X_1, X_2, X_3)$, $\Var(X_3 \mid X_1 + X_3)$.
\item {[4]} Let $Y_n$ be equal to $\E(X_1 + \ldots +  X_{2022} \mid X_1, X_2, \ldots, X_n)$. 

Is the process $Y_1$, $Y_2$, \ldots, $Y_{2022}$ a martingale?
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}
  

\begin{problem}
  Let $S_0 = 0$, $S_t = X_1 + X_2 + \ldots + X_t$. The increments $X_t$ are independent and identically distributed: 

\begin{tabular}{cccc}
\toprule
$x$ & $-1$ & $0$ & $1$ \\
$\P(X_t = x)$ & $0.2$ & $0.2$ & $0.6$ \\
\bottomrule
\end{tabular}

\begin{enumerate}
    \item If possible find all constants $a$ such that $M_t = S_t + at$ is a martingale.
  \item If possible find all constants $b$ such that $R_t = b^{S_t}$ is a martingale.
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
  Let $X_i$ be independent identically distributed with $\P(X_i = 1) = 0.9$, $\P(X_i = -1 ) = 0.1$. 

  Find all constants $a$ and $b$ such that $Y_t = a \exp\left(b\sum_{i=1}^t X_i\right)$ is a martingale. 
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
The random variables $(Z_t)$ are independent identically distributed 
with moment generating function given by $M_{Z}(u) = 1/(1 - 5u)^3$. 

We define $X_t$ as $X_t = \exp(Z_1 + 2Z_2 + 3Z_3 + \ldots + tZ_t)$ with $X_0 = 0$. 

If possible find a martingale of the form $Y_t = h(t) X_t$ where $h()$ is a non-random function.
\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
The process $(Z_t)$ in discrete time is called \textit{stationary} if it has constant expected value 
and constant covariances $\gamma_k$ that do not depend on $t$. 
\[
\begin{cases}
\E(Z_t) = \mu; \\
\Cov(Z_t, Z_t) = \gamma_0; \\
\Cov(Z_t, Z_{t+1}) = \gamma_1; \\
\Cov(Z_t, Z_{t+2}) = \gamma_2; \\
\dots \\
\end{cases}
\]

\begin{enumerate}
  \item If possible provide an example of a martingale that is not stationary.
  \item If possible provide an example of a stationary process that is not a martingale.
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
\begin{sol}

\end{sol}
\end{problem}
  

\begin{problem}
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
\begin{sol}

\end{sol}
\end{problem}
  

  


\subsection{Stopping time}

\begin{leftbar}
Doob's optional stopping time theorem. 
If $(M_t)$ is a martingale and $\tau$ is a stopping time then $\E(M_\tau) = \E(M_0)$ provided at least one of the following conditions hold:
\begin{itemize}
  \item $\P(\tau < \infty) = 1$, the stopped process $X_t = M_{t\wedge \tau}$ is bounded by some constant.
  \item $\E(\tau) < \infty$, the process $D_t = \E(M_{(t+1) \wedge \tau} - M_{t\wedge\tau} \mid \cF_t)$ is bounded by some constant.
  \item $\P(\tau < \infty) = 1$, the process $M_t$ is uniformly integrable, ie 
  \[
  \lim_{a \to \infty }\sup_t \E(M_t \cdot I(M_t > a)) = 0.
  \]
\end{itemize}
\end{leftbar}




\begin{problem}
A gambler wins or looses one rouble in each round in the casino 
with equal probabilities and independently.
Let's denote the result of the $n$-th round by $X_n$.

The gambler starts with initial fortune $S_0 = 0$. 
Let $S_n = X_1 + X_2 +\dots + X_n$ be the wealth at time $n$.
She can have negative balance up to $-a$ roubles. 

She quits the casino when she either reaches the target of $+b$ roubles or the credit limit of $-a$ roubles.

\begin{enumerate}
  \item Is $(S_n)$ a martingale?
  \item Use optional-stopping theorem to find probabilities of reaching $+b$ or $-a$.
  \item Is $M_n = S_n^2 - n$ a martingale?
  \item Find the expected number of rounds before she will stop gambling.
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item $(S_n)$ is a martingale;
  \item $\P(S_{\tau} = b) = a/(a + b)$;
  \item $M_n = S_n^2 - n$ is a martingale;
  \item $\E(\tau) = ab$;
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
A gambler wins or looses one rouble in each round in the casino 
with unequal probabilities and independently.
Let's denote the result of the $n$-th round by $X_n$, $\P(X_n = 1) = p$, $\P(X_n = -1) = q = 1-p$.

The gambler starts with initial fortune $S_0 = 0$. 
Let $S_n = X_1 + X_2 +\dots + X_n$ be the wealth at time $n$.
She can have negative balance up to $-a$ roubles. 

She quits the casino when she either reaches the target of $+b$ roubles or the credit limit of $-a$ roubles.

\begin{enumerate}
  \item Is $(S_n)$ a martingale?
  \item Is $K_n = (q / p)^{S_t}$ a martingale?
  \item Use optional-stopping theorem to find probabilities of reaching $+b$ or $-a$.
  \item Is $M_n = S_n - (p-q)n$ a martingale?
  \item Find the expected number of rounds before she will stop gambling.
\end{enumerate}  
\begin{sol}
  \begin{enumerate}
    \item $(S_n)$ is not a martingale;
    \item $K_n = (q / p)^{S_t}$ is a martingale;
    \item $\P(S_{\tau} = b) = ...$
    \item $M_n = S_n - (p-q)n$ is a martingale;
    \item $\E(\tau) = ...$
  \end{enumerate}
\end{sol}
\end{problem}
    

\begin{problem}
Famous «ABRACADABRA» problem. 

A monkey types randomly letters on a typewriter choosing each time one of the $26$ letters with equal probabilities.  
Let $T$ be the number of keypresses required to write the word «ABRACADABRA» for the first time. 

\begin{enumerate}
  \item Organise a casino to calculate $\E(T)$.
  \item Organise a casino to calculate $\E(T^2)$ and hence $\Var(T)$.
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\E(T) = 26^{11} + 26^4 + 26$.
    \item 
  \end{enumerate}  
\end{sol}
\end{problem}


\begin{problem}
  To survive vampire Boris needs to bite 70 talented students. 
    
These 70 talented students have formed a secret group. They have written their emails on small pieces of paper and have randomly distributed these pieces among them. Each student has exactly one piece of paper with an email\footnote{The group is so secret that it is possible that a student has his own email on his piece of paper}. 

Initially vampire Boris knows contacts of just two persons from the group. Today he will contact them, drink their blood and get the emails they have. Then vampire Boris will contact new victims and so on.

\begin{enumerate}
    \item For $t\geq 1$ consider the process $M_t$, the proportion of non bitten students after the day $t$. 
    
    Is this process a martingale?
    
    \item Using martingale stopping theorem or otherwise find the probability that vampire Boris will bite all 70 students. 
\end{enumerate}
  \begin{sol}
    
  \end{sol}
\end{problem}




\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}

  

\section{Poisson process}

\begin{leftbar}
  The process $(N_t)$ is called \emph{Poisson process} with intensity $\lambda$ if 
  \begin{itemize}
    \item $N_0 = 0$;
    \item Increments are independent:
    If $t_1 < t_2 < t_3 < \dots < t_k$ then random increments $N(t_2) - N(t_1)$, $N(t_3) - N(t-2)$, \dots{ } are independent.
    \item Increments have Poisson distribution:
    \[
    N_b - N_a \sim \dPois(\lambda (b-a));
    \]
  \end{itemize}
  The process $(N_t)$ is called \emph{Poisson process} with intensity $\lambda$ if 
  \begin{itemize}
    \item $N_0 = 0$;
    \item Increments are independent;
    \item Increments are stationary:
    
    The distribution of $N_b - N_a$ depends only on $(b - a)$.
    \item Probability of observing two or more points in a small interval is negligible:
    \[
    \P(N_{t + \Delta} - N_t > 1) = o(\Delta).
    \]
  \end{itemize}
\end{leftbar}


\begin{problem}
Two cashiers Alice and Bob simulteneously started to service their clients.
The service times $X_a$ and $X_b$ are independent and exponentially distributed with rates $\lambda_a = 1$ and $\lambda_b = 2$.

\begin{enumerate}
  \item Find the probability $\P(X_a < X_b)$.
  \item Find the density of $S = X_a + X_b$.
  \item Find the density of $L = \min\{X_a, X_b\}$.
  \item Find the density of $R = \max\{X_a, X_b\}$.
  \item Solve all the previous points for general rates $\lambda_a$ and $\lambda_b$.
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item $\P(X_a < X_b) = \lambda_a/(\lambda_a + \lambda_b)$.
    There are two possible solutions: double integral and first step analysis.
    \item 
    \item 
    \item 
  \end{enumerate}  
\end{sol}
\end{problem}


\begin{problem}
Let $X_t$ and $Y_t$ be two independent Poisson processes. 
Is it true that $S_t = X_t + Y_t$ is also a Poisson process?
\begin{sol}
Yes. 
\end{sol}
\end{problem}

\begin{problem}
Hedgehogs are scattered in a big forest according Poisson process with rate $\lambda = 1$ per $100$ squared meters. 

What should be the edge of a square such that the probability of finding a hedgehog there is $0.7$?
\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Let $N_t$ be a Poisson process with rate $\lambda$.
\begin{enumerate}
  \item Is the process $A_t = N_t - \lambda t$ a martingale?
  \item Is the process $B_t = A_t^2 - \lambda t$ a martingale?
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $A_t = N_t - \lambda t$ is a martingale;
    \item $B_t = A_t^2 - \lambda t$ is a martingale;
  \end{enumerate}  
\end{sol}
\end{problem}


\begin{problem}
  Students arrive in the Grusha café according to the Poisson arrival process $(X_t)$ with constant rate $\lambda$.
  The probability of no visitors during 5 minutes is 0.05.
  \begin{enumerate}
    \item   Find the value of $\lambda$.
    \item  Find the variance and expected number of arrivals between 5 pm and 8 pm.
    \item What is the probability of exactly 5 arrivals between 5 pm and 8 pm?
  \end{enumerate}
\begin{sol}

\end{sol}
\end{problem}
  


\begin{problem}
Masha receives on average 10 sms per minute. 
Sms arrival is well described by the Poisson process.
\begin{enumerate}
  \item  What is the probability that Masha receives exactly 10 sms in the next 40 seconds?
  \item Masha just received an sms. What is the probability that she will wait more that 2.5 seconds before the
  next one?
  \item  Find the covariance between the number of sms in the first 3 minutes and the number of sms in the first
  10 minutes.
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}
  


\begin{problem}
Taxis arrive to the station according to the Poisson process with rate 1 per 5 minutes.
Let $Y_t$ be the number of taxis that will arrive between 0 and $t$ minutes.
\begin{enumerate}
  \item  Sketch the expected value of $Y_t$ as a function of $t$.
\item Sketch the probability $\P(Y_t = Y_{60})$ as a function of $t$.
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}



\begin{problem}
A company gets fines for non-removal of quadrobics video content.
What is the probability that the total amount will exceed two undecillion roubles in 1000 days for each case?

\begin{enumerate}
  \item Fines arrive according to Poisson process with rate $1$ fine per day and each fine has the size $10^{33}$ roubles. 
  Fines are summing up without additional penalties.

  \item Initial fine is $10^5$ roubles but it doubles according to Poisson process with rate $1$ doubling per $10$ days. 
\end{enumerate}

\begin{sol}
Here we may approximate Poisson distribution by normal distribution, $\cN(\lambda t, \lambda t)$.
\end{sol}
\end{problem}
  



\begin{problem}

\begin{sol}

\end{sol}
\end{problem}
  
  


\begin{problem}
Customers order coffee according to Poisson process with rate $1$ cup per minute.
The owner will close the shop if no one orders a coffee in $7$ minutes.

Let $X$ be the closure time. 

Find $\E(X)$ and $\Var(X)$.
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
The arrival of buses at a given stop follows Poisson process with rate $3$. 
The arrival of taxis at same stop follows independent Poisson process with rate $5$. 

\begin{enumerate}
  \item What is the probability that two or more taxis will arrive before next bus?
  \item What is the probability that exactly two taxis will arrive before next bus?
\end{enumerate} 
\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Customers order coffee according to Poisson process with rate $1$ cup per minute.
Let $N_t$ be the number of orders up to time $t$.

Find the probability $\P(N_t \text{ is even})$.
\begin{sol}
Let's denote $a(t) = \P(N_t \text{ is even})$.
\[
a(t+\Delta)=a(t)(1-\Delta)+(1-a(t))\Delta + o(\Delta) 
\]
Hence we get a differential equation  $a'(t)=1-2a(t)$ with $a(0)=1$.
The solution is $a(t)=(1+\exp(-2t))/2$.
\end{sol}
\end{problem}
  


\begin{problem}
Prove that two definitions of Poisson process are equivalent.

Definition A. The process $(N_t)$ is called \emph{Poisson process} with intensity $\lambda$ if 
  \begin{itemize}
    \item $N_0 = 0$;
    \item Increments are independent:
    If $t_1 < t_2 < t_3 < \dots < t_k$ then random increments $N(t_2) - N(t_1)$, $N(t_3) - N(t-2)$, \dots{ } are independent.
    \item Increments have Poisson distribution:
    \[
    N_b - N_a \sim \dPois(\lambda (b-a));
    \]
  \end{itemize}

  Definition B. The process $(N_t)$ is called \emph{Poisson process} with intensity $\lambda$ if 
  \begin{itemize}
    \item $N_0 = 0$;
    \item Increments are independent;
    \item Increments are stationary:
    
    The distribution of $N_b - N_a$ depends only on $(b - a)$.
    \item Probability of observing two or more points in a small interval is negligible:
    \[
    \P(N_{t + \Delta} - N_t > 1) = o(\Delta).
    \]
  \end{itemize}
\begin{sol}

\end{sol}
\end{problem}
  
\section{Wiener Process}

\begin{problem}
Consider a Wiener process $(W_t)$.
\begin{enumerate}
    \item {[4]} Let $Y_t = t W_{2t}$. What is the distribution of $Y_t - Y_s$ for $t\geq s$? Is $Y_t$ a Wiener process?
    \item {[6]} Find a constant $\alpha$ such that $M_t = W_t^3 + \alpha t W_t$ is a martingale. 
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}
  


\begin{problem}
Consider a Wiener process $(W_t)$. For $r<s<t<u$ find the following expected values 
    \begin{enumerate}
    \item $\E((W_u - W_t)^2(W_s - W_r)^2)$;
    \item $\E((W_u - W_s)(W_t - W_r))$;
    \item $\E((W_t - W_r)(W_s - W_r)^2)$;
    \item $\E(W_r W_s W_t)$;
    \item $\E(W_r W_s W_t \mid W_s)$;
    \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
Here $(W_t)$ is a Wiener process.
  \begin{enumerate}
      \item Find $\E(W_5 W_4 \mid W_4)$, $\Var(W_5 W_4 \mid W_4)$.
      \item Find covariance $\Cov(W_4 W_5, W_5 W_6)$.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
For Wiener process $(W_t)$ find $\E(W_1 W_2 W_3)$ and $\E(W_2 W_3 \mid W_1)$.
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\section{Ito's integral}


\begin{problem}
  Consider Ito process $X_t$

  \[
  dX_t = \exp(t) W_t\, dt + \exp(2W_t) \, dW_t, \quad X_0 = 1.
  \]
  
  Consider two processes, $A_t = 1 + t^2 + X_t^3$ and $B_t = 1 + t^2 + X_t^3 W_t^4$.
  
  \begin{enumerate}
      \item Find $dA_t$ and $dB_t$.
      \item Write the corresponding explicit expressions for $A_t$ and $B_t$:
      \[
      const + \int_0^t \ldots dW_u + \int_0^t \ldots du
      \]
      \item Check whether $X_t$ is a martingale.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
Consider the process $X_t$
\[
X_t= tW_t + \int_0^t uW_u^2\, dW_u.
\]
\begin{enumerate}
    \item Find $\E(X_t)$, $\Var(X_t)$.
    \item Find $dX_t$.
    \item Check whether $X_t$ is a martingale.
\end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
Consider $X_t = \int_0^t W_u^3 dW_u + \int_0^t (W_u^3 + 3W_u u ) du - W_t^3 \cdot t$.
  \begin{enumerate}
      \item Find $dX_t$ and the corresponding full form. 
      \item Is $X_t$ a martingale?
  \end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
Consider $X_t = \exp(-2W_t - 2t)$.
\begin{enumerate}
    \item Find $dX_t$. Is $X_t$ a martingale?
    \item Find $\E(X_t)$ and $\Var(X_t)$.
    \item Find $\int_0^t X_u dW_u$.
\end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  Consider an Ito's process $I_t = 2022 + W_t t^2 + \int_0^t W_u^3 dW_u + \int_0^t W_u^2 du$.
  \begin{enumerate}
    \item Find $dI_t$ and check whether $I_t$ is a martingale. 
    \item Check whether $J_t = I_t - \E(I_t)$ is a martingale.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Martingales are everywhere :)
  
  Consider the process $Y_t = \exp( - u W_t)$. 
  
  \begin{enumerate}
    \item Find a multiplier $h(u, t)$ such that $M_t = h(u, t) \cdot Y_t$ is a martingale. 
    \item Find $dY_t$, $\E(Y_t)$ and $\Var(Y_t)$.
    \item Consider $M_t$ that you have found as a function of $u$. 
    Find the Taylor approximation of the function $M_t(u)$ up to $u^4$. 
    \item Consider the coefficient before $u^4$ in the Taylor expansion of $M_t(u)$. 
    Is it a martingale?
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Consider the process $X_t = \int_0^t W_u^2 dW_u + \int_0^t (W_u^2 + 2W_u u ) du - W_t^2 \cdot t$.

  \begin{enumerate}
      \item Find $dX_t$ and the corresponding full form. 
      \item Is $X_t$ a martingale?
      \item Find $\E(X_t)$.
  \end{enumerate}  
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Consider the stochastic process $X_t = f(t) \cos (2021 W_t)$.
  \begin{enumerate}
      \item Find $dX_t$.
      \item Find any $f(t) \neq 0$ such that $X_t$ is a martingale.
      \item Using $f(t)$ from the previous point find $\E(\cos (2021 W_t))$.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}



\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}



\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\section{Binomial asset pricing model}

\begin{problem}
Consider two-period binomial model with initial share price $S_0 = 600$, 
Up and down multipliers are $u=1.2$, $d=0.9$, risk-free interest rate is $r = 0.05$ per period. 

Consider an option that pays you $X_2 = 100$ at $T=2$ if $S_2 > S_1$ and nothing otherwise. 
  
\begin{enumerate}
  \item Find the risk neutral probabilities. 
  \item Find the current price $X_0$ of the asset. 
  \item How much shares should I have at $t=1$ in the «up» state of the world to replicate the option?
\end{enumerate}
\begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}




\section{Black and Scholes model}

\begin{problem}
Consider Black and Scholes model with riskless rate $r$, volatility $\sigma$ and initial share price $S_0$. 

Find the current price $X_0$ of an option that pays you $X_2 = S_1^3$ at time $T=2$. 
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
Ded Moroz would like to receive $X_T = S^{-1}_T$ at time $T$ if $S_T < 1$ and nothing otherwise.
    
Assume the framework of Black and Scholes model, $S_t$ is the share price, $r$ is the risk free rate,
$\sigma$ is the volatility. 

How much Ded Moroz should pay now at $t=0$?
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
Consider the Black and Scholes model with riskless rate $r$, volatility $\sigma$ and initial share price $S_0$. 

Find the current price $X_0$ of an option that pays you one dollar at time $T=2$ only if $S_2 > \exp(3r) S_0$.
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\section{Stationarity}

\begin{problem}
  The variables $x_t$ take values $0$ or $1$ with equal probabilities.
  The variables $u_t$ are normal $\cN(0; 1)$. All variables are independent.
  
  Consider the process  $z_t = x_t (1-x_{t-2}) u_t$.

  \begin{enumerate}
      \item Find the covariance $\Cov(z_t, z_s)$. Is the process $z_t$ stationary?
      \item Given that $z_{100} = 2.3$ find shёrtest predictive intervals for $z_{101}$ and $z_{102}$ with probability of coverage at least 95\%.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}



\section{ARMA}


\begin{problem}
  Consider stationary $AR(2)$ model, $y_t = 2 + 0.3 y_{t-1} - 0.02 y_{t-2} + u_t$, where $(u_t)$ is a white noise
  with $\Var(u_t) = 4$.
  
  The last two observations are $y_{100} = 2$, $y_{99} = 1$.
  \begin{enumerate}
      \item Find 95\% predictive interval for $y_{102}$.
      \item Find the first two values of the autocorrelation function, $\rho_1$, $\rho_2$.
      \item Find the first two values of the partial autocorrelation function, $\phi_{11}$, $\phi_{22}$.
  \end{enumerate}  
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Snegurochka studies a stochastic analog of the Fibonacci sequence
  \[
      y_t = y_{t-1} + y_{t-2} + u_t,
  \]
  where $(u_t)$ is a white noise process. 
  \begin{enumerate}
      \item How many non-stationary solutions are there?
      \item What can you say about the number and the structure of the stationary solutions?
      \item Can Snёgurochka find two starting constants $y_0 = c_0$ and $y_1=c_1$ in such a way to make a solution stationary?
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Stochastic process $X_t$ is defined by $X_t = 7 + u_t + 0.3 u_{t-1}$, where $(u_t)$ is a white noise 
  with variance $\sigma^2$.
  \begin{enumerate}
      \item Is $(X_t)$ stationary? 
      \item Find the autocorrelation function of $(X_t)$.
      \item Find $\E(X_{t+2} \mid X_t, X_{t-1}, \ldots)$.
  \end{enumerate}
  
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  Young investor Winnie-the-Crypto compares two trading strategies: buying bitcoins from good bees and from bad bees. 
  Let $d_t$ be the price difference at day $t$ (bad minus good). 
  Winnie-the-Crypto would like to test $H_0$: $\E(d_t) = 0$ against $H_a$: $\E(d_t) \neq 0$ at $5\%$ significance level.
  
  Winnie assumed that $(d_t)$ can be approximated by a $MA(1)$ process and estimated the parameters using $T=400$ observations, $\hat d_t = 2 + u_t + 0.7 u_{t-1}$ 
  with $\hat\sigma^2_u = 4$.
  
  \begin{enumerate}
    \item Estimate $\E(d_t)$, $\Var(d_t)$ and $\Cov(d_t, d_{t-1})$.
    \item Estimate $\E(\bar d)$, $\Var(\bar d)$ and help Winnie by considering $Z = \frac{\bar d - 0}{se(\bar d)}$.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Consider the following stationary process
  \[
  y_t = 1 + 0.5 y_{t-2} + u_t + u_{t-1},    
  \]
  where random variables $u_t$ are independent $\cN(0; 4)$.
  
  \begin{enumerate}
      \item Find the 95\% predictive interval for $y_{101}$ given that $y_{100} = 2$, $y_{99} = 3$, $y_{98} = 1$, $u_{99} = -1$.
      \item Find the point forecast for $y_{101}$ given that $y_{100}=2$.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}



\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\section{ETS}

\begin{problem}
  The semi-annual $y_t$ is modelled by $ETS(AAA)$ process:
    
    \[
    \begin{cases}
        u_t \sim \cN(0; 4) \\
        s_t = s_{t-2} + 0.1 u_t \\
        b_t = b_{t-1} + 0.2 u_t \\
        \ell_t = \ell_{t-1} + b_{t-1} + 0.3 u_t \\
        y_t = \ell_{t-1} + b_{t-1} + s_{t-2} + u_t \\
    \end{cases}    
    \]

    \begin{enumerate}
        \item Given that $s_{100} = 2$, $s_{99} = -1.9$, $b_{100} = 0.5$, $\ell_{100} = 4$ find 95\% prёdictive interval for $y_{102}$. 
        \item In this problem particular values of parameters are specified. And how many parameters are estimated in semi-annual $ETS(AAA)$ model before real forecasting?
    \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  The $ETS(AAdN)$ model is given by the system
  \[
  \begin{cases}
  u_t  \sim \mathcal{N}(0;20) \\
  b_t = 0.9 b_{t-1} + 0.2 u_t \\
  \ell_t = \ell_{t-1} + 0.9 b_{t-1} + 0.3 u_t \\
  y_t = \ell_{t-1} + 0.9 b_{t-1} + u_t \\
  \end{cases}
  \]
  with $\ell_{100} = 20$ and $b_{100} = 2$.
\begin{enumerate}
  \item Find conditional probability $\P(y_{102} > 30 \mid \ell_{100}, b_{100})$.
  \item Approximately find the best point forecast for $y_{10000}$.
\end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  Consider $ETS(ANN)$ model,
	\[
	\begin{cases}
	y_t = \ell_{t-1} + u_t \\
	\ell_t = \ell_{t-1} + \alpha u_t \\
	u_t \sim \cN(0;\sigma^2). \\
	\end{cases}
	\]
Let $\ell_{99} = 50$, $\alpha = 1/2$, $\sigma^2 = 16$, $y_{98} = 48$, $y_{99} = 52$, $y_{100} = 55$. 

Calculate 95\% predictive interval for $y_{101}$.
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\section{GARCH}

\begin{problem}
  The process $y_t$ is described by a simple $GARCH(1, 1)$ model:
  \[ 
      \begin{cases}
          y_t = \sigma_t \nu_t \\
          \sigma_{t}^{2}= 1 + 0.2 y_{t-1}^{2}+ 0.3 \sigma_{t-1}^{2}    \\
          \nu_t \sim \cN(0;1)
      \end{cases}     
  \]

  The variables $\nu_t$ are independent of past variables $y_{t-k}$, $\nu_{t-k}$, $\sigma_{t-k}$ for all $k\geq 1$.
  The prёcesses $y_t$, $\sigma^2_t$ are stationary. 


  Given $\sigma_{100}=1$ and $\nu_{100} = 0.5$ find 95\% predictive interval for $y_{102}$. 
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}



\section{Method of Moments and maximum likelihood}

\begin{problem}
  The variables $X_1$, \ldots, $X_n$ are independent identically distributed with density 
  \[
  f(x) = \begin{cases}
    \lambda \exp(-\lambda (x - \theta)), \text{ if } x\geq \theta \\
    0, \text{ otherwise}.
  \end{cases}  
  \]
  \begin{enumerate}
    \item {[5]} Find the method of moments estimator of $\lambda$ for known value $\theta = 1$ using the first moment. 
    \item {[5]} Find the method of moments estimator of $\lambda$ for unknown value $\theta$ using the first two moments. 
  \end{enumerate}  
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  The variables $X_1$, \ldots, $X_n$ are independent and normally distributed $\cN(a, 2a)$.
  
  Find the maximum likelihood estimator of $a$.

  Hint: $f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp(-(x-\mu)^2/2\sigma^2)$.  
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  The weight of a fish $Y_i$ is a discrete random variables with 
  distribution and observed frequencies given in the table 
  
  \begin{tabular}{cccc}
      \toprule
      Weight [kg] & 1 & 2 & $a$ \\
      Probability & $0.2 + 0.1a$ & $0.3 - 0.1a$ & $0.5$ \\
      Observed frequency & $N_1$ & $N_2$ & $N_a$ \\
      \bottomrule
  \end{tabular}
  
  Fish weights $Y_i$ are independent, $a > 10$ is unknown. 
  \begin{enumerate}
      \item Find the method of moments estimator of the parameter $a$. 
      \item Find the maximum likelihood estimator of the parameter $a$. 
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}




\section{LR test}

\begin{problem}
  We have two independent random samples $X_1$, $X_2$, \ldots, $X_{n_x}$ and $Y_1$, $Y_2$, \ldots, $Y_{n_y}$.
  The random variables $X_i$ follow Poisson distribution with intensity rate $\lambda_x$, 
  random variables $Y_i$ follow Poisson distribution with intensity rate $\lambda_y$.

  We would like to test $H_0$: $\lambda_x = \lambda_y$ against $H_1$: $\lambda_x \neq \lambda_y$.

  \begin{enumerate}
    \item {[3]} Find the maximal value of log-likelihood under $H_0$.
    \item {[3]} Find the maximal value of log-likelihood under unrestricted model.
    \item {[2]} Construct the likelihood ratio test. 
    \item {[2]} Do you reject $H_0$ if $n_x = 100$, $n_y = 200$, $\sum x_i = 500$, $\sum y_i = 900$ at
    significance level $5\%$?
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  You observe $X_1$, \ldots, $X_{400}$ and $Y_1$, \ldots, $Y_{400}$, $\bar X = 5$, $\bar Y = 6$. 
  All variables are independent. 
  
  Consider the null hypothesis that all random variables are exponentially distributed with common parameter $\lambda$ against alternative
  that parameter is $\lambda_X$ for every $X_i$ and $\lambda_Y$ for every $Y_j$. 
  
  \begin{enumerate}
    \item Estimate common $\lambda$ using maximum likelihood for the restricted model. 
    \item Estimate both $\lambda_X$ and $\lambda_Y$ using maximum likelihood in the unrestricted model. 
    \item Use LR-test to test the null hyphotesis at 5\% significance level. 
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}



\section{Properties of estimators}


\begin{problem}
  The variables $X_1$, \ldots, $X_n$ are independent and uniformly distributed $\dUnif[0;a]$ with $a>1$.
  We do not observe $X_i$ directly but we know whether each $X_i$ is larger than 1. 
  Hence we observe the indicators $Y_i = I(X_i > 1)$.

  Consider the estimator $\hat a = 1 / (1 - \bar Y)$.

  \begin{enumerate}
    \item {[5]} Is $\hat a$ consistent?
    \item {[5]} Is $\hat a$ unbiased for $n=2$?
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\section{Fisher information and Cramer~— Rao}

\begin{problem}
  The variables $X_1$, \ldots, $X_n$ are independent and have Poisson distribution with intensity rate $\lambda$.
  In other words the probability mass function is given by $\P(X_i = k) = \exp(-\lambda) \lambda^k / k!$.
  
  \begin{enumerate}
    \item {[5]} Find theoretical Fisher information for $\lambda$ contained in the sample. 
    \item {[2]} Derive the maximum likelihood estimator for $\lambda$.
    \item {[3]} Does the maximum likelihood estimator attain the Cramer-Rao lower bound for variance? 
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Consider an estimator $\hat a$ with $\E(\hat a) = 0.5a + 3$. For the given sample size the Fisher information is $I_F(a) = 400/a^2$.
  \begin{enumerate}
    \item What is the theoretical minimal variance of $\hat a$?
    \item Assume that $\hat a$ attains the minimal variance boundary and is asymptotically normal. Given that $\hat a = 2022$ provide 95\% CI for $a$.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\section{Sufficiency}

\begin{problem}
  The variables $X_1$, \ldots, $X_n$ are independent and gamma distributed with density
  \[
  f(x) = \begin{cases}
    \lambda^\alpha x^{\alpha - 1} \exp(-\lambda x) / \Gamma(\alpha), \text{ if } x\geq 0 \\
    0, \text{ otherwise}.
  \end{cases}  
  \]
  \begin{enumerate}
    \item {[5]} Find a sufficient statistic for $\alpha$ if we know that $\lambda = 1$. 
    \item {[5]} Find a two dimensional sufficient statistic for unknown $\alpha$ and $\lambda$. 
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  The variables $X_1$, \ldots, $X_n$ are independent and uniformly distributed on $[0; 2a]$ for some positive $a$. 

  \begin{enumerate}
    \item Find any sufficient statistic for $a$. 
    \item How the answer will change if $X_i \sim U[-a; 2a]$?
  \end{enumerate}  
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}





\Closesolutionfile{solution_file}


% для гиперссылок на условия
% http://tex.stackexchange.com/questions/45415
\renewenvironment{solution}[1]{%
         % add some glue
         \vskip .5cm plus 2cm minus 0.1cm%
         {\bfseries \hyperlink{problem:#1}{#1.}}%
}%
{%
}%

\section{Solutions}
\input{all_solutions}


\printindex


\section{Sources of wisdom}

\nocite{buzun2015stochastic}

\nocite{buzun2015stochastic}

\printbibliography[heading=none]


\end{document}
