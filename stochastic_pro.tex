% arara: xelatex: {shell: yes}
% arara: biber
% arara: xelatex: {shell: yes}
% arara: xelatex: {shell: yes}



%!TeX cleanPatterns = $OUTDIR/$JOB!($OUTEXT|.synctex.gz|.tex|.pdf), /$OUTDIR/_minted-$JOB/
\documentclass[12pt, a4paper]{article}
\usepackage{libertine}

% utf8 is the preferred encoding

 % this magick is to solve problem that appeared after update of texlive 2018 to texlive 2020
 % https://tex.stackexchange.com/questions/511341/the-error-occurred-after-the-last-update
\makeatletter
\def\nobreak{\penalty\@M}
\makeatother


\usepackage{fontspec} % что-то про шрифты? % нужно ли загружать?

\usepackage{polyglossia} % русификация xelatex
\usepackage{csquotes}


\setmainlanguage{english}
\setotherlanguage{russian}

% download "Linux Libertine" fonts:
% http://www.linuxlibertine.org/index.php?id=91&L=1
%\setmainfont{Linux Libertine O} % or Helvetica, Arial, Cambria
% why do we need \newfontfamily:
% http://tex.stackexchange.com/questions/91507/
%\newfontfamily{\cyrillicfonttt}{Linux Libertine O}

\newfontfamily\arabicfont[Script=Arabic]{Scheherazade New}


\usepackage{etoolbox} % provides \AtEndPreamble
% etoolbox causes wrong behavior of tocbasic
\AtEndPreamble{ % ради арабского написания Абу ибн-Сина
  \usepackage{arabxetex} 
 \let\textarabic\relax 
 \let\Arabic\relax 
\setotherlanguages{arabic, english}
}
% комбо из:
% https://tex.stackexchange.com/questions/501897
% https://tex.stackexchange.com/questions/392175/

\usepackage{imakeidx} 
\indexsetup{level=\section}
\makeindex[title=Hashtags]

% \usepackage{etex} % расширение классического tex
% в частности позволяет подгружать гораздо больше пакетов, чем мы и займёмся далее

\usepackage{verbatim} % для многострочных комментариев
\usepackage{makeidx} % для создания предметных указателей

\usepackage{setspace}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathrsfs} % sudo yum install texlive-rsfs
\usepackage{dsfont} % sudo yum install texlive-doublestroke
\usepackage{array, multicol, multirow, bigstrut} % sudo yum install texlive-multirow
\usepackage{indentfirst} % установка отступа в первом абзаце главы


\usepackage{bm}
\usepackage{bbm} % шрифт с двойными буквами
%\usepackage[perpage]{footmisc}

\usepackage{dcolumn} % центрирование по разделителю для apsrtable

% создание гиперссылок в pdf
\usepackage[unicode, colorlinks=true, urlcolor=blue, hyperindex, breaklinks]{hyperref}


\usepackage{microtype} % свешиваем пунктуацию
% теперь знаки пунктуации могут вылезать за правую границу текста, при этом текст выглядит ровнее


\usepackage{textcomp}  % Чтобы в формулах можно было русские буквы писать через \text{}

% размер листа бумаги
%\usepackage[paperwidth=145mm,paperheight=215mm,
%height=182mm,width=113mm,top=20mm,includefoot]%{geometry}
\usepackage[paper=a4paper, top=15mm, bottom=13.5mm, left=16.5mm, right=13.5mm, includefoot]{geometry}

\usepackage{xcolor}

\usepackage{framed} %  \leftbar


% \usepackage{float, longtable}
% \usepackage{soulutf8}

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке

\usepackage{mathtools}
\usepackage{cancel, xspace} % sudo yum install texlive-cancel


\usepackage{numprint} % sudo yum install texlive-numprint
\npthousandsep{,}\npthousandthpartsep{}\npdecimalsign{.}


% \usepackage{subfigure} % для создания нескольких рисунков внутри одного

\usepackage{tikz, pgfplots} % язык для рисования графики из latex'a
\pgfplotsset{compat=1.16}
\usetikzlibrary{trees} % tikz-прибамбас для рисовки деревьев
\usepackage{tikz-qtree} % альтернативный tikz-прибамбас для рисовки деревьев
\usetikzlibrary{arrows} % tikz-прибамбас для рисовки стрелочек подлиннее
\usetikzlibrary{automata, positioning, calc}

\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет Последний день Помпеи}
% \listoftodos --- печатает все поставленные \todo'шки



\usepackage{booktabs} %  красивые таблицы
% заповеди из докупентации:
% 1. Не используйте вертикальные линни
% 2. Не используйте двойные линии
% 3. Единицы измерения - в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"

% \usepackage{physics} % avoid garbage
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\scalp}{\langle}{\rangle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% \usepackage{minted} % moved to listings to simplify development
\usepackage{listings}
\lstset{%
basicstyle=\fontfamily{lmtt}\bfseries,
keywordstyle=\fontfamily{lmtt}\bfseries
}
% \usepackage{julia-mono-listings}
% TODO: установить?
\usepackage{answers}




\usepackage[bibencoding=auto, backend=biber, sorting=none, style=alphabetic]{biblatex}

\addbibresource{stochastic_pro.bib}

\setcounter{tocdepth}{1} % в оглавление оставляем уровень 1

\usepackage[titles]{tocloft} % альтернатива tocbasic для настройки toc
% если нужен subfigure, то у tocloft можно добавить опцию subfigure
\renewcommand{\cftbeforesecskip}{0.7pt} % поправка интервала между строками для section в toc
\renewcommand{\cftsecdotsep}{\cftdotsep} % добавляем точечки

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
% \setlist[enumerate, 1]{label=\asbuk*),ref=\asbuk*} % цифра рядом с enumerate = уровень нумерации
\setlist[enumerate, 1]{label=\alph*),ref=\alph*} % цифра рядом с enumerate = уровень нумерации


%%%%%%%%%%%%%%%%%%%%%%%  ПАРАМЕТРЫ  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setstretch{1}                          % Межстрочный интервал
\flushbottom                            % Эта команда заставляет LaTeX чуть растягивать строки, чтобы получить идеально прямоугольную страницу
\righthyphenmin=2                       % Разрешение переноса двух и более символов
%\pagestyle{plain}                       % Нумерация страниц снизу по центру.
\widowpenalty=300                     % Небольшое наказание за вдовствующую строку (одна строка абзаца на этой странице, остальное --- на следующей)
\clubpenalty=3000                     % Приличное наказание за сиротствующую строку (омерзительно висящая одинокая строка в начале страницы)
\setlength{\parindent}{1.5em}           % Красная строка.
%\captiondelim{. }
\setlength{\topsep}{0pt}
\emergencystretch=2em

% делаем короче интервал в списках
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}



\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\sgn}{sign}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\amn}{arg\,min}
\DeclareMathOperator*{\amx}{arg\,max}


\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\sCorr}{sCorr}
\DeclareMathOperator{\sCov}{sCov}
\DeclareMathOperator{\sVar}{sVar}
\DeclareMathOperator{\hVar}{\widehat{\Var}}
\DeclareMathOperator{\hCorr}{\widehat{\Corr}}
\DeclareMathOperator{\hCov}{\widehat{\Cov}}

\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\Med}{Med}

\let\P\relax
\DeclareMathOperator{\P}{\mathbb{P}}

\DeclareMathOperator{\E}{\mathbb{E}}

\newcommand{\e}{\varepsilon}


\newcommand{\cF}{\mathcal{F}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}

\newcommand{\cN}{\mathcal{N}}
\newcommand{\dNorm}{\mathcal{N}}
\newcommand{\dN}{\mathcal{N}}
\newcommand{\dLN}{\mathcal{LN}}

\newcommand{\dBern}{\mathrm{Bern}}
\newcommand{\dPois}{\mathrm{Pois}}
\newcommand{\dBin}{\mathrm{Bin}}
\newcommand{\dMult}{\mathrm{Mult}}
\newcommand{\dGeom}{\mathrm{Geom}}
\newcommand{\dNHGeom}{\mathrm{NHGeom}}
\newcommand{\dHGeom}{\mathrm{HGeom}}
\newcommand{\dDUnif}{\mathrm{DUnif}}
\newcommand{\dFS}{\mathrm{FS}}
\newcommand{\dNBin}{\mathrm{NBin}}

\newcommand{\dTri}{\mathrm{Triangle}}
\newcommand{\dUnif}{\mathrm{Unif}}
\newcommand{\dCauchy}{\mathrm{Cauchy}}
\newcommand{\dExpo}{\mathrm{Expo}}
\newcommand{\dBeta}{\mathrm{Beta}}
\newcommand{\dGamma}{\mathrm{Gamma}}
\newcommand{\dWei}{\mathrm{Wei}}
\newcommand{\dLogistic}{\mathrm{Logistic}}
\newcommand{\dRayleigh}{\mathrm{Rayleigh}}
\newcommand{\dPareto}{\mathrm{Pareto}}

\newcommand{\score}{\mathrm{score}}
\newcommand{\crit}{\mathrm{crit}}

% вместо горизонтальной делаем косую черточку в нестрогих неравенствах
\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}


\newcommand{\wv}{\textrm{word2vec}}



\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}

\newcommand{\Lin}{\mathcal{L}in}
\newcommand{\Linp}{\Lin^{\perp}}

\title{Stochastic Processes problems}
\author{\url{https://github.com/bdemeshev/stochastic_pro}}
\date{\today}


%\newtheorem{problem}{Задача}
%\numberwithin{problem}{section}

\Newassociation{sol}{solution}{solution_file}
% sol --- имя окружения внутри задач
% solution --- имя окружения внутри solution_file
% solution_file --- имя файла в который будет идти запись решений
% можно изменить далее по ходу
\Opensolutionfile{solution_file}[all_solutions]
% в квадратных скобках фактическое имя файла



% магия для автоматических гиперссылок задача-решение
\newlist{myenum}{enumerate}{3}
% \newcounter{problem}[chapter] % нумерация задач внутри глав
\newcounter{problem}[section]

\newenvironment{problem}%
{%
\refstepcounter{problem}%
%  hyperlink to solution
     \hypertarget{problem:{\thesection.\theproblem}}{} % нумерация внутри глав
     % \hypertarget{problem:{\theproblem}}{}
     \Writetofile{solution_file}{\protect\hypertarget{soln:\thesection.\theproblem}{}}
     %\Writetofile{solution_file}{\protect\hypertarget{soln:\theproblem}{}}
     \begin{myenum}[label=\bfseries\protect\hyperlink{soln:\thesection.\theproblem}{\thesection.\theproblem},ref=\thesection.\theproblem]
     % \begin{myenum}[label=\bfseries\protect\hyperlink{soln:\theproblem}{\theproblem},ref=\theproblem]
     \item%
    }%
    {%
    \end{myenum}}
% для гиперссылок обратно надо переопределять окружение
% это происходит непосредственно перед подключением файла с решениями





\begin{document}

\maketitle % ставим сюда название, автора и время создания

% здесь нужна прикольная картинка

\newpage
\tableofcontents{}

\newpage


\section{First step analysis}

\begin{problem}
Biden and Trump alternately throw a fair dice infinite number of times. 
Biden throws first. 
The person who obtains the first $6$ wins the game. 

\begin{enumerate}
  \item What is the probability that Biden will win?
  \item What is expected number of turns?
  \item What is variance of the number of turns?
  \item What is expected number of turns given that Biden won?
  \item Find the transition matrix of this four state Markov chain. 
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item $\P(B) = 6/11$, first step equation for $p = \P(B)$ is
  $p = 1/6 + (5/6)^2 p$ or $p = 1/6 + 5/6 \cdot (1 - p)$.
  \item $\E(N) = 6$, first step equation for $m = \E(N)$ is $m = 1/6 + 5/6 (m + 1)$.
  \item $\E(N^2) = 66$, $\Var(N) = 30$, first step equation is $\E(N^2) = 1/6 + 5/6 \E((N + 1)^2)$.
  \item $\E(N \mid B) = 61/11$. Start by replacing uncoditional probabilities on the tree by conditional ones. 
  First step equation for $\mu = \E(N \mid B)$ is $\mu = 11/36 + 25/36 (\mu + 2)$.
  \item $\begin{pmatrix}
    0 & 5/6 & 1/6 & 0 \\
    5/6 & 0 & 0 & 1/6 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
  \end{pmatrix}$
\end{enumerate}

\end{sol}
\end{problem}



\begin{problem}
  Elon throws an unfair coin until ``head'' appears. 
  The probability of ``head'' is $p \in (0;1)$. 
  Let $N$ be the total number of throws. 
  \begin{enumerate}
    \item Find $\E(N)$, $\Var(N)$, $\E(N^3)$, $\E(\exp(tN))$.
    \item What is the probability than $N$ will be even?
  \end{enumerate}

  \begin{sol}
    \begin{enumerate}
      \item $\E(N) = 1/p$, $\Var(N) = $, $\E(N^3) = $, $\E(\exp(tN)) = $
      \item $a = \P(N \in 2 \cdot \NN)$, $a = (1-p)(1-a)$, $a = (1 - p) / (2 - p)$.
    \end{enumerate}
    
  \end{sol}
\end{problem}
  


\begin{problem}
  Alice and Bob throw a fair coin until the sequence $HTT$ or $THT$ appears.
  Alice wins if $HTT$ appears first, Bob wins if $THT$ appears first. 
  \begin{enumerate}
    \item Find the probability that Alice wins.
    \item Find the expected value and variance of the total number of throws. 
    \item Using any open source software find the probability that Alice wins for all possible combinations 
    of three coins sequences for Alice and Bob. 
    \item Now Alice and Bob play the following game. 
    Alice chooses her three coins winning sequence first. Next Bob, knowing the choice of Alice, chooses his three coins winning sequence. 
    Than they throw a fair coin until either of their sequences appears. 
    What is the best strategy for Alice? For Bob? What is the probability that Alice wins this game?
  \end{enumerate}
  
  \begin{sol}
  
  \end{sol}
\end{problem}

\begin{problem}
You throw a dice unbounded number of times. 
If it shows $1$, $2$ or $3$ then the corresponding amount of dollars 
is added in the pot. 
It it shows $4$ or $5$ the game stops and you get the pot with money. 
If it shows $6$ the game ends and you get nothing. 
Initially the pot is empty. 

\begin{enumerate}
  \item What is probability that the game will end by $6$?
  \item What is expected duration of the game?
  \item What is your expected payoff?
  \item What is your payoff variance?
  \item Consider variation-A of the game. 
  Rules are the same, but initially the pot contains $100$ dollars. 
  How will the answers to questions (a)-(d) change?
  \item Consider variation-B of the game. 
  Initially the pot is empty. One rule is changed. 
  If the dice shows $5$ the content of the pot is burned and the game continues. 
  How will the answers to questions (a)-(d) change?
\end{enumerate}

  \begin{sol}
  Let's denote the throws by $(X_t)$ and the number of throws by $T$.
  Thus the last throw is $X_T$. 
    \begin{enumerate}
      \item $\P(X_T = 6) = 1/3$ as we have three possible endings. 
      One may also sum the probability geometric serie or use first step analysis.
      \item $\E(T) = 0.5 + 0.5 (\E(T) + 1)$;
      \item Let $\mu = \E(S)$ and $\gamma = \P(X_T \in \{4, 5\})$.
      \[
      \mu = \frac{3}{6}\cdot 0 + \frac{1}{6}(\mu + 1\cdot \gamma) + \frac{1}{6}(\mu + 2\cdot \gamma) + \frac{1}{6}(\mu + 3\cdot \gamma)
      \]
      \item 
      \item 
      \item 
      \[
      \mu_B = \frac{2}{6}\cdot 0 + \frac{1}{6}\mu_B + \frac{1}{6}(\mu + 1\cdot \beta) + \frac{1}{6}(\mu + 2\cdot \beta) + \frac{1}{6}(\mu + 3\cdot \beta),
      \]
      with $\beta = 1/3$.
    \end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
Boris Johnson throws a fair coin until $1$ appears or until he says ``quit''.
His payoff is the value of the last throw. 
Boris optimizes his expected payoff. 
If many strategies gives the same expected payoff 
he chooses the strategy that minimizes the expected duration of the game. 

\begin{enumerate}
  \item What is the optimal strategy and the corresponding expected payoff?
  \item What is the expected duration?
  \item How the answers to points (a) and (b) will change
  if Boris should pay $0.3$ dollars for each throw?
  \end{enumerate}

  \begin{sol}
  
  \end{sol}
\end{problem}


\begin{problem}
  Winnie-the-Pooh starts wandering from the point $x=1$.
  Every minute he moves one unit left or one right with equal probabilities.

  Let $T$ be the random moment of time when he reaches $x=0$.

  \begin{enumerate}
   \item Find the generating function $g(u) = \E(u^T)$.
   \item Extract all probabilities $\P(T = k)$ from the function $g(u)$.
  \end{enumerate}

  \begin{sol}
  
  \end{sol}
\end{problem}


\begin{problem}
Gleb Zheglov catches one criminal every day. 
With probability $0.2$ the catched criminal is replaced by $w$ new criminals. 
Initially there are $n$ criminals in the town. 

What is the expected time to the ultimate crime eradication in the town?

\begin{enumerate}
  \item (4 points) Solve the problem for $w=1$ and $n=1$.
  \item (6 points) Solve the problem for arbitrary $w$ and $n$.
\end{enumerate}
  \begin{sol}
  
  \end{sol}
\end{problem}

\begin{problem}
  Consider infinite ladder with steps numbered from $0$ to infinity. 
  I start at step $0$. Every day with probability $u$ I go one step up.
  With probability $d$ I go one step down. With probability $1-u-d$ I stay on the same step.

  If I am at step $0$ then I stay there with probability $1-u$ because it's impossible to go down. 

  Consider the case $d>u$. 
  
  What is the probability that I will be at step $0$ after $10^{1000}$ days?
  \begin{sol}
  
  \end{sol}
\end{problem}

\begin{problem}
  I throw a fair die until the sequence 6-2-6 appears. Let $N$ be the number of throws.
  \begin{enumerate}
      \item What is the expected value $\E(N)$?
      \item Write down the system of linear equations for the moment generating function of $N$. You don't need to solve it!
  \end{enumerate}
  \begin{sol}
    Let's draw the chain

    \begin{center}
    \begin{tikzpicture}[->, >=stealth', auto, semithick, node distance=3cm]
    \tikzstyle{every state}=[fill=white,draw=black,thick,text=black,scale=1]
    \node[state]    (A)                     {$S$};
    \node[state]    (B)[right of=A]   {$6$};
    \node[state]    (C)[right of=B]   {$62$};
    \node[state]    (D)[right of=C]   {$626$};
    \path
    (A) edge[loop below]     node{}         (A)
        edge                node{}     (B)
    (B) edge                node{}           (C)
        edge[loop below]    node{}           (B)
        edge[bend right]    node{}           (A)
    (C) edge                node{}           (D)
        edge[bend right]                node{}           (B)
        edge[bend right=40]         node{}           (A);
    \end{tikzpicture}
    \end{center}
    
    The system of equations for expected values:
    \[
    \begin{cases}
    x_s = 1 + \frac{1}{6} x_6 + \frac{5}{6} x_s \\
    x_6 = 1 + \frac{1}{6} x_6 + \frac{1}{6} x_{62}  + \frac{4}{6} x_s \\
    x_{62} = 1 + \frac{1}{6} \cdot 0 + \frac{1}{6} x_{6}  + \frac{4}{6} x_s \\
    \end{cases}    
    \]
    
    
    The system of equations for moment generating functions:
    \[
    \begin{cases}
    m_s(t) = \exp(t) \left(\frac{1}{6} m_6(t) + \frac{5}{6} m_s(t)\right) \\
    m_6(t) = \exp(t) \left( \frac{1}{6} m_6(t) + \frac{1}{6} m_{62}(t)  + \frac{4}{6} m_s(t)\right) \\
    m_{62}(t) = \exp(t) \left( \frac{1}{6} \cdot 1 + \frac{1}{6} m_{6}(t)  + \frac{4}{6} m_s(t) \right)\\
    \end{cases}    
    \]
  \end{sol}
\end{problem}


\begin{problem}
Consider a symmetric random walk $S_t = S_0 + X_1 + X_2 +\dots + X_t$ where $(X_t)$ are
independent identically distributed and take values $+1$ or $-1$ with equal probabilities.


Random time $T$ is the first moment when we reach the value $-3$ or $7$.
Let's denote by $p_k$ the conditional probability to reach $7$ before $3$ given that we start at $S_0 = k$.

\begin{enumerate}
  \item Find $p_{-3}$ and $p_7$.
  \item Find equation on $p_k$, $p_{k-1}$ and $p_{k+1}$ using first step analysis.
  \item Guess the unique solution of this recurrence equation and hence find $p_0$.
\end{enumerate}

Now let's denote by $e_k$ the conditional expected value of $T$ given that we start at $S_0 = k$.
\begin{enumerate}[resume]
  \item Find $e_{-3}$ and $e_7$.
  \item Find equation on $e_k$, $e_{k-1}$ and $e_{k+1}$ using first step analysis.
  \item Guess the unique solution of this recurrence equation and hence find $e_0$.
  \item Generalize the formulas for $p_0$ and $e_0$ by replacing $-3$ and $7$ by arbitrary $-a < 0$ and $b > 0$. 
\end{enumerate}

  \begin{sol}
  Recurrence equation for $p_k$:
  \[
  p_k  = 0.5 p_{k+1} + 0.5 p_{k-1}, \text{ with } p_{-a} = 0, p_b = 1;
  \]
  Recurrence equation for $e_k$:
  \[
  e_k  = 0.5 e_{k+1} + 0.5 e_{k-1} + 1, \text{ with } e_{-a} = 0, e_b = 0;
  \]
  In general: $p_0 = a / (a + b)$, $e_0 = ab$.
  \end{sol}
\end{problem}


\begin{problem}
  Consider a non-symmetric random walk $S_t = S_0 + X_1 + X_2 +\dots + X_t$ where $(X_t)$ are
  independent identically distributed and take values $+1$ or $-1$ with $\P(X_t = +1) = r$
  
  
  Random time $T$ is the first moment when we reach the value $-3$ or $7$.
  Let's denote by $p_k$ the conditional probability to reach $7$ before $3$ given that we start at $S_0 = k$.
  
  \begin{enumerate}
    \item Find $p_{-3}$ and $p_7$.
    \item Find equation on $p_k$, $p_{k-1}$ and $p_{k+1}$ using first step analysis.
    \item Guess the unique solution of this recurrence equation and hence find $p_0$.
  \end{enumerate}
  
  Now let's denote by $e_k$ the conditional expected value of $T$ given that we start at $S_0 = k$.
  \begin{enumerate}[resume]
    \item Find $e_{-3}$ and $e_7$.
    \item Find equation on $e_k$, $e_{k-1}$ and $e_{k+1}$ using first step analysis.
    \item Guess the unique solution of this recurrence equation and hence find $e_0$.
    \item Generalize the formulas for $p_0$ and $e_0$ by replacing $-3$ and $7$ by arbitrary $-a < 0$ and $b > 0$. 
  \end{enumerate}
  
    \begin{sol}
      Recurrence equation for $p_k$:
  \[
  p_k  = r p_{k+1} + (1 - r) p_{k-1}, \text{ with } p_{-a} = 0, p_b = 1;
  \]
  Recurrence equation for $e_k$:
  \[
  e_k  = r e_{k+1} + (1 - r) e_{k-1} + 1, \text{ with } e_{-a} = 0, e_b = 0;
  \]
  In general:
    \end{sol}
  \end{problem}
  

\begin{problem}
Consider a symmetric random walk $S_t = X_1 + X_2 +\dots + X_t$ where $(X_t)$ are
independent identically distributed and take values $+1$ or $-1$ with equal probabilities.
We start at zero, $S_0 = 0$.

Let $N_k$ the number of visits of point $k$ before the first return of a random walk to $0$.

\begin{enumerate}
    \item What is the probability that a random walk will eventually return to $0$?
    \item What is the probability $\P(N_k = 0)$?
    \item What is the probability $\P(N_k = n)$ for arbitrary natural number $n$?
    \item Compare $\E(N_2)$ and $\E(N_{\text{undecillion}})$.
\end{enumerate}
  \begin{sol}
  
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
  
  \end{sol}
\end{problem}




\begin{problem}
  \begin{sol}
  
  \end{sol}
\end{problem}



\begin{problem}
  \begin{sol}
  
  \end{sol}
\end{problem}



\begin{problem}
  \begin{sol}
  
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
  
  \end{sol}
\end{problem}




\section{Markov chains}

\begin{problem}
HSE student lives in two states: "sleep" and "study" and tries to change the state every 1 hour. 
After the sleep state the student continues sleeping with probability equal to 0.25, 
otherwise a student starts studying. 
If the student is studying, the probabilities to continue studying and to start sleeping are equal.

\begin{enumerate}
\item Write down the transition matrix of this Markov chain.
\item Draw the graph representation.
\item What is the probability that a Sleeping Student will be a Studying Student after 1 hour? After 2 hours?
\item We know that initially student is sleeping with probability $p=\frac{2}{3}$. 
Find the probabilities of sleep and study states after 1 and 2 hours. 
\item Find the probabilities of sleep and study states after 20 and 100 hours (do it with \textbf{matrix} operations and any soft). 
Is there any difference and why?  
\end{enumerate}

\begin{sol}
  
\end{sol}

\end{problem}

\begin{problem}
HSE student has three states: pre-coffee, with-coffee and over-coffee. 
He goes to Jeffrey's each break seeking for a cup of coffee. 
The line is usually too long, so probability to stay pre-coffee is equal to 60\% and to be over-coffee — is zero. 
Caffeinated students can stay in lines longer, so for with-coffee student the probability to become over-coffee is 20\% and to become pre-coffee — 30\%. 
Over-coffee student runs to coffeeshop very fast and able to stay over-coffeed with $p = 0.70$ 
and can suddenly become pre-coffee with $p = 0.10$. 

\begin{enumerate}
\item Draw the graph representation of this Markov chain.
\item Write the transition matrix of this Markov chain.
\item What is the probability that morning pre-coffee student will be with-coffee after 1 break? After 3 breaks?
\item What is the probability that morning pre-coffee student will be with-coffee after 1 break? After 3 breaks? After 200 breaks?
\end{enumerate}

\begin{sol}
  
\end{sol}
\end{problem}


\begin{problem}
Unteachable students in NOTHSE University try to pass the exams. 
Students cheat successfully and pass the exams with probability 10\%. 
In the case of a failure students are allowed to infinite number of retakes. 
All students are unteachable so the amount of knowledge is always the same and doesn't depend of the number of retakes.

\begin{enumerate}
\item Draw the graph representation of this Markov chain.
\item What is the probability to graduate using no more than 5 retakes? 
\item What is the probability to graduate eventually? 
\item Use \textbf{first step analysis} to find the average number of retakes per student in this University.
\end{enumerate}

\begin{sol}
  
\end{sol}

\end{problem}

\begin{problem}
Every month the real estate Galina agent has two options: to increase her commission and to ask an owner to increase the rent. 
If the agent has increased the commission, on the next step she increases the commission again with probability $5/8$. 
If she has asked the owner, she decides to increase the commission with probability equal to $3/4$ on the next step.

\begin{enumerate}
\item Write the transition matrix of this Markov chain.
\item Draw the graph representation.
\item Use \textbf{first step analysis} to find how many steps the agent does between asking the owner to increase the price.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
Alice and Bob toss a coin, writing down the results.
 If the last 3 tosses are Head, Head and Tail, Alice wins. 
 If the last 3 tosses are Tail, Head and Head, Bob wins.

\begin{enumerate}
\item Is it easy to work with matrix representation in this case?
\item Draw the graph representation. Who is more likely to win the game?
\item Use \textbf{first step analysis} to find the probability of Alice's win.
\item Find the probability that the game ends in exactly 4 tosses.
\item Find the expected value and variance of the total number of coin throws in the game.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
HSE student has an unusually caring granny who cooks one pie with probability $0.7$ every weekend. 
Granny's pies are so tasty that HSE student can't resist and he gains 1 kilo for each pie eaten. 
Without pies the student with more than 70 kilos weight looses 1 kilo per week, yeah, he has a lot of studies!
At the beginning of the study year student's weight is $W_0 = 70$ kilos.

Let $W_t$ be the weight of the student $t$ weeks later. 

\begin{enumerate}
\item Find the probability $\P(W_3 \geq 71)$ and expected value $\E(W_3)$.
\item Find the limit weight after infinitely many study weeks $\lim_{t\to\infty} W_t$.
\item Explain whether the chain $(W_t)$ has a stationary distribution.
\end{enumerate}

\begin{sol}
\begin{enumerate}
    \item 
    \item $\E(W_t) \to + \infty$;
    \item No stationary distribution. For stationary distribution $\E(W_t)$ can't tend to infinity. 

\end{enumerate}

\end{sol}
\end{problem}


\begin{problem}
The fair price of Sborbank in discrete stock market is somewhere between 100 and 101 rubles. 
If the price is equal to 100, then the price grows up by 1 ruble with probability $\frac{9}{10}$, 
otherwise it goes down by 1 ruble. 
If the price is greater then 100, it grows by 1 ruble with probability $\frac{1}{3}$ or declines by 1 ruble. 
If the price is lower than 100, it grows by 1 ruble with probability $\frac{2}{3}$ or declines by 1 ruble. 

\begin{enumerate}
\item Draw the graph representation of the corresponding Markov chain.
\item Do you think this chain has some stationary distribution?
\item Find the average time for the stock price to fall from 102 rubles to 98 rubles. 

Hint: you may to decompose the long path into smaller ones and to use the first step analysis.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
The hedgehog Melissa starts at the vertex $A$ of a triangle $\Delta ABC$.
Each minute she randomly moves to an adjacent vertex with probabilities $\P(A \to B) = 0.7$, 
$\P(A \to C) = 0.3$, $\P(B \to C) = \P(B \to A) = 0.5$,  $\P(C \to B) = \P(C \to A) = 0.5$.

\begin{enumerate}
  \item What is the probability that she will be in vertex $B$ after 3 steps?
  \item Write down the transition matrix of this Markov chain. 
  \item What proportion of time Melissa will spend in each state in the long run?
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\P(S_3 = B) = \P(A\to B\to A\to B) + \P(A\to C\to A\to B) + \P(A\to B\to C\to B)$
    \item 
    \[
    \begin{pmatrix}
      0 & 0.7 & 0.3 \\
      0.5 & 0 & 0.5 \\
      0.5 & 0.5 & 0 \\
    \end{pmatrix}  
    \]
    \item 
    \[
    \begin{cases}
      a = 0.5 b + 0.5 c \\
      b = 0.7 a + 0.5 c \\
      c = 0.3 a + 0.5 b \\
      a + b + c = 1  
    \end{cases}
    \]
    The solution is $a= 15/45$, $b=17/45$, $c=13/45$.
  \end{enumerate}
\end{sol}
\end{problem}
  


\begin{problem}
A Hedgehog starts at the point $x=2$ on the real line. 
Every minute he moves one step left with probability $0.3$ or one step right with probability $0.7$.
There are two exceptions from this rule: the absorbing point $x=0$ and the reflecting barrier at $x=3$.

If the Hedgehog reaches the absorbing point $x=0$ then he stops moving and stays there. 
If the Hedgehog reaches the reflecting barrier $x=3$ then his next move will be one step left with probability $1$.

\begin{enumerate}
\item {[2]} Write the transition matrix of this Markov chain. 
\item {[3]} What is the probability that Hedgehog will be at $x=1$ after exactly 3 steps?
\item {[5]} What is the expected time to reach the absorbing state?
\end{enumerate}
\begin{sol}
$\P(X_3 = 1) = 0.3\cdot 0.7\cdot 0.3 + 0.7\cdot 1\cdot 0.3 = 0.21 \cdot 1.3$
    Let's denote $\tau_j = \min \{t \mid X_t = 0, X_0 = j\}$, $\mu_j = \E(\tau_j)$.
    \[
    \begin{cases}
        \mu_0 = 0 \\
        \mu_1 = 1 + 0.7\mu_2 \\
        \mu_2 = 1 + 0.3\mu_1 + 0.7\mu_3 \\
        \mu_3 = \mu_2 + 1 
    \end{cases}    
    \]
    We get $\mu_2 = 200/9$.
\end{sol}
\end{problem}


\begin{problem}
Vampire Petr drinks blood of a new victim every day. 
Unfortunately 20\% of the population are vaccinated against vampires. 
If more than one victim of the last three victims are vaccinated then Petr will be instantaneously cured and will return to the normal life. 

For simplicity let's assume that the last three victims were not vaccinated. 

\begin{enumerate}
    \item What is the probability that vampire Petr will be cured in the next three days?
    \item How many victims will be bitten by vampire Petr on average?
\end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}
  


\begin{problem}
  A hedgehog moves at random on the vertices $A$, $B$, $C$ and $D$ of a regular tetrahedron (тетраэдр).
  She start at the vertex $A$ and every minute changes her position to one of the adjacent vertices with probability $1/3$
  independently of past moves. 
  
  \begin{enumerate}
      \item Write down the transition matrix of this Markov chain. 
      \item What is the expected time of the first return to the starting vertex $A$?
  \end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  The Cat can be only in two states: Sleeping ($S$) and Eating ($E$). 
  Cat's mood depends only on the previous state. 
  The transition probabilities are given below:
  
  %\begin{minipage}
  \begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
      \node [circle, draw] (one) {$E$};
      \node [circle, draw] (two) [right of=one] {$S$};
      \path (one) edge [bend left] node [above] {$0.1$} (two);
      \path (two) edge [bend left] node [below]{$0.2$} (one);
      \path (two) edge [loop right] node {} (two);
      \path (one) edge [loop left] node {} (one);
  \end{tikzpicture}
  %\end{minipage}
  
  
  \begin{enumerate}
      \item Compute the missing probabilities on the graph.
      \item Write down the transition matrix.
      \item Compute $\P(X_3 = \text{Eating} \mid X_0 = \text{Eating})$.
  \end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  Cowboy Joe enters the Epsilon Bar and orders one pint of beer. 
  He drinks it and orders one pint more. 
  And so on and so on and so on\dots{ }
  The problem is that the barmaid waters down each pint with probability $0.2$ independently of other pints.
  Joe does not like watered down beer. 
  He will blow the Epsilon Bar to hell if two or more out of the last three pints are watered down. 
  
  We point out that Joe never drinks less than 3 pints in a bar. 
  
  \begin{enumerate}
      \item What is the expected number of pints of beer Joe will drink?
  \end{enumerate}
  
  Let $Y_t$ be the indicator that the pint number $t$ was watered down. 
  Consider the Markov chain $S_t = (y_{t-2}, y_{t-1}, y_t)$.
  For example, $S_t = (100)$ means that the pint number $t-2$ was watered down while pints number $t-1$ and $t$ are good. 
  
  \begin{enumerate}[resume]
  \item What are the possible values of $S_3$ and their probabilities?
  \item Write down the transition matrix of this Markov chain.
  \end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  Pavel Durov starts at the point $X_0 = 3$ on the real line. 
  Each minute he moves left with probability $0.4$ or right with probability $0.6$ independently of past moves.
  The points $0$ and $5$ are absorbing. 
  If Pavel reaches $0$ or $5$ he stays there forever.
  Let $X_t$ be the coordinate of Pavel after $t$ minutes. 
  
  \begin{enumerate}
      \item Write down the transition matrix of this Markov chain. 
      \item Calculate the distribution of $X_7$ [list all values of the random variable $X_7$ and estimate the probabilities].
  \end{enumerate}
  
  Hint: you are free to use python for this problem :)
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  Consider two identical hedgehogs starting at the vertices $A$ and $B$ of a polygon $ABCD$. 
  Each minute each hedehog simulteneously and independently chooses to go clockwise to the adjacent point, to go counter-clockwise to the adjacent point or to stay at his location.
  
  Thus the brotherhood of two brave hedgehogs can be in three states: in one vertex, 
  in two adjacent vertices, in two non-adjacent vertices.


  \begin{enumerate}
    \item Draw the graph for the brotherhood Markov chain and calculate all transition probabilities. 
    \item Write down the transition matrix of the brotherhood Markov chain. 
    \item What is the probability that they will be in one vertex after 3 steps?
  \end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  Consider the following Markov chain:

  \begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
      \node [circle, draw] (one) {$A$};
      \node [circle, draw] (two) [right of=one] {$B$};
      \node [circle,  draw] (three) [right of=two] {$C$};
      \path (one) edge [bend left] node [above] {$0.3$} (two);
      \path (two) edge [bend right] node [below] {$0.3$} (three);
      \path (two) edge [bend left] node [below]{$0.7$} (one);
      \path (three) edge [loop right] node {$0.2$} (three);
      \path (three) edge [bend right] node [above] {$0.8$} (two);
      \path (one) edge [loop left] node {$0.7$} (one);
  \end{tikzpicture}

  \begin{enumerate}
      \item Find the stationary distribution of this Markov chain.
  \end{enumerate}
  
  The Markov chains starts at the vertex $A$.
  Let $N$ be the first moment when the state $C$ will be reached.

  \begin{enumerate}[resume]
    \item Find the expected value $\E(N)$.
    \item Find the variance $\Var(N)$. 
  \end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  Design a Markov chain with 3 states and unique stationary distribution $\pi = (0.1, 0.2, 0.7)$.
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  Consider three games:
  \begin{enumerate}
      \item[Game A:] You toss a biased coin with probability $0.48$ of $H$. 
      You get $+1$ dollar for $H$ and $-1$ dollar for $T$.  

      \item[Game B:] If your wellfare is divisible by three you toss a coin that
      lands on $H$ with probability $0.09$. 
      If your wellfare is not divisible by three you toss a coin that lands on $H$ with probability $0.74$.
      You get $+1$ dollar for $H$ and $-1$ dollar for $T$.  
      
      \item[Game C:] You toss an unbiased coin.
      If it lands on $H$ you play Game A. If it lands on $T$ you play Game B. 
  \end{enumerate}
  
  Your initial capital is $10000\$$.

  \begin{enumerate}
      \item Generate and plot two random trajectories of your wellfare if you play Game A $10^6$ times.
      \item Generate and plot two random trajectories of your wellfare if you play Game B $10^6$ times.
      \item Generate and plot two random trajectories of your wellfare if you play Game C $10^6$ times.
  \end{enumerate}

% game_a = function(x) {
% a = sample(c(-1, +1), size = 1, prob = c(0.52, 0.48))
% return(x + a)
% }

% game_b = function(x) {
% a = sample(c(-1, +1), size = 1, prob = c(0.25, 0.75))
% b = sample(c(-1, +1), size = 1, prob = c(0.91, 0.09))
% if ((x %% 3) == 0) {
%     res = x + b
% } else {
%     res = x + a
% }
% return(res)
% }

% game_c = function(x) {
% a = sample(c(-1, +1), size = 1, prob = c(0.5, 0.5))
% if (a == 1) {
%     res = game_a(x)
% } else {
%     res = game_b(x)
% }
% return(res)
% }  
  
% w = 100
% for (i in 1:1000000) {
% print(w)
% w = game_b(w)
% }
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}


\section{Classification of states}

\begin{problem}
We randomly wander on the graph choosing at each moment of time one of the possible directions.
If probabilities are not given we choose equiprobably.   

\begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
  \node [circle, draw] (A) {$a$};
  \node [circle, draw] (B) [right of=A] {$b$};
  \node [circle, draw] (C) [right of=B] {$c$};
  \node [circle, draw] (D) [below of=A] {$d$};
  \node [circle, draw] (E) [right of=D] {$e$};
  \node [circle, draw] (F) [right of=E] {$f$};

  \path (A) edge (D);
  \path (B) edge (A);
  \path (B) edge (C);
  \path (D) edge (B);
  \path (C) edge (F);
  \path (B) edge (F);
  \path (E) edge [bend left] (F);
  \path (F) edge [bend left] (E);



  % \path (one) edge [bend left] node [above] {$0.1$} (two);
  %\path (two) edge [bend left] node [below]{$0.2$} (one);
  %\path (two) edge [loop right] node {} (two);
  %\path (one) edge [loop left] node {} (one);
\end{tikzpicture}


\begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
  \node [circle, draw] (A) {$a$};
  \node [circle, draw] (B) [right of=A] {$b$};
  \node [circle, draw] (C) [below of=B] {$c$};
  \node [circle, draw] (D) [below of=A] {$d$};
  \node [circle, draw] (E) [right of=B] {$e$};

  \path (A) edge [bend left] (B);
  \path (B) edge [bend left] (A);
  \path (B) edge [bend left] (C);
  %\path (A) edge (C);
  \path (C) edge [bend left] (B);
  \path (C) edge [bend left] (D);
  \path (D) edge [bend left] (C);
  \path (D) edge [bend left] (A);
  \path (A) edge [bend left] (D);

  \path (B) edge (E);
  \path (E) edge [loop right] (E);
\end{tikzpicture}


\begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
  \node [circle, draw] (A) {$a$};
  \node [circle, draw] (B) [right of=A] {$b$};
  \node [circle, draw] (C) [right of=B] {$c$};
  \node [circle, draw] (D) [below of=A] {$d$};
  \node [circle, draw] (E) [right of=D] {$e$};
  \node [circle, draw] (F) [right of=E] {$f$};
  \node [circle, draw] (G) [right of=C] {$g$};


  \path (A) edge (D);
  \path (D) edge (E);
  \path (E) edge (A);
  \path (B) edge (E);
  \path (B) edge (C);
  \path (C) edge (F);
  \path (C) edge (G);
  \path (F) edge (B);


  \path (D) edge [loop left] (D);
  \path (E) edge [loop right] (E);
  \path (B) edge [loop left] (B);
  \path (G) edge [loop below] (G);
\end{tikzpicture}


\begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=2cm]
  \node (A) {\dots};
  \node [circle, draw] (B) [right of=A] {$-1$};
  \node [circle, draw] (C) [right of=B] {$0$};
  \node [circle, draw] (D) [right of=C] {$1$};
  \node (E) [right of=D] {\dots};

  \path (A) edge [bend left] node [above] {$0.1$} (B);
  \path (B) edge [bend left] node [below] {$0.9$} (A);
  \path (B) edge [bend left] node [above] {$0.1$} (C);
  \path (C) edge [bend left] node [below] {$0.9$} (B);
  \path (C) edge [bend left] node [above] {$0.1$} (D);
  \path (D) edge [bend left] node [below] {$0.9$} (C);
  \path (D) edge [bend left] node [above] {$0.1$} (E);
  \path (E) edge [bend left] node [below] {$0.9$} (D);
\end{tikzpicture}



\begin{enumerate}
  \item Split each Markov chain into communicating classes. 
  \item Find the period of every state. 
  \item Classify each state as transient, null-recurrent and positive recurrent.
  \item For positive recurrent states find the expected return time.
  \item Find all stationary distributions. 
\end{enumerate}


  \begin{sol}
    
  \end{sol}
\end{problem}


\begin{problem}
  A Knight randomly wanders on the chessboard. 
  At each step he randomly chooses one of the possible Knight-moves with equal probabilities. 
  
\begin{enumerate}
    \item Find the stationary distribution. 
    \item Find the expected return time for every square.
    \item Find the period of every square. 
\end{enumerate}
  
\begin{sol}
      
\end{sol}
\end{problem}
  
\begin{problem}
Consider the Markov chain with the transition matrix
\[
  P = \begin{pmatrix}
    0.2 & 0.2 & 0 & 0.6 \\
    0.3 & 0.3 & 0.4 & 0 \\
    0 & 0 & 0.1 & 0.9 \\
    0 & 0 & 0.8 & 0.2 \\
  \end{pmatrix}.
\]

\begin{enumerate}
  \item (3 points) Split the chain in classes and classify them into closed or not closed.
  \item (2 points) Classify the states into recurrent or transient.
  \item (5 points) A Hedgehog starts in the state one and moves 
  randomly between states according to the transition matrix.

  What is the approximate probability that the Hedgehog will be in the 
  state four after $10^{2021}$ moves?
\end{enumerate}

\begin{sol}
      
\end{sol}
\end{problem}



\begin{problem}
  Consider the Markov chain with the transition matrix
\[
  P = \begin{pmatrix}
    0.2 & 0.2 & 0 & 0.6 & 0 \\
    0.3 & 0.3 & 0.4 & 0 & 0\\
    0 & 0 & 0.3 & 0.7 & 0 \\
    0 & 0 & 0.8 & 0.2 & 0 \\
    0 & 0 & 0 & 0 & 1 \\
  \end{pmatrix}.
\]

\begin{enumerate}
  \item (3 points) Split the chain in classes and classify them into closed or not closed.
  \item (2 points) Classify the states into recurrent or transient.
  \item (5 points) A Hedgehog starts in the state one and moves 
  randomly between states according to the transition matrix.

  What is the approximate probability that the Hedgehog will be in the 
  state four after $10^{2021}$ moves?
\end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}

  \begin{sol}
      
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
      
  \end{sol}
\end{problem}



\section{Generating functions}

\begin{problem}
The MGF (moment generating function) of the random variable $X$ is give by $M(t) = 0.3 \exp(2t) + 0.2 \exp(3t) + 0.5 \exp(7t)$.

Recover the distribution of the random variable $X$.
  \begin{sol}

  \end{sol}
\end{problem}



\begin{problem}
The random variable $Y$ takes values $1$, $2$ or $3$ with equal probabilities.

Find the MGF of the random variable $Y$.
  \begin{sol}
  $M(t) = (\exp(t) + \exp(2t) + \exp(3t))/3$.
  \end{sol}
\end{problem}


\begin{problem}
The MGF of the random variable $W$ has a Taylor expansion that starts with $M(t) = 1 + 2t + 7t^2 + 20t^3 + \ldots$.

Find $\E(W)$, $\Var(W)$, $\E(W^3)$.

  \begin{sol}
  $\E(W) = 2$, $\Var(W) = 7\cdot 2 - 2^2$, $\E(W^3) = 20 \cdot 3!$.
  \end{sol}
\end{problem}


\begin{problem}
The random variable $X$ takes non-negative integer values.
The generating function $g(u) = \E(u^X)$ has a Taylor expansion that
starts with $g(u) = 0.1 + 0.2 u + 0.15 u^2 + \ldots$.

Find probabilities $\P(X = 0)$, $\P(X = 1)$, $\P(X = 2)$.

  \begin{sol}
  $\P(X = 0) = 0.1$, $\P(X = 1) = 0.2$, $\P(X = 2) = 0.15$.
  \end{sol}
\end{problem}




\begin{problem}
  Random variables $X_i$ are mutually independent and $X_i$ has Gamma distribution $\dGamma(\alpha_i, \beta_i)$.

  I sum up the random number $N$ of terms,
  \[
   S = X_1 + X_2 + \ldots + X_N.
  \]
  The number $N$ has Poisson distribution $\dPois(\lambda)$ and is independent of the sequence $(X_i)$.

  \begin{enumerate}
   \item Find the MGF of $S$. You may the MGF formula for Gamma distribution as known.
   \item Find $\E(S)$ and $\Var(S)$.
  \end{enumerate}

  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  The random variable $X$ take non-negative integer values.
  Its moment generating function is equal to $M(t) = (2 - \exp(t))^{-7}$.

  \begin{enumerate}
   \item Find the probability generating function $g(u) = \E(u^X)$.
   \item Find $\E(X)$, $\Var(X)$, $\P(X = 0)$, $\P(X = 1)$, $\P(X = 2)$.
   \item Find $\P(X = k)$.
  \end{enumerate}


  \begin{sol}
      $g(u) = g(\exp(t)) = \E(\exp(tX)) = M(t)$
  \end{sol}
\end{problem}


\begin{problem}
The number of players $N$ who will win the lottery
is a random variable with probability mass function $\P(N = k) = 7\cdot 0.3^k / 3$ for $k\geq 1$.
Each player will get a random prize $X_i \sim U[0;1]$.
All random variables are independent. 
Let $S$ be the sum of all the prizes. 

\begin{enumerate}
  \item Find $\E(S \mid N)$ and conditional moment generating function $M_{S\mid N}(u)$.
  \item Find the unconditional moment generating function $M_S(u)$.
  \item What is the probabilistic meaning of $M_S''(0) - (M_S'(0))^2$? 
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item Consider $N$ as known fixed value, $\E(S \mid N) = N\E(X_1) = N\cdot 0.5$.
    First, let's find moment generating funciton for $X_i$:
    \[
    M_X(u) = \int_0^1 \exp(xu) \cdot 1 \, dx = \frac{\exp(u) - 1}{u}; 
    \]
    Hence $M_{S\mid N}(u) = (M_X(u))^N$ as $S$ is the sum of $N$ independent variables.
    \item Random variable $N$ is discrete, $M_S(u) = \P(N=1)(M_X(u))^1 + \P(N=2)(M_X(u))^2 + \ldots = \frac{0.7 M_X(u)}{ 1 -0.3M_X(u)}$.
    \item Moment generating function is used to calculate moments, $M_S''(0) - (M_S'(0))^2 = \Var(S)$.
  \end{enumerate}
\end{sol}
\end{problem}

\begin{problem}
Prince Myshkin throws a fair coin until two consecutive heads appear. 
Let $N$ be the number of throws. 
  
Find the moment generating function of $N$. 
  \begin{sol}
    
  \end{sol}
\end{problem}

\begin{problem}
  The moment generating function of a random variable $X$ is $1/(1-2t)$.
  \begin{enumerate}
      \item Find the moment generating function of $2X$.
      \item Find the moment generating function of $X + Y$ where $X$ and $Y$ are independent and identically distributed.
      \item Do you remember the sum of geometric progression? Find $\E(X^{2021})$.
  \end{enumerate}
  \begin{sol}
    
  \end{sol}
\end{problem}

\begin{problem}
  The ultimate goal of this exercise is to prove the good upper bound for tail probability of a normal distribution: 
  if $X\sim \cN(0; \sigma^2)$ then $\P(X > c) \leq \exp(-c^2/2\sigma^2)$.
  
  Here are the guiding hints (you free to use not use them): 
  
  \begin{enumerate}
    \item State the MGF of $X$. You may derive it or simply write it if you remember.
    \item Consider $Y = \exp(uX)$. Using Markov inequality provide the upper bound for $\P(Y > \exp(uc))$.
    \item Prove that $\P(X > c) \leq MGF_X(u)\exp(-uc)$ for any $u$.
    \item Find the value of $u$ that makes the upper bound as tight as possible. 
  \end{enumerate}
  \begin{sol}
    
  \end{sol}
\end{problem}

\begin{problem}
  I have an unfair coin with probability of heads equal to $h \in (0;1)$.
  \begin{enumerate}
      \item Let $N$ be the number of tails before the first head. Find the MGF of $N$.
      \item Let $S$ be the number of tails before $k$ heads (not necessary consecutive). Find the MGF of $S$.
      \item What is the limit of $MGF_S(t)$ when $k \to \infty$ and $k \times h \to 0.5$? 
      \item What is the limit of $MGF_S(t)$ when $k \to \infty$ and $k \times (1 - h) \to 0.5$? 
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item Moment generating function
  \[
  m_N(t) = \sum_{j=0} \exp(tj) (1-h)^j h = h \sum_{j=0} (\exp(t) (1-h))^j = \frac{h}{1 - \exp(t) (1 - h)}  
  \]
  \item As $S = N_1 + N_2 + \ldots + N_k$:
  \[
  m_S(t) =  \left( \frac{h}{1 - \exp(t) (1 - h)} \right)^k
  \]
  \item $0$;
  
  \item 
  
  \end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
  I have three problems in the home assignment. 
  Time spent on each problem is modelled by independend exponentially distributed random variables with rate $\lambda$: $X_1$, $X_2$, $X_3$.

  \begin{enumerate}
      \item Find the moment generating function of $X_i$ and hence the moment generating function of $S = X_1 + X_2 + X_3$.
      \item Find $\E(S^3)$.
      \item Find the joint density of $R = X_1 / (X_1 + X_2 + X_3)$ and $S$.
  \end{enumerate}
  \begin{sol}
    
  \end{sol}
\end{problem}

\begin{problem}
  Recognise the distribution family and its parameters by looking at the moment-generating function:
    
  \begin{enumerate}
      \item $0.7 + 0.3\exp(t)$;
      \item $\exp(2024\exp(t)) / \exp(2024)$;
      \item $\frac{\exp(3t) - 1}{3t\exp(-2t)}$;
      \item $\exp(6t + 2024t^2)$;
      \item $1/(5t - 1)^{2024}$.
  \end{enumerate}
  
  You may use the table from the article 
  
  \url{https://en.wikipedia.org/wiki/Moment-generating_function}.
  \begin{sol}
    
  \end{sol}
\end{problem}



\begin{problem}
  Consider the moment-generating function of a random variable $X$:
  \[
   g(t) = \frac{\exp(3t) - 1}{3t\exp(-2t)}.
  \]
  
  \begin{enumerate}
      \item Expand the function $g(t)$ as Taylor series up to $t^4$ included. 
      \item Find $\E(X)$, $\E(X^2)$, $\E(X^3)$, $\E(X^4)$.
  \end{enumerate}
  \begin{sol}
    
  \end{sol}
\end{problem}



\begin{problem}
  The moment-generating function of the pair of random variables $(X, Y)$ is given by 
  $\exp(6t_1 + 5t_2 + t_1^2 + 20t_2^2 - 2t_1t_2)$.

  Find $\E(X)$, $\Var(Y)$, $\E(XY)$.
  \begin{sol}
    
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}



\section{Inequalities}

\begin{problem}
  I have $100$ numbers written on small sheets of paper: $x_1$, $x_2$, \ldots, $x_{100}$. The sum of these numbers is $1$. 
    
    Find the possible values of the sum 
    \[
    \frac{x_1}{\sqrt{1-x_1}} +     \frac{x_2}{\sqrt{1-x_2}} + \ldots + \frac{x_{100}}{\sqrt{1-x_{100}}}.
    \]
    

    Hint: consider a randomly selected number $X$ and apply the Jensen's inequality.
  \begin{sol}
    
  \end{sol}
\end{problem}


\begin{problem}
Ordinary non-random sequences also have alternative limit consept!
The real number $c$ is a Cesaro limit of a sequence $(a_n)$ if 
this number is a limit of a sequence of cumulative averages, $c = \lim \bar{a}_n$.

\begin{enumerate}
  \item Consider a sequence $(a_n) = (1, 0, 1, 0, 1, 0, \dots)$. 
  Does it have ordinary limit? Does it have Cesaro limit?
  \item Construct an aperiodic sequence that does not have ordinary limit but has Cesaro limit. 
  \item Is it true that any ordinary convergent sequence in also Cesaro-converges?  
\end{enumerate}
You now that $(a_n)$ Cesaro-converges to $a$ and $(b_n)$ Cesaro-converges to $b$.

\begin{enumerate}[resume]
  \item Is it true that $(a_n + b_n)$ is Cesaro-convergent?
  \item Is it true that $(a_n \cdot  b_n)$ is Cesaro-convergent?
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item Cesaro-limit is $1/2$. It's like an average long-term salary. 
  \item Split the sequence into groups of two numbers. 
  In each group put one $0$ and one $1$ in an aperiodic way. 
  \item Yes, ordinary convergences implies Cesaro-convergence. 
  \item Yes, Cesaro-limit of a sum is a sum of Cesaro-limits. 
  \item No. 
  One sequence has $1$ at even places, the other sequence has $1$ at odd places.
\end{enumerate}    
\end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}






\section{Limits}

\begin{problem}
Polina loves sweet chestnuts. 
She has infinite sequence of baskets before her.
In the basket number~$n$ there are $n$~chestnuts in total.
Unfortunately only one chestnut in every basket is a sweet one. 

She picks chestnuts one by one at random from all the buskets sequentially. 
First she picks the unique chestnut from the basket number one, 
than she picks in a random order two chestnuts from the basket number two and so on. 

The random variable $S_t$ indicates whether the chestnut number $t$ was a sweet one. 

\begin{enumerate}
  \item Find $\lim S_t$ or prove that the limit does not exist.
  \item Find $\plim S_t$ or prove that the limit does not exist.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Let $(X_n)$ be independent, each variable $X_n$ has exponential distribution with rate $\lambda_n = n$.
\begin{enumerate}
  \item Find the probability limit $\plim X_n$ or prove that it does not exist.
\end{enumerate}
Let $(Y_n)$ be independent, each variable $Y_n$ has exponential distribution with rate $\lambda_n = n / (n + 1)$.
\begin{enumerate}[resume]
  \item Find the probability limit $\plim Y_n$ or prove that it does not exist.
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item $\plim X_n = 0$;
  \item $\plim Y_n$ does not exist.
\end{enumerate}

\end{sol}
\end{problem}


\begin{problem}
Let $(X_n)$ be independent normally distributed $\cN(5; 10)$.

\begin{enumerate}
  \item Find the probability limit 
  \[
  \plim \frac{X_1 + X_2 + \dots + X_n}{7n}.
  \]
  \item Find the probability limit 
  \[
  \plim \frac{X_1^2 + X_2^2 + \dots + X_n^2}{7n}.
  \]
  \item Find the probability limit 
  \[
  \plim \ln(X_1^2 + X_2^2 + \dots + X_n^2) - \ln n.
  \]
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item $\plim \frac{X_1 + X_2 + \dots + X_n}{7n} = 5/7$;
  \item $\plim \frac{X_1^2 + X_2^2 + \dots + X_n^2}{7n} = 5$;
  \item $\plim \ln(X_1^2 + X_2^2 + \dots + X_n^2) - \ln n = \ln 35$.
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
  Let $(X_n)$ be independent uniform on $[0; 1]$.
  Let $Y_n = X_n^2 + X_n^3$.

  
  \begin{enumerate}
    \item Find the probability limit $\plim V_n$ for 
    \[
    V_n = \max\{Y_1, Y_2, \dots, Y_n\}.
    \]
    \item Find the probability limit $\plim W_n$ for 
    \[
    W_n = \max\{X_1 + Y_1, X_1 + Y_2, \dots, X_1 + Y_n\}.
    \]
  \end{enumerate}
  \begin{sol}
  \begin{enumerate}
    \item $\plim \max\{Y_1, Y_2, \dots, Y_n\} = 2$;
    \item $\plim \max\{X_1 + Y_1, X_1 + Y_2, \dots, X_1 + Y_n\} = X_1 + 2$.
  \end{enumerate}
  \end{sol}
  \end{problem}

\begin{problem}
Consider the random variable $X$ and the sequence of random variables $Y_n$ with $\E(Y_n) = \frac{1}{n}$ 
and $\Var(Y_n) = \frac{\sigma^2}{n}$. 
Let $W_n = X + Y_n$.

\begin{enumerate}
        \item Find the probability limit $\plim Y_n$;
        \item Find the probability limit $\plim W_n$.
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\plim Y_n = 0$;
    \item $\plim W_n = X$.
  \end{enumerate}
  \end{sol}
\end{problem}
  

\begin{problem}
  The random variables $X_i$ are independent and uniformly distributed on $[0; 1]$.  
  Let $Y_n = \min{X_1, \ldots X_n}$. 

  \begin{enumerate}
        \item Find the almost sure limit of $Y_n$;
        \item Find the probability limit of $Y_n$;
        \item Find the limiting distribution of $Y_n$.
  \end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\P(\lim Y_n = 0 )=1$;
    \item $\plim Y_n = 0$;
    \item Limiting distribution is a constant $0$.
\end{enumerate}
\end{sol}

\end{problem}


\begin{problem}
Let $X$ and $Y$ be independent and uniformly distributed on $[0; 1]$.
Let $V_n = n^2 Y \cdot I(X \leq 1/n)$ and $W_n = Y \cdot I(X > 1/n)$. 
  \begin{enumerate}
        \item Find $\plim V_n$ and $\plim W_n$.
        \item Does $(V_n)$ converge in mean squared?
        \item Does $(W_n)$ converge in mean squared?
  \end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\plim V_n = 0$, $\plim W_n = Y$;
    \item The sequence $V_n$ does not converge in mean squared;
    \item  $W_n$ converges to $Y$ in mean squared.
\end{enumerate}
\end{sol}

\end{problem}


\begin{problem}
  \begin{enumerate}
    \item As a warm-up find the limit
    \[
    \lim_{n\to\infty} \frac{2n^2 + 3n + 6}{5n^2 + 2n + 9}.
    \]
  \end{enumerate}
  Now consider the sequence with parameters:
  \[
  X_n = \frac{L n^2 + 3n + 6}{R n^2 + 2n + 9}
  \]
  \begin{enumerate}[resume]
    \item For each value of parameters $L$ and $R$ find the limit $\lim X_n$.
    \item Find the almost surely limit of $X_n$ if $L$ and $R$ are independent and $\dUnif[0;1]$.
    Does the pointwise limit exist?
    \item Random variables $L$ and $Q$ be independent and take values $0$ or $1$ with equal probabilities.
    Let $R = L + Q$. Find the almost surely limit of $X_n$ in terms of $L$ and $R$.
    Does the pointwise limit exist?
  \end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $2/5$;
    \item $L/R$ or $+\infty$ or $-\infty$ or $3/2$.
    \item Almost surely limit is $L/R$, pointwise limit does not exist.
    \item Pointwise and almost surely limits are $\frac{2}{5} I(R>0) + \frac{3}{2} I(R=0)$.
  \end{enumerate}
\end{sol}
\end{problem}

\begin{problem}
Consider the sequence $Y_n = U^n$ with parameter $U$.

\begin{enumerate}
  \item Find the ordinary limit of $Y_n$ for all values of $U$ for which the sequence converges.
  \item Find the almost surely limit of $Y_n$ if $U\sim \dUnif[0;1]$.
  \item What is the probability that $Y_n$ converges if $U \sim \dUnif[0;2]$?
  \item What is the probability that $Y_n$ converges if $U$ takes values $+1$ or $-1$ with equal probabilities?
  \item Does $Y_n$ converges in distribution if $U$ takes values $+1$ or $-1$ with equal probabilities?
\end{enumerate}
\begin{sol}
\begin{enumerate}
  \item $0$, $1$, $+\infty$ or does not exist. 
  \item Almost surely limit is $0$;
  \item $1/2$;
  \item $\P(Y_n \text{ converges }) = 0$;
  \item Yes, as every $Y_n$ has the same distribution. 
\end{enumerate}
\end{sol}
\end{problem}

\begin{problem}
The random variables $X_i$ are independend and uniformly distributed on $[0;2]$.
Find the probability limit
\[
\plim_{n\to\infty}  \max \left\{ \frac{\sum_{i=1}^{10} X_i}{n}, \frac{\sum_{i=1}^n X^3_i}{n+1} \right\}.
\]
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
The random variables $X_i$ are independend and uniformly distributed on $[0;1]$.
Find the probability limit
\[
\plim_{n\to\infty}  \max \left\{ \frac{\sum_{i=1}^n X_i}{n}, \frac{2\sum_{i=1}^n X^2_i}{n} \right\}.
\]
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
The random variables $X_i$ are independend and uniformly distributed on $[0;2]$.
Find 
\[
  \plim_{n\to\infty}  \frac{(X_1 - \bar X)^3 + (X_2 - \bar X)^3 + \ldots + (X_n - \bar X)^3}{n + 2022}.
\]
\begin{sol}
  \begin{align*}
    \plim \frac{\sum (X_i - \bar X)^3}{n+2022} = \plim \frac{\sum X_i^3 - 3\bar X\sum X_i^2 +3\bar X^2 \sum X_i - \sum \bar X^3}{n+2022} = \\
    = \E(X_1^3) - 3\E(X_1^2) + 3\E(X_1) - 1 = 0;
\end{align*}
Note that $\E(X_1^2) = 4/3$, $\E(X_1^3) = 2$.
\end{sol}
\end{problem}
  
\begin{problem}
Consider the stochastic process $(X_n)$, where $X_0$ is uniform on $[0;2]$ and
$X_n = (1 + X_{n-1}) / 2$.

\begin{enumerate}
  \item Find $\E(X_n)$ and $\Var(X_n)$.
  \item Find the probability limit $\plim X_n$.
\end{enumerate}
\begin{sol}
  Start with $X_0$: $\E(X_0) = 1$, $\Var(X_0) = 4/12 = 1/3$.

    \begin{enumerate}
      \item Expected value is constant, $\E(X_n) = 0.5 + 0.5 \E(X_{n-1})$, hence $\E(X_n) = 1$. 
      Variance goes to zero, $\Var(X_n) = 0.25 \Var(X_{n-1})$.
      \item $\plim X_n = 1$
    \end{enumerate}
\end{sol}
\end{problem}
  
\begin{problem}
  The random variables $X_i$ are independent and exponentially distributed with rate $\lambda = 1$. 

  \begin{enumerate}
      \item Find the probability limit
      \[
      \plim \frac{X_1 + X_2 + X_3 + \dots + X_n}{2n + 7}.
      \]
      \item Find the probability limit
      \[
      \plim \frac{X_1^2 + X_2^2 + X_3^2 + \dots + X_n^2}{2n + 7}.
      \]
      \item Find the probability limit
      \[
          \plim \min\{X_1, X_2, X_3, \dots, X_n\}.
      \]
      \item Find the probability limit
      \[
      \plim \sqrt[n]{\exp(2X_1 + 2X_2 + \dots + 2X_n)}.
      \]
  \end{enumerate}
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
  Consider two Markov chains, $(X_t)$ and $(Y_t)$: 

  \begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=3cm]
      \node [circle, draw] (one) {$x = -1$};
      \node [circle, draw] (two) [right of=one] {$x = 0$};
      \node [circle,  draw] (three) [right of=two] {$x = 1$};
      % \path (one) edge [bend left] node [above] {$1$} (two);
      \path (two) edge node [below] {$0.3$} (three);
      \path (two) edge node [below]{$0.7$} (one);
      %\path (three) edge [loop right] node {$0.2$} (three);
      %\path (three) edge [bend right] node [above] {$0.8$} (two);
      \path (one) edge [loop left] node {$1$} (one);
      \path (three) edge [loop right] node {$1$} (three);
  \end{tikzpicture} with $X_0 = 0$;
  
  and
  
  \begin{tikzpicture}[->,>= stealth', shorten >=2pt , line width=0.5pt, node distance=3cm]
      \node [circle, draw] (one) {$y = -1$};
      \node [circle, draw] (two) [right of=one] {$y = 0$};
      \node [circle,  draw] (three) [right of=two] {$y = 1$};
      \node [circle,  draw] (four) [right of=three] {$y = 2$};
      % \path (one) edge [bend left] node [above] {$1$} (two);
      \path (two) edge node [below] {$0.3$} (three);
      \path (two) edge node [below]{$0.7$} (one);
      % \path (three) edge [loop right] node {$0.2$} (three);
      \path (three) edge [bend left] node [above] {$0.5$} (four);
      \path (four) edge [bend left] node [above] {$1$} (three);
      \path (one) edge [loop left] node {$1$} (one);
      \path (three) edge [loop below] node {$0.5$} (three);
  \end{tikzpicture} with $Y_0 = 0$.
  
  
  
  \begin{enumerate}
      \item Find $\P(\lim X_n \text{ exists})$ and $\P(\lim Y_n \text{ exists})$.
      \item Find the limiting distribution of $(X_n)$ and the limiting distribution of $(Y_n)$.
  
      Hint: here you need to calculate all limits $\lim \P(X_n = k)$, $\lim \P(Y_n = k)$.
      
      \item Does $(X_n)$ converges almost surely? In distribution? In probability?
      \item Does $(Y_n)$ converges almost surely? In distribution? In probability?
  \end{enumerate}  
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}




\section{Conditional expected value without sigma-algebras}

\begin{problem}
We randomly uniformly select a point inside triangle $A = (6, 0)$, $B = (0, 2)$ and $O = (0, 0)$.
Let $(X, Y)$ be coordinates of this random point.

\begin{enumerate}
  \item Find conditional expected values $\E(Y \mid X)$ and $\E(X \mid Y)$.
  \item Find conditional variances $\Var(Y \mid X)$ and $\Var(X \mid Y)$.
\end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item $\E(Y \mid X) = 1 - X/6$ and $\E(X \mid Y) = 3 - 1.5 X$.
      \item $\Var(Y \mid X) = $, $\Var(X \mid Y) = $.
    \end{enumerate}        
  \end{sol}
\end{problem}


\begin{problem}
The pair of random variables $X$ and $Y$ has joint probability density
\[
f(x, y) = \begin{cases}
  x + y, \text{ if } x\in [0;1], y\in [0;1]; \\
  0, \text{ otherwise.}
\end{cases}
\]  

\begin{enumerate}
  \item Find the marginal densities $f_X(x)$ and $f_Y(y)$.
  \item Find the conditional densities $f(x \mid y)$ and $f(y \mid x)$.
  \item Find the conditional expected values $\E(Y \mid X)$ and $\E(X \mid Y)$.
  \item Find the conditional variances $\Var(Y \mid X)$ and $\Var(X \mid Y)$.
\end{enumerate}

  \begin{sol}
\begin{enumerate}
  \item 
  \[
  f(x) = \begin{cases}
    x + 0.5, \text{ if } x\in [0;1]; \\
    0, \text{ otherwise.}
  \end{cases}
  \]
  \item 
\[
f(x, y) = \begin{cases}
  (x + y) / (x + 0.5), \text{ if } x\in [0;1], y\in [0;1]; \\
  0, \text{ otherwise.}
\end{cases}
\]  
\item 
\[
\E(Y \mid X) = \frac{0.5X + 1/3}{X + 0.5}.
\]  
\item 

\end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
The random variables $X$ and $Y$ are independend with Poisson distribution with rate $\lambda = 1$.
Let $S = X + Y$.

\begin{enumerate}
  \item Find conditional probabilities $\P(X = x \mid S = s)$ and $\P(Y = y \mid S = s)$.
  \item Find conditional expected values $\E(X \mid S)$ and $\E(Y \mid S)$.
  \item Find conditional variances $\Var(X \mid S)$ and $\Var(Y \mid S)$.
  \item How the answers will change if $X \sim \dPois(\lambda_x)$ and $Y \sim \dPois(\lambda_y)$?
\end{enumerate}

  \begin{sol}
    \begin{enumerate}
      \item 
      \item $\E(X \mid S) = \E(Y \mid S) = S/2$;
      \item 
      \item  
    \end{enumerate}
    
  \end{sol}
\end{problem}
  



\begin{problem}
Let $X$ and $Y$ be independent and exponentially distributed with rate $\lambda = 1$ and $S = X + Y$.

\begin{enumerate}
  \item Find conditional densities $f(x \mid s)$ and $f(y \mid s)$.
  \item Find conditional expected values $\E(X \mid S)$ and $\E(Y \mid S)$.
  \item Find conditional variances $\Var(X \mid S)$ and $\Var(Y \mid S)$.
  \item Find $\Cov(X, Y \mid S)$ and $\Corr(X, Y \mid S)$.
  \item How the answers will change if $X \sim \dExpo(\lambda_x)$ and $Y \sim \dExpo(\lambda_y)$?
\end{enumerate}

  \begin{sol}
    \begin{enumerate}
      \item $X \mid S \sim \dUnif[0; S]$, $Y \mid S \sim \dUnif[0; S]$.
      \item $\E(X \mid S) = \E(Y \mid S) = S/2$;
      \item $\Var(X \mid S) = \Var(Y \mid S) = S^2 / 12$;
      \item  
    \end{enumerate}
    
  \end{sol}
\end{problem}


\begin{problem}
The random variable $X$ has Poisson distribution with rate $\lambda = 1$.
The random variable $Y$ has uniform distribution on $[1; 2]$. 
Random variables $X$ and $Y$ are independent. 

Find $\E(XY \mid X)$, $\Var(XY + X^3 \mid X)$, $\Cov(X, Y \mid X)$, $\Cov(XY, X^2Y \mid X)$.

\begin{sol}
    $\E(XY \mid X) = X\E(Y) = X/2$, $\Var(XY  + X^3\mid X) = X^2 \Var(Y) = X^2 /12$, $\Cov(X, Y \mid X) = 0$, $\Cov(XY, X^2Y \mid X) = X^3 \Var(Y) = X^3 / 12$
\end{sol}
\end{problem}


\begin{problem}
The random variables $X_1$ and $X_2$ are independent and normally distributed, 
$X_1 \sim \cN(1;1)$, $X_2 \sim \cN(2;2)$. 
I choose $X_1$ with probability $0.3$ and $X_2$ with probability $0.7$ without knowing their values.

Casino pays me the value $Y$ that is equal to the chosen random variable. 

Let the indicator $I$ be equal to $1$ if I choose $X_1$ and $0$ otherwise. 

\begin{enumerate}
  \item Express $Y$ in terms of $X_1$, $X_2$ and $I$.
  \item Find $\E(Y \mid I)$, $\Var(Y \mid I)$.
  \item Find $\E(Y)$ and $\Var(Y)$. 
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $Y = IX_1 + (1-I)X_2$
    \item Consider $I$ as known or fixed variable, $\E(Y \mid I) = I\E(X_1) + (1 - I)\E(X_2)$.
    Note that $I^2 = I$ and $(1-I)^2 = 1-I$, hence $\Var(Y \mid I) = I\Var(X_1) + (1-I)\Var(X_2)$.
    \item $\E(Y) = p \mu_1 + (1-p) \mu_2$ and $\Var(Y) = p(1-p)(\mu_1 - \mu_2)^2 + p\sigma^2_1 + (1-p)\sigma^2_2$,
    where $p = 0.3$, $\mu_1 = \sigma_1^2= 1$, $\mu_2 = \sigma_2^2 =2$.
  \end{enumerate}
\end{sol}
\end{problem}

\begin{problem}
A Hedgehog in the fog starts in $(0, 0)$ at $t=0$ and moves randomly with equal probabilities in four directions (north, south, east, west) by one unit every minute. 

Let $X_t$ and $Y_t$ be his coordinates after $t$ minutes and $S_t = X_t + Y_t$.

\begin{enumerate}
    \item Find $\E(X_2 \mid S_2)$;
    \item Find $\Var(X_2 \mid S_2)$.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
  Bonnie and Clyde start at the points $(5, 0)$ and $(-5, 0)$ of the plane. 
  Each minute each of them simulteneously and independently makes one step in one of the four possible directions (south, north, east, west).

  Each of them does $n$ steps.
  Let $X$ be the number of times they will be at the same point.
  \begin{enumerate}
      \item Estimate the probability $\P(X \geq 1)$ for $n=50$ using $B=10000$ simulations. 
      \item Estimate $\E(X)$ and $\Var(X)$ for $n=50$ using $B=10000$ simulations. 
      \item Plot the estimated value of $\E(X)$ as a function of $n$ for $n$ from $1$ to $200$ using $B=10000$ simulations. 
  \end{enumerate}
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
  Albert Nikolayevich Shiryaev randomly selects a natural number $N$ from $1$ to $7$.
  Let $Y$ be the remainder after division of $N$ by $2$ and $X$ be the remainder after division of $N$ by $3$. 
  
  \begin{enumerate}
      \item Write the joint probability table for $(X, Y)$.
      \item Find $\E(Y \mid X)$. Is it linear in $X$?
      \item Find $\E(X \mid Y)$. Is it linear in $Y$?
      \item Find $\E(\E(Y \mid X))$ and $\Var(\E(Y \mid X))$.
      \item Find $\Var(Y \mid X)$.
      \item Find $\E(\Var(Y \mid X))$.
      \item Find $\E(\Var(Y \mid X)) + \Var(\E(Y \mid X))$.
  \end{enumerate}
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
  Albert Nikolayevich selects a random point uniformly inside a quadrilateral $ABCD$ where $A = (0, 0)$, $B = (0, 2)$,
  $C = (4, 4)$, $D = (4, 0)$.
  
  \begin{enumerate}
      \item Find $\E(Y \mid X)$ and $\E(X \mid Y)$.
      \item Find $\Var(Y \mid X)$ and $\Var(X \mid Y)$.
  \end{enumerate}
  
  Hint: you may use the formula for the variance of uniform distribution :)  
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
  Albert Nikolayevich selects a random point $(X, Y)$ with joint probability density
  \[
  f(x, y) = \begin{cases}
      (3x^2 + 4y^3) / 2, \text{ if } x \in [0;1], y \in [0;1]; \\
      0, \text{ otherwise}. 
  \end{cases}
  \]
  \begin{enumerate}
      \item For the random variable $x$ find the marginal probability density function $f(x)$.
      %\footnote{%
      %Hereinafter we abuse notation. Strictly speaking we shoud write $f_X(x)$ and $f_Y(y)$ because these are different functions.} 
      \item Find the conditional density $f(y \mid x)$.
      \item Find the conditional expected value $\E(Y \mid X)$. Is it linear in $X$?
      \item Find $\Var(Y \mid X)$. Is it constant?
      \item Find $\E(X)$, $\E(Y)$, $\Cov(X, Y)$ and $\Var(X)$.
      \item Find the best linear approximation $Y^* = \beta_0 + \beta_1 X$ of the random variable $Y$.
      
      Hint: Here you should minimize $\E((Y - Y^*)^2)$ with respect to the true constants $\beta_0$ and $\beta_1$.
  \end{enumerate}
  \begin{sol}
  \end{sol}
\end{problem}
      

\begin{problem}
  Consider a fair dice. 
  In the experiment we throw the dice until the first six appears.
  
  \begin{enumerate}
      \item Simulate $B = 100000$ experiments. 
      For every experiment number $i$ record the total number of throws, $y_i$, and the number of even faces appeared, $x_i$.
      \item For all values of $x$ where you have more than $100$ records estimate $\hat\mu(x) = \hat{\E}(y_i \mid x_i = x)$ and $\hat v(x) = \widehat{\Var}(y_i \mid x_i = x)$.
      \item Explain intuitively why $\hat\mu(0)$ is less than $3$. 
      \item Randomly select $100$ experiments out of all $B$ experiments.
      Draw the scatter plot $(x_i, y_i)$ for randomly selected experiments.
      Add the line $\hat\mu(x)$ with bands $\hat\mu(x) \pm 2\sqrt{\hat v(x)}$ to the scatter plot. 
      \item Is it reasonable to assume that $\hat \mu(x)$ is linear?
      \item Is it reasonable to assume that $\hat v(x)$ is constant?
  \end{enumerate}
  
  No formal tests are required for the last two questions, graphical analysis is sufficient.  
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
The joint distribution of vector $(X,Y)$ is given by $\P(X=i,Y=j)=0.1$  for $1\leq i\leq j\leq 4$. 

Find $\E(Y\mid X)$.
  \begin{sol}
Joint distribution table of $X$ and $Y$ is:

\begin{tabular}{lllll}
\toprule
$X/Y$ & $1$ & $2$ & $3$ & $4$\\
\midrule
1& 0.1 & 0.1 & 0.1 & 0.1 \\
2& 0 & 0.1 & 0.1 & 0.1 \\
3& 0 & 0 & 0.1 & 0.1 \\
4& 0 & 0 & 0 & 0.1 \\
\bottomrule
\end{tabular}

Fixing a certain value of $X$ we find the conditional expectation of $Y$:
\[
\E(Y \mid X) =
\begin{cases}
  \frac{1+2+3+4}{4} = 2.5, \text{ if }  x=1\\
  \frac{2+3+4}{3} = 3, \text{ if } x=2\\
  \frac{3+4}{2} = 3.5, \text{ if } x=3\\
  \frac{4}{1} = 4, \text{ if }  x=4\\
\end{cases}
\]

Or, simply, $\E(Y \mid X) = 2 + X/2$.
  \end{sol}
\end{problem}


\begin{problem}
The random variable $X$ is exponentially distributed with parameter $\lambda$. 
The random variable $Y$ is exponentially distributed with parameter $1/X$. 

Find $\E(Y \mid X)$, $\E(Y)$  and $\Var(Y)$.
  \begin{sol}
    Variable $X$ has exponential distribution, so
\[
f_X(x)=\lambda e^{-\lambda x}
\]
And we know conditional distribution of $Y$:
\[
f_{Y \mid X}(x,y)=\frac{1}{x} e^{-\frac{1}{x}y}
\]
So,
\[
\E(Y \mid X) = \int_{-\infty}^{+\infty}  y { f_{Y \mid X}(X,y)} dy=  \int _0^{+\infty} y  \frac{1}{X} e^{-\frac{1}{X}y} dy =  (1/X)^{-1} = X.
\]
Expected value:
\[
\E(Y) =\E(\E(Y \mid X)) =\E(X) =  \int _{-\infty}^{+\infty}x f_X(x) \, dx=  \int _0^{+\infty} x \lambda e^{-\lambda x} \, dx =\lambda^{-1}
\]
Variance:
\[
\Var(Y) =\Var(\E(Y \mid X))+ \E( \Var(Y \mid X)) = \Var(X)+ \E ((1/X)^{-2})= \lambda^{-2} + \E(X^2)
\]
We know that $\E(X^2) = \Var(X) + (\E(X))^2$, so:
\[
\Var(Y) = \lambda^{-2}+\lambda^{-2} + \lambda^{-2} = 3\lambda^{-2}
\]
  \end{sol}
\end{problem}



\begin{problem}
  In the first bag there balls numbered from $0$ to $9$, in the second bag there are balls numbered from $1$ to $10$. 
  One ball was selected from the first bag and one ball from the second one. 
  You will select at random one ball from these two and you will know only its number. 
  Let's denote its number by $X$ and the number of the other of the two balls by $Y$.
  
  Find $\E(Y\mid X)$.
  \begin{sol}
      If $X=0$ we know that that the chosen ball was in the first basket, consequently $Y$ was in the second one.
      \[
      \E(Y \mid X=0) = \E(Y \mid Y \text{ from second basket } )=\frac{1+2+\dots+10}{10}=5.5
      \]
      Similarly
      \[
      \E(Y \mid X=10) = \E(Y \mid Y \text{ from first basket } )=\frac{0+1+\dots+9}{10}=4.5
      \]
      Finaly,
      \[
      \E(Y \mid 0<X<10) = \frac{1}{2}\E(Y \mid Y \text{ from first basket }) +\frac{1}{2}\E(Y \mid Y \text{ from second basket }) = 0.5 (5.5 + 4.5) = 5
      \]
      Answer:
      \[
      \E(Y \mid X) =
       \begin{cases}
          5.5, \text{ if } X=0\\
          5, \text{ if } 0<X<10\\
          4.5, \text{ if }  X=10\\
        \end{cases}
      \]        
  \end{sol}
\end{problem}


\begin{problem}
  The random variable $X$ is uniformly distributed on $[0;a]$. 
  Conditionally of $X$ the random variable $Y$ is uniformly distributed on $[0;X]$.
  
  Find $\E(Y\mid X)$, $\E(Y)$  and $\Var(Y)$.
  \begin{sol}
$\E(Y \mid X)=X/2$, $\E(Y)=\E(X/2)=a/4$,
\[
\Var(Y)=\E(\Var(Y \mid X))+\Var(\E(Y \mid X))=\E(X^2/12)+\Var(X/2)=\frac{7a^2}{12^2}
\]
  \end{sol}
\end{problem}


  
\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}





\section{Sigma-algebras and measurability}

\begin{leftbar}
Sigma-algebra generated by discrete random variable $X$, $\sigma(X)$ — the list of all events that can be stated using $X$.

Sigma-algebra generated by arbitrary random variable $X$, $\sigma(X)$ — the smallest list of events that satisfies two properties:
\begin{itemize}
  \item The list contains all events of the form $\{X \leq t\}$, that means one can compare $X$ with any number;
  \item If one takes countably many events from this list and does logical operations (union, complement, intersection) then one will obtain an event from the list.
\end{itemize}
\end{leftbar}


\begin{problem}
The random variable $X$ takes values $1$, $2$ and $-2$ with equal probabilities.

\begin{enumerate}
  \item Find the sigma-algebra $\sigma(X)$.
  \item How the answer will change if one modifies probability distribution of $X$?
  \item Find the sigma-algebra $\sigma(\abs{X})$.
  \item Foma knows $\abs{X}$ and Yeryoma knows $X^2$.
  What can one say about sigma-algebras that model their knowledge?
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item
\item Sigma-algebras do not depend on probabilities.
\item $\sigma(\abs{X}) = \{\emptyset, \Omega, \{\abs{X} = 2\}, \{X= 1\}\}$.
\item $\sigma(\abs{X}) = \sigma(X^2)$;
\end{enumerate}

\end{sol}
\end{problem}


\begin{problem}
Experiment may end by one of the six outcomes:

\begin{tabular}{*{4}{c}}
\toprule
& $X=-1$ & $X=0$ & $X=1$ \\
\midrule
$Y=0$ & 0.1 & 0.2 & 0.3  \\
$Y=1$ & 0.2 & 0.1 & 0.1  \\
\bottomrule
\end{tabular}

\begin{enumerate}
  \item Find expicitely the sigma-algebras $\sigma(X)$, $\sigma(Y)$, $\sigma(X \cdot Y)$, $\sigma(X^2)$, $\sigma(2X+3)$.
  \item How many elements are there in $\sigma(X, Y)$, $\sigma(X + Y)$, $\sigma(X, Y, X+Y)$?
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item $\sigma(X^2) = \{\{X^2 = 1\}, \{X = 0\}, \emptyset, \Omega\}$, $\sigma(2X+3) = \sigma(X)$
  \item $\card \sigma(X, Y) = 2^6$, $\card \sigma(X + Y) = 2^4$, $\card \sigma(X, Y, X+Y) = 2^6$.
\end{enumerate}

\end{sol}
\end{problem}




\begin{problem}
Let's look at the number of possible elements in a sigma-algebra.
\begin{enumerate}
  \item The random variable $X$ has five possible values.
How many events are there in $\sigma(X)$?
\item Can a sigma-algebra contain exactly $1000$ events? Exactly $1024$ events?
\end{enumerate}
Maria throws a coin $100$ times and remembers very well all the tosses.
\begin{enumerate}[resume]
  \item How many elementary outcomes are there in the probability space $\Omega$?
 \item How many events are there in a sigma-algebra that models Maria's knowledge?
\end{enumerate}



\begin{sol}
\begin{enumerate}
  \item $2^5$;
  \item Only $2^k$ or infinity;
  \item $2^{100}$;
  \item $2^{2^{100}}$.
\end{enumerate}
\end{sol}
\end{problem}





\begin{problem}
How sigma-algebras $\sigma(X)$ and $\sigma(f(X))$ are related? When they are equal?
\begin{sol}
In general $\sigma(f(X)) \subseteq \sigma(X)$; If $f$ is a bijection then $\sigma(f(X)) = \sigma(X)$.
\end{sol}
\end{problem}





\begin{problem}
How many different $\sigma$-algebras can be created using the set of outcomes $\Omega$ has three elements? And if $\Omega$ has four elements?

\begin{sol}
In the finite case sigma-algebra corresponds to partitions.
We get five sigma-algebras on a set of three elements and $15$ sigma-algebras on a set of four elements. 
These numbers are known as Bell numbers.
\end{sol}
\end{problem}



\begin{problem}
Provide an example of algebra that is not a $\sigma$-algebra.

\begin{sol}
Let $\Omega = \NN$, $\cA$ contains all finite sets and sets with finite complement.
\end{sol}
\end{problem}

\begin{problem}
Prove a statement or provide a counter-example:
\begin{enumerate}
  \item The intersection of two sigma-algebras is a sigma-algebra.
  \item If the intersection of two sigma-algebras is a sigma-algebra then one of them  is contained in the other one.
  \item The union of two sigma-algebras is a sigma-algebra.
  \item If the union of two sigma-algebras is a sigma-algebra then one of them  is contained in the other one.
\end{enumerate}


\begin{sol}
\begin{enumerate}
  \item The intersection of two sigma-algebras is always a sigma-algebra.
  \item The intersection of two sigma-algebras is always a sigma-algebra.
  \item The union of two sigma-algebras is not always a sigma-algebra.
  \item 
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
Let $\cF$ be some $\sigma$-algebra of subsets of $\Omega$ and $B\subseteq\Omega$. 
Consider the collection of sets $\cH=\{A: A\subseteq B \text{ or } B^{c}\subseteq A\}$.

Is $\cH$ a $\sigma$-algebra?

\begin{sol}
Yes. This is convinient do draw $\Omega$ as a segment. With «пескари» $A\subseteq B$ and «sharks» $A \supseteq B^{c}$.
\end{sol}
\end{problem}


\begin{problem}
Будем обозначать количество элементов множества с помощью $\card A$.
Рассмотрим подмножества натуральных чисел, $A \subseteq \mathbb{N}$.
Определим для подмножества плотность Чезаро (Cesaro density),
\[
\gamma(A)=\lim_{n\to \infty}\frac{\card (A\cap \{1,2,3, \ldots,n\})}{n}
\]
в тех случаях, когда этот предел существует.

Плотность Чезаро показывает, какую «долю» от всех натуральных чисел составляет указанное подмножество.
Обозначим с помощью $\cH$ все подмножества, имеющие плотность Чезаро.

\begin{enumerate}
\item Чему равна плотность Чезаро у нечётных чисел?
\item Приведите пример множества, у которого не определена доля Чезаро.
\item Верно ли, что у натуральных чисел, в записи которых присутствует
хотя бы одна единица, есть доля? Если да, то чему она равна?
\item Верно ли, что у натуральных чисел, в записи которых присутствует
ровно одна единица, есть доля? Если да, то чему она равна?
\item Верно ли, что $\cH$ — алгебра? Сигма-алгебра?
\end{enumerate}


\begin{sol}
Разобьем натуральный ряд на пары соседних чисел. Можно
так подобрать множества $A$ и $B$, что в каждом из них из каждой
пары взято только одно число.
Поэтому $\gamma(A)=\gamma(B)=\frac{1}{2}$. Подобрав
совпадение-несовпадение в паре, можно сделать так, что
$\gamma(A\cap B)$ не существует.
\end{sol}
\end{problem}


\begin{problem}
We throw a fair dice.
Let $Y$ be the indicator of a even score and $Z$ be the indicator of score bigger than $2$.
\begin{enumerate}
\item Find the sigma-algebra $\sigma(Z)$.
\item Find the sigma algebra $\sigma(Y\cdot Z)$.
\item How many elements are there in $\sigma(Y,Z)$?
\item How are related the $\sigma$-algebras $\sigma(Y\cdot Z)$ and $\sigma(Y,Z)$?
\end{enumerate}


\begin{sol}
  \begin{enumerate}
    \item $\sigma(Z) = \{\{Z =1\}, \{Z = 0\}, \Omega, \emptyset \}$.
    \item $\sigma(YZ) = \{\{YZ =1\}, \{YZ = 0\}, \Omega, \emptyset \}$.    
    \item $2^4$;
    \item $\sigma(Y\cdot Z) \subseteq \sigma(Y,Z)$.
    \end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
We throw a coin infinitely many times. 
Let $X_{n}$ be the indicator that the coin landed on Head at toss number $n$.
Consider a pack of $ \sigma$-algebras: $\cF_{n}:=\sigma(X_1, X_2, \ldots, X_n)$, $\cH_{n}:=\sigma(X_{n}, X_{n+1}, X_{n+2}, \ldots)$.

\begin{enumerate}
\item For each case provide two examples of $\sigma$-algebras that contain the corresponding event
\begin{enumerate}
\item $\{X_{37}>0 \}$;
\item $\{X_{37}>X_{2024} \}$;
\item $\{ X_{37}>X_{2024}>X_{12} \}$;
\end{enumerate}
  
\item Simplify experessions: $\cF_{11}\cap \cF_{25}$, $\cF_{11}\cup \cF_{25}$, $\cH_{11}\cap \cH_{25} $, $\cH_{11}\cup \cH_{25}$.

\item For each case provide two non-trivial examples (different from $\Omega$ and $\emptyset$) of an event $A$ such that

\begin{enumerate}
\item $A\in \cF_{2024}$;
\item $A\notin \cF_{2025}$;
\item $A \in \cH_{n}$ for all possible $n$;
\end{enumerate}


\end{enumerate}


\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
Правда ли равносильны три набора требований к списку множеств $\cF$?

Тариф «Классический»:
\begin{enumerate}
  \item $\Omega \in \cF$;
  \item Если $A\in \cF$, то $A^c \in \cF$;
  \item Если $A_1, A_2, A_3, \ldots, \in \cF$, то $\cup A_i \in \cF$.
\end{enumerate}


Тариф «Перевёрнутый»:
\begin{enumerate}
  \item $\emptyset \in \cF$;
  \item Если $A\in \cF$, то $A^c \in \cF$;
  \item Если $A_1, A_2, A_3, \ldots, \in \cF$, то $\cap A_i \in \cF$.
\end{enumerate}


Тариф «Хочу всё»:
\begin{enumerate}
  \item $\Omega \in \cF$, $\emptyset \in \cF$;
  \item Если $A\in \cF$ и $B\in \cF$, то $A \backslash B \in \cF$;
  \item Если $A_1, A_2, A_3, \ldots, \in \cF$, то $\cup A_i \in \cF$ и $\cap A_i \in \cF$.
\end{enumerate}


\begin{sol}
Yes!
\end{sol}
\end{problem}



\begin{problem}
Рассмотрим $\Omega=[0;1]$ и набор множества $\cF$ таких,
что либо каждое множество не более, чем счётно, либо дополнение к нему не более, чем счётно.

\begin{enumerate}
\item Верно ли, что $\cF$ — алгебра? $\sigma$-алгебра?
\item Придумайте $B\subset\Omega$, такое что $B \notin\cF$.
\end{enumerate}

\begin{sol}
Например, $B$ — Канторово множество, или, гораздо проще,
$B=[0;0,5]$. Оно само более чем счетно и дополнение к нему более
чем счетно.

Набор $\cF$ действительно $\sigma$-алгебра.
$\emptyset$ лежит в $\cF$, так как имеет ноль элементов.

Если $A$ не более чем счетно, то $A^{c}$ лежит в $\cF$,
так как дополнение к $A^{c}$ содержит не более чем счетное
число элементов.

Если дополнение к $A$ не более чем счетно, то $A^{c}$ лежит
в $\cF$, так как содержит не более чем счетное число элементов.

Проверяем счетное объединение $\bigcup_{i} A_{i}$.
Если среди $A_{i}$ встречаются только не более чем счетные, то и
их объединение — не более чем счетно.
Если среди $A_{i}$ встретилось хотя бы одно множество с не более
чем счетным дополнением, то $\bigcup_{i} A_{i}$ тоже будет
обладать не более чем счетным дополнением, так как объединение не
может быть меньше ни одного из объединяемых множеств.
\end{sol}
\end{problem}



\begin{problem}
В лесу есть три вида грибов: рыжики, лисички и мухоморы. Попадаются они равновероятно и независимо друг от друга. Маша нашла 100 грибов. Пусть $R$ — количество рыжиков, $L$ — количество лисичек, а $M$ — количество мухоморов среди найденных грибов.
\begin{enumerate}
\item Сколько элементов $ \sigma(R)$?
\item Сколько элементов $ \sigma(R,M)$?
\item Измерима ли $L$ относительно $ \sigma(R)$?
\item Измерима ли $L$ относительно $ \sigma(R,M)$?
\item Измерима ли $L$ относительно $ \sigma(R+M)$?
\item Измерима ли $L$ относительно $ \sigma(R-M)$?
\end{enumerate}

\begin{sol}
$ 2^{101} $, $2^{101\cdot 51}$,
\end{sol}
\end{problem}



\begin{problem}
Сейчас либо солнечно, либо дождь, либо пасмурно без дождя.
Соответственно, множество $\Omega$ состоит из трёх исходов, $\Omega=\{\text{солнечно},\text{дождь},\text{пасмурно}\}$.
Джеймс Бонд пойман и привязан к стулу с завязанными глазами, но он может на слух отличать, идёт ли дождь.
\begin{enumerate}
\item Как выглядит $\sigma$-алгебра событий, которые различает агент 007?
\item Как выглядит минимальная $\sigma$-алгебра, содержащая событие $A=\{\text{не видно солнце}\}$?
\item Сколько различных $\sigma$-алгебр можно придумать для данного $\Omega$?
\end{enumerate}
\begin{sol}
$\cF=\{\emptyset, \Omega, \{\text{дождь}\}, \{\text{солнечно}, \text{пасмурно}\}$. Всего есть $1+1+3=5$ $\sigma$-алгебр.
\end{sol}
\end{problem}

  

\begin{problem}
The random variables $X_i$ are independent and they take values $+1$ or $-1$ with equal probability. 
\begin{enumerate}
\item {[3]} Explicitely list all the events in sigma-algebra $\sigma(X_1 \cdot X_2)$.
\item {[3]} Pavel says that he knows only whether $X_1$ and $X_3$ are equal. 
How will you describe his knowledge with sigma-algebra?
\item {[4]} How many events are in the sigma-algebra $\sigma(X_1, X_1 + X_2, X_1 + X_2 + X_3)$?
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\sigma(X_1 \cdot X_2) = \{\emptyset, \Omega, \{X_1 X_2 = 1\}, \{X_1 X_2 = -1\}\}$;
    \item $\sigma(X_1 = X_3)$;
    \item Note that $\sigma(X_1, X_1 + X_2, X_1+ X_2+X_3) = \sigma(X_1, X_2, X_3)$, the number of events in sigma-algebra is
    $\card \sigma(X_1, X_1 + X_2, X_1+ X_2+X_3) = 2^8 = 256$.
\end{enumerate}
\end{sol}
\end{problem}
  

\begin{problem}
Vincenzo Peruggia makes attempts to steal the Mona Lisa painting until the first 
success. 
Each attempt is successful with probability $0.1$.

Let $X$ be the number of attempts and $Z = \min\{X, 5\}$.

\begin{enumerate}
  \item (5 points) How many events are in sigma-algebras $\sigma(Z)$ and $\sigma(X)$?
  \item (5 points) If possible provide an example of events $A$ and $B$ such that: $A\in \sigma(Z)$ but $A\not\in\sigma(X)$; $B\in \sigma(X)$ but $B\not\in\sigma(Z)$.
  \item (10 points) Find $\E(Z \mid X)$ and $\E(X \mid Z)$.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}
  

\begin{problem}
  Variables $X_1$, $X_2$, \ldots $X_{100}$ are independent and identically distributed
with mean $1$ and variance $2$. Each $X_i$ has only three possible values: 0, 1, and 2. 

\begin{enumerate}
  \item (5 points) How many events are in sigma-algebras $\sigma(X_1, X_2)$ and $\sigma(X_1 - X_2)$?
  \item (5 points) If possible provide an example of events $A$ and $B$ such that: $A\in \sigma(X_1, X_2)$ but $A\not\in\sigma(X_1 - X_2)$; $B\in \sigma(X_1 - X_2)$ but $B\not\in\sigma(X_1, X_2)$.
  \item (10 points) Find $\E(X_1 + \ldots + X_{100} \mid X_1 + \ldots + X_{50})$ and $\E(X_1 + \ldots + X_{50} \mid X_1 + \ldots + X_{100})$.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}
  
\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}
      



\section{Sigma-algebras and conditional expected value}


\begin{problem}
At time moment $t = 0$ in the casiono there are countably many players with perfect memory.
Let's number them as Miss First, Mister Second, etc. 


Time is discrete. 
Random variables $X_t$ are independent and take values $+1$ or $-1$ with equal probabilities.
At each moment of time $t > 0$ everybode gets $X_t$ roubles and than the player number $t$ leaves the casiono.

The cumulative sum $S_t = X_1 + \dots + X_t$ reaches its first local maximum at the random time $T$.
At time $T+1$ the dealer calls his friend Black Jack and says «It's time!»
They have agreed beforehand on the call time. 

Black Jack chases the player number $T$ and steals all his information before the police can intervent.  
Let's describe the information of Black Jack by sigma-algebra $\cF_{J}$ and the information of every player $t$ at the last moment in casino by $\cF_t$.

\begin{enumerate}
  \item Which sigma-algebras contain the event $\{T = 10\}$?
  \item Provide an example of two events from $\cF_J$ that do not enter in neither $\cF_t$.
  \item Find conditional expected values $\E(T \mid \cF_J)$, $\E(X_T \mid \cF_J)$, $\E(X_{T+1} \mid \cF_J)$.
  \item Find conditional expected values $\E(S_{T - 1} \mid \cF_J)$, $\E(S_T \mid \cF_J)$, $\E(S_{T+1} \mid \cF_J)$, $\E(S_{T+2} \mid \cF_J)$.
\end{enumerate}
Let's define $Y_{T - k}$ as 
\[
Y_{T - k} = \begin{cases}
  X_{T - k}, \text{ if } T - k > 0; \\
  0, \text{ otherwise.} 
\end{cases}
\]
\begin{enumerate}[resume]
  \item Find $\E(Y_{T - 10} \mid \cF_J)$.
  \item Find conditional expected values $\E(X_T \mid \cF_{10})$, $\E(X_{T+1} \mid \cF_{10})$.
  \item Find $\E(T \mid \cF_{10})$ and $\E(S_T \mid \cF_{10})$.
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item $\cF_J$, $\cF_{11}$, $\cF_{12}$, \dots
    \item $\{T \text{ is divisible by } 2\}$, $\{T \geq 3, X_{T-2} = 1\}$.
    \item $\E(T \mid \cF_J) = T$, $\E(X_{T} \mid \cF_J) = 1$, $\E(X_{T+1} \mid \cF_J) = -1$.
    \item $\E(S_T \mid \cF_J) = S_T$, $\E(S_{T+1} \mid \cF_J) = S_T - 1$, $\E(S_{T+2} \mid \cF_J) = S_T - 1$.
    \item $\E(Y_{T - k} \mid \cF_J) = Y_{T- k }$.
    \item $\E(X_T \mid \cF_{10}) = 1$, $\E(X_{T+1} \mid \cF_{10}) = -1$.
    \item $\E(T \mid \cF_{10}) = ... $, $\E(S_T \mid \cF_{10}) = ...$.  
\end{enumerate}
    
\end{sol}
\end{problem}


\begin{problem}
Bad police officers operate in groups of $1$, $2$ or $3$ people with probabilities $0.5$, $0.2$ and $0.3$. 
If you cross the road in the wrong place, they will catch you and demand a bribe $X$ of $1$, $5$ or 10 thousand rubles respectively. 

For each of the following cases write down the $\sigma$-algebra $\cF$ that models your information and calculate $\E(X \mid \cF)$.

  \begin{enumerate}
    \item you can see how many officers are going to stop you;
    \item they are sitting in the car and you don't know their number;
    \item it is dark and you can only say if it is one policeman or more than one. 
    \end{enumerate}
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
HSE student rolled the dice once. 
Find the $\sigma$-algebras that model the following situations:
\begin{enumerate}
    \item she only knows that the dice was rolled once;
    \item she knows the result of the roll;
    \item she observes the result of the roll but she is able to count only up to two.
\end{enumerate}
\begin{sol}
  Here $\Omega = \{1, 2, 3, 4, 5, 6\}$.
  \begin{enumerate}
    \item $\cF = \{\emptyset, \Omega\}$;
    \item $\cF = 2^{\Omega}$, this notation means «all subsets of $\Omega$».
    \item $\cF = \sigma(\{1\}, \{2\})$, eight events in total;
  \end{enumerate}

\end{sol}
\end{problem}

\section{Martinales}

\begin{leftbar}
\begin{itemize}
  \item \emph{Natural filtration} of a process $(X_n)$ is given by $\cF_n = \sigma(X_1, X_2, \dots, X_n)$.
  \item A process $(X_n)$ is a \emph{martingale} if $\E(X_{n + k} \mid X_n, X_{n-1}, \dots, X_1) = X_{n}$ for all $k \geq 1$.
  \item A process $(X_n)$ is \emph{adapted} to filtration $(\cF_n)$ if every random variable $X_n$ is measurable wrt to sigma-algebra $\cF_n$.
  \item A process $(X_n)$ is a \emph{martingale wrt to filtration} $(\cF_n)$ if $\E(X_{n + k} \mid \cF_n) = X_{n}$.
\end{itemize}
\end{leftbar}


\begin{problem}
Consider the sequence $(X_t)$ of independent identically distributed random variables with mean $\E(X_t) = 2$ and variance $\Var(X_t) = 3$.
Let's work with natural filtration $\cF_t = \sigma(X_1, X_2, \dots, X_t)$.

On the base of $(X_t)$ let's create more sequences: $S_t = X_1 + X_2 + \dots + X_t$, $W_t = S_t - 2t$ and $Y_t = W_t^2 - 3t$.

\begin{enumerate}
  \item Is $(X_t)$ a martingale with respect to $(\cF_t)$?
  \item Is $(S_t)$ a martingale with respect to $(\cF_t)$?
  \item Is $(W_t)$ a martingale with respect to $(\cF_t)$?
  \item Is $(Y_t)$ a martingale with respect to $(\cF_t)$?
  \item Is $(W_t)$ a martingale with respect to $(\cF_{t-1})$?
  \item Is $(W_t)$ a martingale with respect to $(\cF_{t+1})$?
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $(X_t)$ is not a martingale with respect to $(\cF_t)$;
    \item $(S_t)$ is not a martingale with respect to $(\cF_t)$;
    \item $(W_t)$ is a martingale with respect to $(\cF_t)$;
    \item $(Y_t)$ is a martingale with respect to $(\cF_t)$;
    \item $(W_t)$ is not a martingale with respect to $(\cF_{t-1})$;
    \item $(W_t)$ is not a martingale with respect to $(\cF_{t+1})$;
  \end{enumerate}  
\end{sol}
\end{problem}

\begin{problem}
Vasiliy has found three non-random infinite sequences in his garage: $a_n = n$, $b_n = -n$ and $c_n = 0$.
He randomly selects one of these sequences with equal probabilities and hence obtain a sequence of random variables $(X_n)$.
\begin{enumerate}
  \item What is the distribution of $X_7$?
  \item Find $\E(X_n)$ and $\Var(X_n)$.
  \item Is $(X_n)$ a Markov chain?
  \item Is $(X_n)$ a martingale?
  \item Explicitely find the $\sigma$-algebra $\sigma(X_1, X_2, X_3, \dots, X_{1000})$.
  \item Find the probability that limit of $(X_n)$ exists.
  \item Does $\plim X_n$ exist?
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item What is the distribution of $X_7$?
    \item $\E(X_n) = 0$ and $\Var(X_n) = $.
    \item The process $(X_n)$ is a Markov chain!
    \item The process $(X_n)$ is not a martingale.
    \item $\sigma(X_1, X_2, X_3, \dots, X_{1000}) = \sigma(X_1)$
    \item $\P(\lim X_n \text{ exists}) = 1/3$.
    \item $\plim X_n$ does not exist.
  \end{enumerate}  
\end{sol}
\end{problem}

\begin{problem}
Consider the sequence $(X_t)$ of independent identically distributed random variables
that take values $0$ or $1$ with equal probabilities.
Let's work with natural filtration $\cF_t = \sigma(X_1, X_2, \dots, X_t)$.

On the base of $(X_t)$ let's create more sequences: $S_t = X_1 + X_2 + \dots + X_t$, $W_t = S_t - at$, $M_t = \exp(bS_t)$.

\begin{enumerate}
  \item Is $(X_t)$ a martingale?
  \item Is $(S_t)$ a martingale?
  \item For which values of $a$ the process $(W_t)$ is a martingale?
  \item For which values of $b$ the process $(M_t)$ is a martingale?
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $(X_t)$ is not a martingale;
    \item $(S_t)$ is not a martingale;
    \item $a = 0.5$;
    \item $b = 0$ and $b = ...$
  \end{enumerate}  
\end{sol}
\end{problem}
  


\begin{problem}
Consider a well-mixed standard deck of $52$ cards. 
James Bond in an elegant outfit\footnote{Sponsors are wellcome to contact us for product placement!} 
opens cards one by one. 
Let the sigma-algebra $(\cF_n)$ model his information 
and $(X_n)$ be the proportion of Queens in the closed part of the deck after opening $n$ cards.

\begin{enumerate}
  \item Find the marginal distribution of $X_0$, $X_1$ and $X_{51}$.
  \item Find the joint distribution of $X_{50}$ and $X_{51}$.
  \item Is $(X_n)$ a martingale with respect to $(\cF_n)$?
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item $X_0 = 4/52$; $X_1$ is $4/51$ with probability $48/52$ or $3/51$ with probability $4/52$;
     $X_{51}$ is $1$ with probability $4/52$ and $0$ with probability $48/52$.
    \item 
    \item 
    With probability $X_n$ James Bond will pick up a Queen and the current number of closed Queens $X_n(52 - n)$ will decrease by $1$.
    With probability $(1 - X_n)$ James Bond will pick up a card different from Queen and the number of closed Queens $X_n(52 - n)$ will stay the same. 
    \[
    \E(X_{n+1} \mid \cF_n) = X_n \left( \frac{X_n(52 - n) - 1}{52 - n  - 1} \right) + (1 - X_n) \left( \frac{X_n(52 - n)}{52 - n - 1} \right) = \dots = X_n.
    \]
    Hence the process $(X_n)$ is a martingale with respect to $(\cF_n)$.
  \end{enumerate}
  
\end{sol}
\end{problem}

\begin{problem}
If possible create a martingale $(X_n)$ such that simulteneously $\P(X_n = 0 \text{ infinitely often}) = 1$ and
$\P(X_n = 1 \text{ infinitely often}) = 1$.
\begin{sol}
It is possible. 
\end{sol}
\end{problem}

\begin{problem}
At time $t=0$ there is one black and one white ball in the vase. 
At each moment of time we take out randomly one ball from the vase and put back two balls of the same color. 
Let $(W_t)$ be the proportion of white balls in the vase after $t$ extractions and $(Q_t)$ be the number of times when white ball was extracted.
\begin{enumerate}
  \item What is the distribution of $W_1$? Of $W_2$?
  \item Is $(W_t)$ a martingale?
  \item Consider a fixed parameter $p \in (0;1)$ and the process $M_t = (t + 1) C_t^{Q_t} p^{Q_t}(1-p)^{t - Q_t}$.
  Is $(M_t)$ a martingale?
  \item What is the limiting distribution of $(W_t)$?
  % \item What is the limiting distribution of $(Q_t / t)$? (need to think)
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item $W_1$ is equal to $1/3$ or $2/3$ with equal probabilities; $W_2$ is equal to $1/4$, $2/4$, $3/4$
    \item $(W_t)$ is a martingale;
    \item $(M_t)$ is a martingale;
    \item The limiting distribution of $(W_t)$ is uniform on $[0;1]$;
    % \item What is the limiting distribution of $(Q_t / t)$? (need to think)
  \end{enumerate}
  
\end{sol}
\end{problem}
  



\begin{problem}
Consider non-random sequence of numbers $(a_n)$. 
How can this sequence be a martingale?
\begin{sol}
Only constant non-random sequences are martingales.
\end{sol}
\end{problem}



\begin{problem}
Let $(M_n)$ be a martingale and $a<b<c<d$. 
\begin{enumerate}
  \item Find covariance $\Cov(M_d - M_c, M_b - M_a)$.
  \item Are $(M_d - M_c)$ and $(M_b - M_a)$ independent?
\end{enumerate}
\begin{sol}
$\Cov(M_d - M_c, M_b - M_a) = 0$;
\end{sol}
\end{problem}

\begin{problem}
Let $(M_t)$ be a process adapted to filtration $(\cF_t)$.

Is it true that in discrete time conditions 
\[
  \E(M_{t+1} \mid \cF_t) = M_t
\]
and
\[
\E(M_{t+k} \mid \cF_t) = M_t \text{ for all } k\geq 1
\]
are equivalent?
\begin{sol}
Yes. 
\end{sol}
\end{problem}

\begin{problem}
Initial wealth of a player is equal to $W_0 = 1$.
At each moment of time she can bet any proportion of her wellfare on the toss of a coin. 
If she guesses wrong she loses her bet. 
If she guesses right she gets profit equal to hear bet. 
The coin is not fair lands on head with probability $0.8$. 

\begin{enumerate}
  \item Find the bet that maximises one period log interest rate $\E(\ln (W_{t+1} / W_t) \mid \cF_t)$.
  \item Assume that the player maximises one period log interest rate every time. 
  Find a constant $a$ such that $\ln W_n - an$ is a martingale. 
\end{enumerate}


\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
For each case provide an example of a process.
\begin{enumerate}
  \item $(X_n)$ is a Markov chain and a martingale.
  \item $(X_n)$ is a Markov chain but not a martingale.
  \item $(X_n)$ is a martingale but not a Markov chain.
  \item $(X_n)$ is neither a Markov chain nor a martingale.
\end{enumerate}

\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Let $(X_n)$ be a simple symmetric random walk and $(\cF_n)$ its natural filtration. 

Find a deterministic (non-random) sequence $a_n$ such that $M_n = X_n^3 + a_n X_n$ is a martingale with respect to $(\cF_n)$.
\begin{sol}
For example, $a_n = -3n$, but one may add any constant. 
\end{sol}
\end{problem}

\begin{problem}
The random variables $X_i$ are independent and they take values $+1$ or $-1$ with equal probability. 

\begin{enumerate}
\item {[3]} Find $\E(X_3 \mid X_1, X_2)$, $\E(X_3 \mid X_1 + X_3)$.
\item {[3]} Find $\Var(X_3 \mid X_1, X_2, X_3)$, $\Var(X_3 \mid X_1 + X_3)$.
\item {[4]} Let $Y_n$ be equal to $\E(X_1 + \ldots +  X_{2022} \mid X_1, X_2, \ldots, X_n)$. 

Is the process $Y_1$, $Y_2$, \ldots, $Y_{2022}$ a martingale?
\end{enumerate}
\begin{sol}
$\E(X_3 \mid X_1, X_2) = \E(X_3) = 0$, $\E(X_3 \mid X_1 + X_3) = (X_1 + X_3) / 2$, $\Var(X_3 \mid X_1, X_3) = 0$,
    $\Var(X_3 \mid X_1 + X_3) = 1 - (X_1 + X_3)^2/4$.

Посчитаем ожидание и получим $Y_n = X_1 + X_2 + \ldots + X_n$, the process $(Y_n)$ is a martingale.
\end{sol}
\end{problem}
  

\begin{problem}
  Let $S_0 = 0$, $S_t = X_1 + X_2 + \ldots + X_t$. The increments $X_t$ are independent and identically distributed: 

\begin{tabular}{cccc}
\toprule
$x$ & $-1$ & $0$ & $1$ \\
$\P(X_t = x)$ & $0.2$ & $0.2$ & $0.6$ \\
\bottomrule
\end{tabular}

\begin{enumerate}
    \item If possible find all constants $a$ such that $M_t = S_t + at$ is a martingale.
  \item If possible find all constants $b$ such that $R_t = b^{S_t}$ is a martingale.
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
  Let $X_i$ be independent identically distributed with $\P(X_i = 1) = 0.9$, $\P(X_i = -1 ) = 0.1$. 

  Find all constants $a$ and $b$ such that $Y_t = a \exp\left(b\sum_{i=1}^t X_i\right)$ is a martingale. 
\begin{sol}
\[
 \E(Y_{t+1} \mid Y_t) = Y_t \E(e^{b X_{t+1}})
 \]    
\[
\E(e^{b X_{t+1}}) = 1 \rightarrow b = 0 or b = \ln(1/9)
\]
Trivial solution: $a = 0$ and any $b$.
\end{sol}
\end{problem}

\begin{problem}
The random variables $(Z_t)$ are independent identically distributed 
with moment generating function given by $M_{Z}(u) = 1/(1 - 5u)^3$. 

We define $X_t$ as $X_t = \exp(Z_1 + 2Z_2 + 3Z_3 + \ldots + tZ_t)$ with $X_0 = 0$. 

If possible find a martingale of the form $Y_t = h(t) X_t$ where $h()$ is a non-random function.
\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
The process $(Z_t)$ in discrete time is called \textit{stationary} if it has constant expected value 
and constant covariances $\gamma_k$ that do not depend on $t$. 
\[
\begin{cases}
\E(Z_t) = \mu; \\
\Cov(Z_t, Z_t) = \gamma_0; \\
\Cov(Z_t, Z_{t+1}) = \gamma_1; \\
\Cov(Z_t, Z_{t+2}) = \gamma_2; \\
\dots \\
\end{cases}
\]

\begin{enumerate}
  \item If possible provide an example of a martingale that is not stationary.
  \item If possible provide an example of a stationary process that is not a martingale.
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
  The population starts with one microbe Eve.
  So the size of the initial generations is $G_0 = 1$. 
  After one minute every microbe either dies with probability $0.2$, remains alive with probability $0.5$ or splits in two copies with probability $0.3$.
  Let $G_n$ be the size of microbe population after $n$ minutes.
  
  \begin{enumerate}
      \item Draw a pretty picture of Eve :)
      \item Find the distribution of $G_2$. 
      \item Find a constant $a$ such that $M_n = G_n /a^n$ is a martingale. 
      \item Let $D$ be the event of eventual death of the microbe civilization. 
      Check whether the process $K_n = \E(I_D \mid G_n, G_{n-1}, \dots, G_0)$ is martingale. 
      Here $I_D$ is the indicator of the event $D$.
      \item Using first step analysis find $\P(D)$.
      
      Hint: you may obtain a quadratic equation for $\P(D)$, the smallest root is your friend :)
  \end{enumerate}
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
  The random variables $X_n$ are independent and take values $+1$ with probability $0.7$ or $2$ with probability $0.3$.
Let $S_n = X_1 + X_2 + \dots + X_n$ be the cumulative sum. 

\begin{enumerate}
    \item Find the constant $a$ such that $M_n = S_n - a n$ is a martingale.
    \item Find all constants $b$ such that $K_n = \exp(b S_n)$ is a martingale.
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
\begin{sol}

\end{sol}
\end{problem}
  

  


\subsection{Stopping time}

\begin{leftbar}
Doob's optional stopping time theorem. 
If $(M_t)$ is a martingale and $\tau$ is a stopping time then $\E(M_\tau) = \E(M_0)$ provided at least one of the following conditions hold:
\begin{itemize}
  \item $\P(\tau < \infty) = 1$, the stopped process $X_t = M_{t\wedge \tau}$ is bounded by some constant.
  \item $\E(\tau) < \infty$, the process $D_t = \E(M_{(t+1) \wedge \tau} - M_{t\wedge\tau} \mid \cF_t)$ is bounded by some constant.
  \item $\P(\tau < \infty) = 1$, the process $M_t$ is uniformly integrable, ie 
  \[
  \lim_{a \to \infty }\sup_t \E(M_t \cdot I(M_t > a)) = 0.
  \]
\end{itemize}
\end{leftbar}




\begin{problem}
A gambler wins or looses one rouble in each round in the casino 
with equal probabilities and independently.
Let's denote the result of the $n$-th round by $X_n$.

The gambler starts with initial fortune $S_0 = 0$. 
Let $S_n = X_1 + X_2 +\dots + X_n$ be the wealth at time $n$.
She can have negative balance up to $-a$ roubles. 

She quits the casino when she either reaches the target of $+b$ roubles or the credit limit of $-a$ roubles.

\begin{enumerate}
  \item Is $(S_n)$ a martingale?
  \item Use optional-stopping theorem to find probabilities of reaching $+b$ or $-a$.
  \item Is $M_n = S_n^2 - n$ a martingale?
  \item Find the expected number of rounds before she will stop gambling.
\end{enumerate}

\begin{sol}
\begin{enumerate}
  \item $(S_n)$ is a martingale;
  \item $\P(S_{\tau} = b) = a/(a + b)$;
  \item $M_n = S_n^2 - n$ is a martingale;
  \item $\E(\tau) = ab$;
\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
A gambler wins or looses one rouble in each round in the casino 
with unequal probabilities and independently.
Let's denote the result of the $n$-th round by $X_n$, $\P(X_n = 1) = p$, $\P(X_n = -1) = q = 1-p$.

The gambler starts with initial fortune $S_0 = 0$. 
Let $S_n = X_1 + X_2 +\dots + X_n$ be the wealth at time $n$.
She can have negative balance up to $-a$ roubles. 

She quits the casino when she either reaches the target of $+b$ roubles or the credit limit of $-a$ roubles.

\begin{enumerate}
  \item Is $(S_n)$ a martingale?
  \item Is $K_n = (q / p)^{S_t}$ a martingale?
  \item Use optional-stopping theorem to find probabilities of reaching $+b$ or $-a$.
  \item Is $M_n = S_n - (p-q)n$ a martingale?
  \item Find the expected number of rounds before she will stop gambling.
\end{enumerate}  
\begin{sol}
  \begin{enumerate}
    \item $(S_n)$ is not a martingale;
    \item $K_n = (q / p)^{S_t}$ is a martingale;
    \item $\P(S_{\tau} = b) = ...$
    \item $M_n = S_n - (p-q)n$ is a martingale;
    \item $\E(\tau) = ...$
  \end{enumerate}
\end{sol}
\end{problem}
    

\begin{problem}
Famous «ABRACADABRA» problem. 

A monkey types randomly letters on a typewriter choosing each time one of the $26$ letters with equal probabilities.  
Let $T$ be the number of keypresses required to write the word «ABRACADABRA» for the first time. 

\begin{enumerate}
  \item Organise a casino to calculate $\E(T)$.
  \item Organise a casino to calculate $\E(T^2)$ and hence $\Var(T)$.
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $\E(T) = 26^{11} + 26^4 + 26$.
    \item 
  \end{enumerate}  
\end{sol}
\end{problem}


\begin{problem}
  To survive vampire Boris needs to bite 70 talented students. 
    
These 70 talented students have formed a secret group. They have written their emails on small pieces of paper and have randomly distributed these pieces among them. Each student has exactly one piece of paper with an email\footnote{The group is so secret that it is possible that a student has his own email on his piece of paper}. 

Initially vampire Boris knows contacts of just two persons from the group. Today he will contact them, drink their blood and get the emails they have. Then vampire Boris will contact new victims and so on.

\begin{enumerate}
    \item For $t\geq 1$ consider the process $M_t$, the proportion of non bitten students after the day $t$. 
    
    Is this process a martingale?
    
    \item Using martingale stopping theorem or otherwise find the probability that vampire Boris will bite all 70 students. 
\end{enumerate}
  \begin{sol}
    
  \end{sol}
\end{problem}




\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
    
  \end{sol}
\end{problem}

  

\section{Poisson process}

\begin{leftbar}
  The process $(N_t)$ is called \emph{Poisson process} with intensity $\lambda$ if 
  \begin{itemize}
    \item $N_0 = 0$;
    \item Increments are independent:
    If $t_1 < t_2 < t_3 < \dots < t_k$ then random increments $N(t_2) - N(t_1)$, $N(t_3) - N(t-2)$, \dots{ } are independent.
    \item Increments have Poisson distribution:
    \[
    N_b - N_a \sim \dPois(\lambda (b-a));
    \]
  \end{itemize}
  Alternative definition. The process $(N_t)$ is called \emph{Poisson process} with intensity $\lambda$ if 
  \begin{itemize}
    \item $N_0 = 0$;
    \item Increments are independent;
    \item Increments are stationary:
    
    The distribution of $N_b - N_a$ depends only on $(b - a)$.
    \item Probability of observing two or more points in a small interval is negligible:
    \[
    \P(N_{t + \Delta} - N_t > 1) = o(\Delta).
    \]
    \item Probability of observing one point is approximately proportional to the length of time interval:
    \[
      \P(N_{t + \Delta} - N_t = 1) = \lambda + \Delta o(\Delta).
    \]  
  \end{itemize}
\end{leftbar}


\begin{problem}
Two cashiers Alice and Bob simulteneously started to service their clients.
The service times $X_a$ and $X_b$ are independent and exponentially distributed with rates $\lambda_a = 1$ and $\lambda_b = 2$.

\begin{enumerate}
  \item Find the probability $\P(X_a < X_b)$.
  \item Find the density of $S = X_a + X_b$.
  \item Find the density of $L = \min\{X_a, X_b\}$.
  \item Find the density of $R = \max\{X_a, X_b\}$.
  \item Solve all the previous points for general rates $\lambda_a$ and $\lambda_b$.
\end{enumerate}

\begin{sol}
  \begin{enumerate}
    \item $\P(X_a < X_b) = \lambda_a/(\lambda_a + \lambda_b)$.
    There are two possible solutions: double integral and first step analysis.
    \item 
    \item 
    \item 
  \end{enumerate}  
\end{sol}
\end{problem}


\begin{problem}
Let $X_t$ and $Y_t$ be two independent Poisson processes. 
Is it true that $S_t = X_t + Y_t$ is also a Poisson process?
\begin{sol}
Yes. 
\end{sol}
\end{problem}

\begin{problem}
Hedgehogs are scattered in a big forest according Poisson process with rate $\lambda = 1$ per $100$ squared meters. 

What should be the edge of a square such that the probability of finding a hedgehog there is $0.7$?
\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Let $N_t$ be a Poisson process with rate $\lambda$.
\begin{enumerate}
  \item Is the process $A_t = N_t - \lambda t$ a martingale?
  \item Is the process $B_t = A_t^2 - \lambda t$ a martingale?
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $A_t = N_t - \lambda t$ is a martingale;
    \item $B_t = A_t^2 - \lambda t$ is a martingale;
  \end{enumerate}  
\end{sol}
\end{problem}


\begin{problem}
  Students arrive in the Grusha café according to the Poisson arrival process $(X_t)$ with constant rate $\lambda$.
  The probability of no visitors during 5 minutes is 0.05.
  \begin{enumerate}
    \item   Find the value of $\lambda$.
    \item  Find the variance and expected number of arrivals between 5 pm and 8 pm.
    \item What is the probability of exactly 5 arrivals between 5 pm and 8 pm?
  \end{enumerate}
\begin{sol}
  Let's measure time in minutes. 
  \begin{enumerate}
    \item $\P(X_5 = 0) = \exp(-5 \lambda) = 0.05$, so $\lambda = \ln (0.05) / -5 = \ln(20) /5$.
    \item $\E(X_{180}) = 180 \lambda$, $\Var(X_{180}) = 180\lambda$
    \item $\P(X_{180} = 5) = \exp(-180 \lambda) (180\lambda)^5/5!$
  \end{enumerate}
\end{sol}
\end{problem}
  


\begin{problem}
Masha receives on average 10 sms per minute. 
Sms arrival is well described by the Poisson process.
\begin{enumerate}
  \item  What is the probability that Masha receives exactly 10 sms in the next 40 seconds?
  \item Masha just received an sms. What is the probability that she will wait more that 2.5 seconds before the
  next one?
  \item  Find the covariance between the number of sms in the first 3 minutes and the number of sms in the first
  10 minutes.
\end{enumerate}
\begin{sol}
$\Cov(N_3, N_{10}) = \Cov(N_3, N_3 + (N_{10} - N_3)) = \Var(N_3) = 3\lambda$.
\end{sol}
\end{problem}
  


\begin{problem}
Taxis arrive to the station according to the Poisson process with rate 1 per 5 minutes.
Let $Y_t$ be the number of taxis that will arrive between 0 and $t$ minutes.
\begin{enumerate}
  \item  Sketch the expected value of $Y_t$ as a function of $t$.
\item Sketch the probability $\P(Y_t = Y_{60})$ as a function of $t$.
\end{enumerate}
\begin{sol}

\end{sol}
\end{problem}



\begin{problem}
A company gets fines for non-removal of quadrobics video content.
What is the probability that the total amount will exceed two undecillion roubles in 1000 days for each case?

\begin{enumerate}
  \item Fines arrive according to Poisson process with rate $1$ fine per day and each fine has the size $10^{33}$ roubles. 
  Fines are summing up without additional penalties.

  \item Initial fine is $10^5$ roubles but it doubles according to Poisson process with rate $1$ doubling per $10$ days. 
\end{enumerate}

\begin{sol}
Here we may approximate Poisson distribution by normal distribution, $\cN(\lambda t, \lambda t)$.
\end{sol}
\end{problem}
  



\begin{problem}
Let $(N_t)$ be a Poisson process with intensity rate $2$. 
Consider the vector $Y = (N_1, N_2, N_{10})$.

Find $\E(Y)$ and covariance matrix $\Var(Y)$.
\begin{sol}
\[
\E(Y) = (2, 4, 20), \quad \Var(Y) = \begin{pmatrix}
  2 & ? & ? \\
  ? & 4 & ? \\
  ? & ? & 20 \\
\end{pmatrix}
\]
\end{sol}
\end{problem}
  
  


\begin{problem}
Customers order coffee according to Poisson process with rate $1$ cup per minute.
The owner will close the shop if no one orders a coffee in $7$ minutes.

Let $X$ be the closure time. 

Find $\E(X)$ and $\Var(X)$.
\begin{sol}

\end{sol}
\end{problem}

\begin{problem}
The arrival of buses at a given stop follows Poisson process with rate $3$. 
The arrival of taxis at same stop follows independent Poisson process with rate $5$. 

\begin{enumerate}
  \item What is the probability that two or more taxis will arrive before next bus?
  \item What is the probability that exactly two taxis will arrive before next bus?
\end{enumerate} 
\begin{sol}

\end{sol}
\end{problem}


\begin{problem}
Customers order coffee according to Poisson process with rate $1$ cup per minute.
Let $N_t$ be the number of orders up to time $t$.

Find the probability $\P(N_t \text{ is even})$.
\begin{sol}
Let's denote $a(t) = \P(N_t \text{ is even})$.
\[
a(t+\Delta)=a(t)(1-\Delta)+(1-a(t))\Delta + o(\Delta) 
\]
Hence we get a differential equation  $a'(t)=1-2a(t)$ with $a(0)=1$.
The solution is $a(t)=(1+\exp(-2t))/2$.
\end{sol}
\end{problem}
  

\begin{problem}
  Arrivals of buses at a stop follow Poisson point process with rate $\lambda = 2$ buses per hour.
 
  \begin{enumerate}
      \item Find the probability that you will wait for the bus more than $30$ minutes.
    \item Find the probability to wait for the bus more than $30$ minutes, 
    if you have already waited $10$ minutes.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}




\begin{problem}
  When light bulb burns out it's immediately replaced by a new similar one. 
  The number or burn-outs during any interval of $t$ hours has Poisson distribution with parameter $\lambda t$. 
  
  Find the distribution of the first burn-out time $Y_1$. Prove your result.
    \begin{sol}
      $Y_1 \sim \dExpo(\lambda)$
    \end{sol}
  \end{problem}


\begin{problem}
Prove that two definitions of Poisson process are equivalent.

Definition A. The process $(N_t)$ is called \emph{Poisson process} with intensity $\lambda$ if 
  \begin{itemize}
    \item $N_0 = 0$;
    \item Increments are independent:
    If $t_1 < t_2 < t_3 < \dots < t_k$ then random increments $N(t_2) - N(t_1)$, $N(t_3) - N(t-2)$, \dots{ } are independent.
    \item Increments have Poisson distribution:
    \[
    N_b - N_a \sim \dPois(\lambda (b-a));
    \]
  \end{itemize}

  Definition B. The process $(N_t)$ is called \emph{Poisson process} with intensity $\lambda$ if 
  \begin{itemize}
    \item $N_0 = 0$;
    \item Increments are independent;
    \item Increments are stationary:
    
    The distribution of $N_b - N_a$ depends only on $(b - a)$.
    \item Probability of observing two or more points in a small interval is negligible:
    \[
    \P(N_{t + \Delta} - N_t > 1) = o(\Delta).
    \]
    \item Probability of observing one point is approximately proportional to the length of time interval:
    \[
      \P(N_{t + \Delta} - N_t = 1) = \lambda \Delta  + o(\Delta).
    \]  
  \end{itemize}
\begin{sol}
\url{https://math.stackexchange.com/questions/3919517/inter-arrival-time-distribution-of-a-poisson-process}
\end{sol}
\end{problem}


\begin{problem}
Class teacher solves three exercises per class on average.
Exercise solution times are independent exponentially distributed with the same rate. 
Class duration is two academic hours. 
 
    \begin{enumerate}
      \item Find the probability that only one exercise will be solved during the first academic hour.
      \item Find the probability that one exercise will be completed during the first first academic hour and the second exercise will be completed during the second hour. 
    \end{enumerate}
  \begin{sol}
    The rate is $\lambda = 3/2$ exercises per academic hour. 
    \begin{enumerate}
      \item $\P(X_1 = 1) = \exp(-3/2) (3/2)^1 / 1!$;
      \item $\P(X_1 = 1, \, X_2 = 2) = \P(X_1 = 1) \cdot \P(X_2 - X_1 = 1 \mid X_1 = 1) = \exp(-3) (9/4)$;
    \end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}



\subsection{Poisson approximation}

\begin{leftbar}
If $X \sim \dBin(n, p)$, $Y \sim \dPois(\lambda)$ with $\lambda = np$ and $A \subset \RR$ then 
\[
\abs{\P(X \in A) - \P(Y \in A)} \leq \min\{p, np^2 \}
\]
\end{leftbar}

\begin{problem}
Random variable $X$ has binomial distribution $\dBin(n, p=\lambda /n)$.

\begin{enumerate}
  \item Find $\E(X)$.
  \item Find $\lim_{n\to\infty} \Var(X)$.
  \item Find $\lim_{n\to\infty} \P(X = 0)$, $\lim_{n\to\infty} \P(X = 1)$.
  \item Find $\lim_{n\to\infty} \P(X = k)$.
\end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item $\E(X) = \lambda$;
      \item $\lim_{n\to\infty} \Var(X) = \lambda$;
      \item $\lim_{n\to\infty} \P(X = 0) = \exp(-\lambda)$, $\lim_{n\to\infty} \P(X = 1) = \lambda \exp(-\lambda)$;
      \item $\lim_{n\to\infty} \P(X = k) = \exp(-\lambda) \lambda^k / k!$.
    \end{enumerate}
    
  \end{sol}
\end{problem}


\begin{problem}
A manufacturer produces light-bulbs that are packed into boxes of $100$. 
Quality control studies indicate that $0.5\%$ of the light-bulbs produced are defective.


  \begin{enumerate}
    \item Estimate the probability of no defective light-bulbs in a box using Poisson distribution.
    \item Estimate the probability of two or more defective light-bulbs in a box using Poisson distribution.
    \item Provide bounds for the estimation error in points (a) and (b). 
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
The home assignment has three exercises.  
Alice makes no errors in an exercise with probability $0.9$ and one arithmetic error with probability $0.1$.
The errors in different exercises are independent.

\begin{enumerate}
  \item What is the probability that Alice will make exactly two errors in the home assignment?
  \item What is the expected number of errors in Alice home assignment?
\end{enumerate}
The number of errors Bob will make in the home assignment follows Poisson distribution.
The expected number of errors per home assignment for Bob and Alice are equal. 
\begin{enumerate}[resume]
  \item What is the rate of Bob's Poisson distribution?
  \item What is the probability that Bob will make exactly two errors?
  \item Solve points (a)-(d) if Alice and Bob write an online test that consists of $40$ questions with Alice error probability $0.01$ for each question. 
\end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}





  
\section{Wiener Process}

\begin{problem}
Consider a Wiener process $(W_t)$.
\begin{enumerate}
    \item {[4]} Let $Y_t = t W_{2t}$. What is the distribution of $Y_t - Y_s$ for $t\geq s$? Is $Y_t$ a Wiener process?
    \item {[6]} Find a constant $\alpha$ such that $M_t = W_t^3 + \alpha t W_t$ is a martingale. 
\end{enumerate}
\begin{sol}
$\Var(Y_t - Y_s) = \Var(tW_{2t} - sW_{2s}) = 2t^3 + 2s^3 - 4ts^2$.
    We get $\E(M_{t+u} \mid \cF_t ) = W_t^3 + 3W_t u  + \alpha(t + u) W_t$.
    From $\E(M_{t+u} \mid \cF_t ) = W_t^3 + \alpha t W_t$ it follows that $\alpha = -3$.
\end{sol}
\end{problem}
  


\begin{problem}
Consider a Wiener process $(W_t)$. For $r<s<t<u$ find the following expected values 
    \begin{enumerate}
    \item $\E((W_u - W_t)^2(W_s - W_r)^2)$;
    \item $\E((W_u - W_s)(W_t - W_r))$;
    \item $\E((W_t - W_r)(W_s - W_r)^2)$;
    \item $\E(W_r W_s W_t)$;
    \item $\E(W_r W_s W_t \mid W_s)$;
    \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
Here $(W_t)$ is a Wiener process.
  \begin{enumerate}
      \item Find $\E(W_5 W_4 \mid W_4)$, $\Var(W_5 W_4 \mid W_4)$.
      \item Find covariance $\Cov(W_4 W_5, W_5 W_6)$.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item $\E(W_5 W_4 \mid W_4) = W_4^2$, $\Var(W_5 W_4 \mid W_4) = W_4^2$;
      \item $\Cov(W_5 W_4, W_5 W_6) = 40$;
  \end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
For Wiener process $(W_t)$ find $\E(W_1 W_2 W_3)$ and $\E(W_2 W_3 \mid W_1)$.
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\section{Ito's integral}


\begin{problem}
  Consider Ito process $X_t$

  \[
  dX_t = \exp(t) W_t\, dt + \exp(2W_t) \, dW_t, \quad X_0 = 1.
  \]
  
  Consider two processes, $A_t = 1 + t^2 + X_t^3$ and $B_t = 1 + t^2 + X_t^3 W_t^4$.
  
  \begin{enumerate}
      \item Find $dA_t$ and $dB_t$.
      \item Write the corresponding explicit expressions for $A_t$ and $B_t$:
      \[
      const + \int_0^t \ldots dW_u + \int_0^t \ldots du
      \]
      \item Check whether $X_t$ is a martingale.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
Consider the process $X_t$
\[
X_t= tW_t + \int_0^t uW_u^2\, dW_u.
\]
\begin{enumerate}
    \item Find $\E(X_t)$, $\Var(X_t)$.
    \item Find $dX_t$.
    \item Check whether $X_t$ is a martingale.
\end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
Consider $X_t = \int_0^t W_u^3 dW_u + \int_0^t (W_u^3 + 3W_u u ) du - W_t^3 \cdot t$.
  \begin{enumerate}
      \item Find $dX_t$ and the corresponding full form. 
      \item Is $X_t$ a martingale?
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item $dX_t = (W^3_t-3W^2_t\cdot t)dW_t$ (4 points), 
      \[ 
      X_t = X_0 + \int_{0}^{t} W^3_u-3W^2_u\cdot u \,dWu
      \]
      \item A process is a martingale as in short form $A_t dt =0$.
  \end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
Consider $X_t = \exp(-2W_t - 2t)$.
\begin{enumerate}
    \item Find $dX_t$. Is $X_t$ a martingale?
    \item Find $\E(X_t)$ and $\Var(X_t)$.
    \item Find $\int_0^t X_u dW_u$.
\end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item $dX_t=-2X_t dW_t$, this process is a martingale;
      \item $\E(X)=1$, $\Var(X)=\exp(4t)-1$.
      \item $\int_{0}^{t} X_u\,dWu = \frac{1-X_t}{2}$.
  \end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
  Consider an Ito's process $I_t = 2022 + W_t t^2 + \int_0^t W_u^3 dW_u + \int_0^t W_u^2 du$.
  \begin{enumerate}
    \item Find $dI_t$ and check whether $I_t$ is a martingale. 
    \item Check whether $J_t = I_t - \E(I_t)$ is a martingale.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Martingales are everywhere :)
  
  Consider the process $Y_t = \exp( - u W_t)$. 
  
  \begin{enumerate}
    \item Find a multiplier $h(u, t)$ such that $M_t = h(u, t) \cdot Y_t$ is a martingale. 
    \item Find $dY_t$, $\E(Y_t)$ and $\Var(Y_t)$.
    \item Consider $M_t$ that you have found as a function of $u$. 
    Find the Taylor approximation of the function $M_t(u)$ up to $u^4$. 
    \item Consider the coefficient before $u^4$ in the Taylor expansion of $M_t(u)$. 
    Is it a martingale?
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Consider the process $X_t = \int_0^t W_u^2 dW_u + \int_0^t (W_u^2 + 2W_u u ) du - W_t^2 \cdot t$.

  \begin{enumerate}
      \item Find $dX_t$ and the corresponding full form. 
      \item Is $X_t$ a martingale?
      \item Find $\E(X_t)$.
  \end{enumerate}  
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Consider the stochastic process $X_t = f(t) \cos (2021 W_t)$.
  \begin{enumerate}
      \item Find $dX_t$.
      \item Find any $f(t) \neq 0$ such that $X_t$ is a martingale.
      \item Using $f(t)$ from the previous point find $\E(\cos (2021 W_t))$.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item Let's use Ito's lemma
      \[
      dX_t = f'(t) \cos (2021 W_t) dt - 2021 f(t) \sin (2021 W_t) dW_t + \frac{1}{2}2021^2 f(t) \cos(2021 W_t) dt    
      \]
      \item To make $X_t$ a martingale we should kill $dt$ term. 
      \item As $X_t$ is martingale $\E(X_t) = \E(X_0) = f(0)$.
      So $\E(\cos (2021 W_t)) = f(0) / f(t)$.
  \end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
Let $Y_t=W_t^3-3tW_t$.
\begin{enumerate}
\item Using Ito's lemma find $dY_t$.
\item Using your previous result find $\E(Y_t)$ and $\Var(Y_t)$.
\end{enumerate}  
  \begin{sol}
Using Ito's lemma $dY_t = (3W_t^2 - 3t) \, dW_t + (-3W_t + \frac{1}{2}6W_t) \, dt= (3W_t^2 - 3t) \, dW_t$. 
Hence, $Y_t$ is a martingale. $Y_0 = W_0^3 - 3\cdot 0 W_0 = 0$.

Now $\E(Y_t) = \E(Y_0) = 0$ and using Ito's isometry:
\[
\Var(Y_t)=
\int_0^t \E((3W_s^2-3s)^2) \, ds = \int_0^t 27s^2 +9s^2-18s^2 \, ds = \int_0^t 18s^2 \, ds = 6t^3
\]
%
Here we have used the facts that $\E(W_s^2) = s$ and $\E(W_s^4) = 3s^2$.
  \end{sol}
\end{problem}
  
\begin{problem}
  Let $Y_t=\exp(-aW_t-{a^2 t}/{2}$.
  \begin{enumerate}
  \item Using Ito's lemma find $dY_t$.
  \item Using your previous result find $\E(Y_t)$ and $\Var(Y_t)$.
  \end{enumerate}
  \begin{sol}
      $dY_t=-a Y_t \, dW_t$, so $Y_t$ is a martingale, and $\E(Y_t)=Y_0=1$. To find variance one may use Ito's isometry.
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}



\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}



\section{Stochastic Differential Equations}

\begin{problem}
Let's consider the following system of stochastic differential equations
\[
\begin{cases}
    dX_t=aX_t dt-Y_t dW_t \\
    dY_t=aY_t dt+X_t dW_t
\end{cases}
\]
with initial conditions $X_0=x_0$ and $Y_0=0$.
\begin{enumerate}
\item Find the solution of the form $X_t=f(t)\cos W_t$ and $Y_t=g(t)\sin W_t$.
\item Prove that for any solution $D_t=X_t^2+Y_t^2$ is nonstochastic.
\end{enumerate}
  \begin{sol}
    Using Ito's lemma  we find $dX_t$ and $dY_t$
\label{spiral_wiener}
\begin{multline*}
dX_t=f_t'(t) \cos W_t dt -f(t)\sin W_t \, dW_t-0.5f(t)\cos W_t\, dt=\\
= (f_t'(t)-0.5f(t))\cos W_tdt -f(t)\sin W_t \,dWt
\end{multline*}
\begin{multline*}
dY_t=g_t'(t) \sin W_t \, dt +g(t)\cos W_t \, dW_t-0.5g(t)\sin W_t\,dt=\\
= (g_t'(t)-0.5g(t))\sin W_t\,dt +g(t)\cos W_t \,dWt
\end{multline*}
%
Comparing these expressions with the system we receive:
\[
\begin{cases}
f_t'(t)-0.5f(t) = af(t)\\
f(t) = g(t)\\
  g_t'(t)-0.5g(t) = ag(t)\\
g(t) = f(t)
\end{cases}
\]
%
The general solution has the form
\[
f(t)=g(t)=A e^{(a+0.5)t}, \quad A\in  \RR
\]
%
From initial condition we get $A=x_0$ and
\[
f(t)=g(t)=x_0 e^{(a+0.5)t}
\]
%
Answer:
\[
\begin{array}{l}
X_t=x_0+x_0\int_0^{t} a e^{(a+0.5)u} \cos W_u du - x_0\int_0^{t} e^{(a+0.5)t}\sin W_u \,dWu\\
Y_t=x_0\int_0^{t} a e^{(a+0.5)u} \sin W_u du +x_0\int_0^{t} e^{(a+0.5)t}\cos W_u \, dWu
\end{array}
\]
%
Now we use Ito's lemma once again:
\begin{multline*}
dD_t=2X_t \, dX_t+2Y_t \, dY_t=2X_t((f_t'(t)-0.5f(t))\cos W_t \, dt -f(t)\sin W_t \,dWt)+\\
+ 2Y_t((g_t'(t)-0.5g(t))\sin W_t ], dt +g(t)\cos W_t\,dWt)=\\
=2f(t)(f_t'(t)-0.5f(t))\cos^2 W_t \, dt-f^2(t)\cos W_t\sin W_t\,dWt+\\
+2f(t)(f_t'(t)-0.5f(t))\sin^2 W_t \, dt+f^2(t)\cos W_t\sin W_t\,dWt=\\
=2f(t)(f_t'(t)-0.5f(t) \, dt=2af^2(t)\,dt
\end{multline*}
%
\[
D_t=D_0+\int_0^{t} 2a x_0 e^{(2a+1)u }du= x_0^2+\frac{2ax_0}{2a+1}(e^{(2a+1)t}-1)
\]
  \end{sol}
\end{problem}
  

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\section{Binomial asset pricing model}

\begin{problem}
Consider two-period binomial model with initial share price $S_0 = 600$, 
Up and down multipliers are $u=1.2$, $d=0.9$, risk-free interest rate is $r = 0.05$ per period. 

Consider an option that pays you $X_2 = 100$ at $T=2$ if $S_2 > S_1$ and nothing otherwise. 
  
\begin{enumerate}
  \item Find the risk neutral probabilities. 
  \item Find the current price $X_0$ of the asset. 
  \item How much shares should I have at $t=1$ in the «up» state of the world to replicate the option?
\end{enumerate}
\begin{sol}
  \begin{enumerate}
    \item $p^*_u = p^*_d = 1/2$;
    \item $X_1^u = X_1^d = (0.5\cdot 100 + 0.5\cdot 0) / 1.05$, hence $X_0 = 50/1.05^2 \approx 45.35$;
    \item $\alpha = X_2^{uu} - X_2^{ud} / (S_2^{uu} - S_2^{ud}) = 100/216 \approx 0.46$;
\end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}




\section{Black and Scholes model}

\begin{problem}
Consider Black and Scholes model with riskless rate $r$, volatility $\sigma$ and initial share price $S_0$. 

Find the current price $X_0$ of an option that pays you $X_2 = S_1^3$ at time $T=2$. 
  \begin{sol}
\[
X_0 = \exp(-2r) \E_{*}(X_2)    
\]
\[
X_2 = S_1^3 = S_0^3 \exp(3r)\exp(3\sigma W_1^* - 9\sigma^2/2)    
\]
\[
X_0 = S_0^3 \exp(r)\exp(3\sigma^2)
\]
  \end{sol}
\end{problem}


\begin{problem}
Ded Moroz would like to receive $X_T = S^{-1}_T$ at time $T$ if $S_T < 1$ and nothing otherwise.
    
Assume the framework of Black and Scholes model, $S_t$ is the share price, $r$ is the risk free rate,
$\sigma$ is the volatility. 

How much Ded Moroz should pay now at $t=0$?
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
Consider the Black and Scholes model with riskless rate $r$, volatility $\sigma$ and initial share price $S_0$. 

Find the current price $X_0$ of an option that pays you one dollar at time $T=2$ only if $S_2 > \exp(3r) S_0$.
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  In the framework of the Black and Scholes model find the price at $t=0$  of an asset that pays $\min\{M,\ln S_t\}$  at time $T$.
  Here  $S_T$ denotes the price of one share at time $T$, $M$ — arbitrary constant, specified at the moment of the issue.
  \begin{sol}
\[
X_0 = \E_{\tilde{p}}( \exp( - rT )X_{T} \mid \cF_0)
\]
  \end{sol}
\end{problem}


\begin{problem}
  The price of a share in euros is driven by the equation $dS=\sigma S dW+\alpha S dt$, 
  the dollar/euro exchange rate is driven by the equation $dU=b U dW+c U dt$. 
  
  Find the current price in dollars of a European call option with maturity date $T$, strike price $K$.
  \begin{sol}
      We need to find a price of the classic European option in dollars, while
stock prices are in euros.
  \end{sol}
\end{problem}



\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\section{Stationarity}

\begin{problem}
  The variables $x_t$ take values $0$ or $1$ with equal probabilities.
  The variables $u_t$ are normal $\cN(0; 1)$. All variables are independent.
  
  Consider the process  $z_t = x_t (1-x_{t-2}) u_t$.

  \begin{enumerate}
      \item Find the covariance $\Cov(z_t, z_s)$. Is the process $z_t$ stationary?
      \item Given that $z_{100} = 2.3$ find shёrtest predictive intervals for $z_{101}$ and $z_{102}$ with probability of coverage at least 95\%.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}



\section{ARMA}


\begin{problem}
  Consider stationary $AR(2)$ model, $y_t = 2 + 0.3 y_{t-1} - 0.02 y_{t-2} + u_t$, where $(u_t)$ is a white noise
  with $\Var(u_t) = 4$.
  
  The last two observations are $y_{100} = 2$, $y_{99} = 1$.
  \begin{enumerate}
      \item Find 95\% predictive interval for $y_{102}$.
      \item Find the first two values of the autocorrelation function, $\rho_1$, $\rho_2$.
      \item Find the first two values of the partial autocorrelation function, $\phi_{11}$, $\phi_{22}$.
  \end{enumerate}  
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Snegurochka studies a stochastic analog of the Fibonacci sequence
  \[
      y_t = y_{t-1} + y_{t-2} + u_t,
  \]
  where $(u_t)$ is a white noise process. 
  \begin{enumerate}
      \item How many non-stationary solutions are there?
      \item What can you say about the number and the structure of the stationary solutions?
      \item Can Snёgurochka find two starting constants $y_0 = c_0$ and $y_1=c_1$ in such a way to make a solution stationary?
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Stochastic process $X_t$ is defined by $X_t = 7 + u_t + 0.3 u_{t-1}$, where $(u_t)$ is a white noise 
  with variance $\sigma^2$.
  \begin{enumerate}
      \item Is $(X_t)$ stationary? 
      \item Find the autocorrelation function of $(X_t)$.
      \item Find $\E(X_{t+2} \mid X_t, X_{t-1}, \ldots)$.
  \end{enumerate}
  
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  Young investor Winnie-the-Crypto compares two trading strategies: buying bitcoins from good bees and from bad bees. 
  Let $d_t$ be the price difference at day $t$ (bad minus good). 
  Winnie-the-Crypto would like to test $H_0$: $\E(d_t) = 0$ against $H_a$: $\E(d_t) \neq 0$ at $5\%$ significance level.
  
  Winnie assumed that $(d_t)$ can be approximated by a $MA(1)$ process and estimated the parameters using $T=400$ observations, $\hat d_t = 2 + u_t + 0.7 u_{t-1}$ 
  with $\hat\sigma^2_u = 4$.
  
  \begin{enumerate}
    \item Estimate $\E(d_t)$, $\Var(d_t)$ and $\Cov(d_t, d_{t-1})$.
    \item Estimate $\E(\bar d)$, $\Var(\bar d)$ and help Winnie by considering $Z = \frac{\bar d - 0}{se(\bar d)}$.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Consider the following stationary process
  \[
  y_t = 1 + 0.5 y_{t-2} + u_t + u_{t-1},    
  \]
  where random variables $u_t$ are independent $\cN(0; 4)$.
  
  \begin{enumerate}
      \item Find the 95\% predictive interval for $y_{101}$ given that $y_{100} = 2$, $y_{99} = 3$, $y_{98} = 1$, $u_{99} = -1$.
      \item Find the point forecast for $y_{101}$ given that $y_{100}=2$.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item Let's denote by $x$ all available information, 
      \[
      x = \begin{pmatrix}
          y_{100} \\
          y_{99} \\
          y_{98} \\
          u_{99}
      \end{pmatrix}    
      \]
      Let's use $t=100$:
      \[
      y_{100} = 1 + 0.5 y_{98} + u_{100} + u_{99}    
      \]
  
      Using all available information we obtain $u_{100}  = 1.5$ and hence
      \[
      y_{101} \mid x \sim  \cN(1 + 0.5 y_{99} + u_{100} ; 4)
      \]
  
      \item Here we work with true betas:
      \[
      \E(y_{101} \mid y_{100}) = \mu_y + \frac{\Cov(y_{100}, y_{101})}{\Var(y_{100})}(y_{100} - \mu_y)    
      \]
  
  \end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
  Consider $MA(2)$ process given by 
  \[
  y_t = 5 + u_t + 2u_{t-1} + 4 u_{t-2},
  \]
  where $(u_t)$ is a white noise with $\Var(u_t) = \sigma^2$.
  
  \begin{enumerate}
  \item {[1]} Find the expected value $\E(y_t)$.
  \item {[7]} Find the autocorrelation function $\rho_k = \Corr(y_t, y_{t-k})$.
  \item {[2]} Is the process $(y_t)$ stationary?
  \end{enumerate}  
  \begin{sol}
    \begin{enumerate}
      \item $\E(y_t) = 5$
      \item $\rho_3 = \rho_4 = \ldots = 0$
      \item The process is stationary.
  \end{enumerate}
  \end{sol}
\end{problem}

\begin{problem}
  Consider $MA(2)$ process given by 
\[
y_t = 5 + u_t + 2u_{t-1} + 4 u_{t-2},
\]
where $u_t$ are normal independent random variables with $\Var(u_t) = 4$.

You know that $u_{100} = 2$ and $u_{99} = -1$.

\begin{enumerate}
\item {[5]} Find the 95\% predictive interval for $y_{101}$.
\item {[5]} Find the 95\% predictive interval for $y_{1000001}$.
\end{enumerate}
  \begin{sol}
\begin{enumerate}
  \item 
  \item Here past information is not useful, $\E(y_t) = 5$, $\Var(y_t) = 4 \cdot (1 + 2^2 + 4^2) =84$.
  \[
  [5 - 1.96 \sqrt{84}; 5 + 1.96 \sqrt{84}]
  \]
\end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
  The stationary process $(y_t)$ has autocorrelation function $\rho_k = 0.2^k$ and expected value $100$.
\begin{enumerate}
\item {[7]} Find the first two values of the partial autocorrelation function, $\phi_{11}$ and $\phi_{22}$.
\item {[3]} Provide a possible linear recurrence equation for this process. 
Your equation may include $y_t$, its lags and a white noise process $(u_t)$.
\end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item $\phi_{11} = \rho_1 = 0.2$, $\phi_{22} = 0$
      \item Possible equation is $y_t = 0.2 y_{t-1} + u_t$. 
      Another possibility is $y_t = 5 y_{t-1} + u_t$. 
      In the second case the stationary solution will be forward-looking and not $MA(\infty)$ with respect to $(u_t)$.
  \end{enumerate}
  \end{sol}
\end{problem}

\begin{problem}
  Consider the equation $y_t = 5 + 2.5 y_{t-1} - y_{t-2} + u_t$, where $(u_t)$ is a white noise process. 
\begin{enumerate}
\item {[3]} Find the roots of the corresponding characteristic equation. 
\item {[4]} Rewrite the process as $A(L)(y_t - \mu) = u_t$. 
You should explicitely write the lag polynomial $A(L)$ and the value of $\mu$.
\item {[1]} How many non-stationary solutions does the equation have?
\item {[1]} How many stationary solutions does the equation have?
\item {[1]} How many stationary solutions of the $MA(\infty)$ form with respect to $(u_t)$ does the equation have?
\end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item $\lambda_1 = 2$, $\lambda_2 = 0.5$, here the roots of the lag polynomial are exactly the same. 
      \item $(1 - 2L)(1 - 0.5L) (y_t + 10) = u_t$
      \item The equation has infinitely many non-stationary solutions.
      \item The equation has unique stationary solution.
      \item The equation has no stationary solutions that are $MA(\infty)$ with respect to $(u_t)$.
  \end{enumerate}
  \end{sol}
\end{problem}



\begin{problem}
  Consider the difference equation:
  \[
  y_t = 0.7y_{t-1} - 0.12 y_{t-2} + u_t,    
  \]
  where $(u_t)$ is a white noise. 
  \begin{enumerate}
      \item How many stationary and non-stationary solutions does the difference equation have?
  \end{enumerate}
  
  Consider stationary $AR(2)$ process that satisfies the difference equation. 
  
  \begin{enumerate}[resume]
      \item Find first two values of autocorrelation function.
      % \item Find first two values of partial autocorrelation function.
      \item Find $\alpha_1$ and $\alpha_2$ in $MA(\infty)$ representation 
  \[
  y_t = u_t + \alpha_1 u_{t-1} + \alpha_2 u_{t-2} + \alpha_3 u_{t-3} + \ldots
  \]
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item {[2 points]} $\lambda_1 = 0.3$, $\lambda_2 = 0.4$, one stationary solution, infinitely many non-stationary solutions. 
      \item {[6 points]}: {[2 points] for the system} + {[2 points] for $\rho_1$} + {[2 points] for $\rho_2$}.
      \[
          \begin{cases}
              \gamma_1 = 0.7 \gamma_0  - 0.12 \gamma_1 \\
              \gamma_2 = 0.7 \gamma_1 - 0.12 \gamma_0. 
          \end{cases}
      \]
      \[
      \rho_1 = 70/112 = 0.625, \quad \rho_2 = 49/112 - 0.12 = 0.3175    
      \]
      \item {[2 points]}
      \[
      \alpha_1 = 0.7, \quad \alpha_2 = 0.37    
      \]
  \end{enumerate}  
  \end{sol}
\end{problem}

\begin{problem}
  Consider the process $y_t = 4 + u_t + u_{t-1} + 2 u_{t-2}$, where $(u_t)$ is a white noise with variance $16$.

  \begin{enumerate}
    \item Is this process stationary? Explain. 
    \item Find the autocorrelation function of this process. Explain the meaning of $\rho_2$.
    \item Consider the process $d_t = \Delta y_t$. Is it $ARIMA(p, d, q)$? If yes, then find $p$, $d$ and $q$.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item Yes, the process is stationary, that is $MA(2)$ process. 
      \item $\rho_3 = \rho_4 = \ldots = 0$
      \item $d_t = u_t + u_{t-1} + 2u_{t-2} - u_{t-1} - u_{t-2} - 2 u_{t-3}$, hence $d_t \sim ARIMA(0, 0, 3)$.
  \end{enumerate}
  
  \end{sol}
\end{problem}

\begin{problem}
  Consider the stationary $AR(2)$ process $y_t = 5 - 0.9y_{t-1} - 0.2y_{t-2} + u_t$, where $(u_t)$ is a white noise. 
  \begin{enumerate}
    \item Find the first value of autocorrelation function $\rho_1$.
    \item Find the partial autocorrelation function of this process. Explain the meaning of $\phi_{22}$.
    \item What is the relationship between values of autocorrelation function $\rho_{100}$, $\rho_{99}$ and $\rho_{98}$.
  \end{enumerate}
  
  Hint: values $\phi_{22}$, $\phi_{33}$ etc may be calculated almost effortlessly :)
  \begin{sol}
    \begin{enumerate}
      \item 
      \item $\phi_{11} = \rho_1$, $\phi_{22} = -0.2$, $\phi_{33} = \phi_{44} = \ldots = 0$. 
      The partial correlation $\phi_{22}$ measures how will $y_t$ on average react to the unit change of $y_{t-2}$ given fixed $y_{t-1}$.
      \item $\rho_{100} = -0.9 \rho_{99} - 0.2 \rho_{98}$
  \end{enumerate}  
  \end{sol}
\end{problem}

\begin{problem}
  Random variables $x_t$ are iid with $\P(x_t = 0) = \P(x_t = 1) = 0.5$. 
  Consider the process $r_t = x_t \cdot x_{t-1} - 0.25$.
  \begin{enumerate}
  	\item Is $(r_t)$ stationary?
  	\item Elon Musk states that this is $MA(1)$ that can be rewritten as $r_t = u_t + \alpha u_{t-1}$.
    
  	Is Elon Musk right? 
    If yes, then express $u_t$ using $x_t$ and its lagged values. 
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\section{ETS}

\begin{problem}
  The semi-annual $y_t$ is modelled by $ETS(AAA)$ process:
    
    \[
    \begin{cases}
        u_t \sim \cN(0; 4) \\
        s_t = s_{t-2} + 0.1 u_t \\
        b_t = b_{t-1} + 0.2 u_t \\
        \ell_t = \ell_{t-1} + b_{t-1} + 0.3 u_t \\
        y_t = \ell_{t-1} + b_{t-1} + s_{t-2} + u_t \\
    \end{cases}    
    \]

    \begin{enumerate}
        \item Given that $s_{100} = 2$, $s_{99} = -1.9$, $b_{100} = 0.5$, $\ell_{100} = 4$ find 95\% prёdictive interval for $y_{102}$. 
        \item In this problem particular values of parameters are specified. And how many parameters are estimated in semi-annual $ETS(AAA)$ model before real forecasting?
    \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  The $ETS(AAdN)$ model is given by the system
  \[
  \begin{cases}
  u_t  \sim \mathcal{N}(0;20) \\
  b_t = 0.9 b_{t-1} + 0.2 u_t \\
  \ell_t = \ell_{t-1} + 0.9 b_{t-1} + 0.3 u_t \\
  y_t = \ell_{t-1} + 0.9 b_{t-1} + u_t \\
  \end{cases}
  \]
  with $\ell_{100} = 20$ and $b_{100} = 2$.
\begin{enumerate}
  \item Find conditional probability $\P(y_{102} > 30 \mid \ell_{100}, b_{100})$.
  \item Approximately find the best point forecast for $y_{10000}$.
\end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  Consider $ETS(ANN)$ model,
	\[
	\begin{cases}
	y_t = \ell_{t-1} + u_t \\
	\ell_t = \ell_{t-1} + \alpha u_t \\
	u_t \sim \cN(0;\sigma^2). \\
	\end{cases}
	\]
Let $\ell_{99} = 50$, $\alpha = 1/2$, $\sigma^2 = 16$, $y_{98} = 48$, $y_{99} = 52$, $y_{100} = 55$. 

Calculate 95\% predictive interval for $y_{101}$.
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  The semi-annual $(y_t)$ is modelled by $ETS(ANA)$ process:

  \[
  \begin{cases}
      u_t \sim \cN(0; 4) \\
      s_t = s_{t-2} + 0.1 u_t \\
      \ell_t = \ell_{t-1} + 0.3 u_t \\
      y_t = \ell_{t-1} + s_{t-2} + u_t \\
  \end{cases}    
  \]
  
  Given that $s_{100} = 3$, $s_{99} = -2$, $\ell_{100} = 100$ find 95\% predictive interval for $y_{102}$.   
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  The semi-annual $(y_t)$ is modelled by $ETS(ANA)$ process:

  \[
  \begin{cases}
      u_t \sim \cN(0; 4) \\
      s_t = s_{t-2} + 0.1 u_t \\
      \ell_t = \ell_{t-1} + 0.3 u_t \\
      y_t = \ell_{t-1} + s_{t-2} + u_t \\
      \ell_0 = 100, s_0 = -3, s_{-1} = 3 \\
  \end{cases}    
  \]
  
Check whether the process $(y_t)$ is stationary.   
  \begin{sol}
    The process is not stationary as $\E(y_1) = 3$ and $\E(y_2) = -3$.
  \end{sol}
\end{problem}

\begin{problem}
  Consider $ETS(AAdN)$ model 
  \[
  \begin{cases}
  u_t  \sim \cN(0;20) \\
  b_t = 0.9 b_{t-1} + 0.2 u_t \\
  \ell_t = \ell_{t-1} + 0.9 b_{t-1} + 0.3 u_t \\
  y_t = \ell_{t-1} + 0.9 b_{t-1} + u_t \\
  \end{cases}
  \]
  with $\ell_{100} = 20$ and $b_{100} = 1$.
  \begin{enumerate}
      \item Find 95\% prediction interval for $y_{102}$.
      \item Approximately find the best point forecast for $y_{10000}$.
  \end{enumerate}
  
  \begin{sol}
    \begin{enumerate}
      \item
      \[
          y_{102}= \ell_{100} + (0.9 + 0.9^2) b_{100} + (0.3 + 0.18)u_{101} + u_{102}    
          \]
          \[
          (y_{102} \mid y_1, \ldots, y_{100}) \sim \cN(21.71, 24.608)    
          \]
          The interval
          \[
          [21.71 - 1.96 \cdot 4.96;21.71 + 1.96 \cdot 4.96]    
          \]
      \item 
      \[
      \lim_{h\to\infty} \E(y_{100+h} \mid y_1, \ldots, y_{100}) = \ell_{100} + (0.9 + 0.9^2 +\ldots) b_{100} = 20 + 9\cdot 1    
      \]    
  \end{enumerate}  
  \end{sol}
\end{problem}

\begin{problem}
  Consider $ETS(AAN)$ model,
	$
	\begin{cases}
	y_t = \ell_{t-1} + b_{t-1} + u_t \\
	\ell_t = \ell_{t-1} + b_{t-1} + \alpha u_t \\
	b_t = b_{t-1} + \beta u_t \\
	u_t \sim \cN(0;\sigma^2). \\
	% s_t = s_{t-12} + \gamma \varepsilon_t \\
	\end{cases}
	$
		
Let $\ell_{100} = 50$, $b_{100} = 2$, $\alpha=0.4$, $\beta=0.5$, $\sigma^2 = 16$.

Calculate one step and two steps ahead 95\% predictive intervals. 
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}





\section{GARCH}

\begin{problem}
  The process $y_t$ is described by a simple $GARCH(1, 1)$ model:
  \[ 
      \begin{cases}
          y_t = \sigma_t \nu_t \\
          \sigma_{t}^{2}= 1 + 0.2 y_{t-1}^{2}+ 0.3 \sigma_{t-1}^{2}    \\
          \nu_t \sim \cN(0;1)
      \end{cases}     
  \]

  The variables $\nu_t$ are independent of past variables $y_{t-k}$, $\nu_{t-k}$, $\sigma_{t-k}$ for all $k\geq 1$.
  The prёcesses $y_t$, $\sigma^2_t$ are stationary. 


  Given $\sigma_{100}=1$ and $\nu_{100} = 0.5$ find 95\% predictive interval for $y_{102}$. 
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  The strictly stationary white noise $(u_t)$ follows $ARCH(1)$ model $\sigma^2_t = 3 + 0.5 u_{t-1}^2$ where 
  $u_t = \sigma_t \nu_t$ and $\nu_t \sim \mathcal{N}(0;1)$.
  \begin{enumerate}
      \item Find 95\% prediction interval for $u_{101}$ given that $u_{100} = -1$.
      \item Find $\E(u_t)$, $\Var(u_t)$.
      \item Find $\Corr(u_t, u_{t-1})$, $\Corr(u_t^2, u_{t-1}^2)$.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item {[4 points]}
      \[
      \sigma^2_{101} = 3 + 0.5 (-1)^2= 3.5    
      \]
      \[
      (u_{101}\mid \sigma_{101}) \sim \cN(0; \sigma^2_{101})    
      \]
      \[
      [-1.96 \sqrt{3.5}; +1.96 \sqrt{3.5}]    
      \]
      \item {[3 points]} {[1 point]} for $\E(u_t)$ and {[2 points]} for $\Var(u_t)$ 
      The process $(u_t)$ is a white noise, hence
      \[
      \E(u_t) = 0.    
      \]
      \[
      \sigma^2_u = 3 + 0.5 \cdot \sigma^2_u    
      \]
      \item {[3 points]}: {[1 point]} for $\Corr(u_t, u_{t-1})$ and {[2 points]} for $\Corr(u_t^2, u_{t-1}^2)$ 
      The process $(u_t)$ is a white noise, hence
      \[
      \Corr(u_t, u_{t-1}) = 0.    
      \]
      \[
      u_t^2 = 3 + 0.5 u_{t-1}^2 + (u_t^2 - \sigma_t^2)    
      \]
      We notice that $r_t = u_t^2 - \sigma_t^2$ is a white noise, hence $u_t^2$ is an $AR(1)$ process.
      Hence, $\Corr(u_t^2, u_{t-1}^2) = 0.5$.
  \end{enumerate}  
  \end{sol}
\end{problem}

\begin{problem}
  Consider the $ARCH(1)$ model, $u_t = \sigma_t \nu_t$, where $\nu_t$ are iid $\cN(0;1)$ and 
  $\sigma^2_t = 1 + 0.3 u_{t-1}^2$. 
  \begin{enumerate}
    \item Find 95\% predictive interval for $u_{101}$ if $u_{100} = -2$.
    \item Find the autocorrelation function of $r_t = u_t^2$. 
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}
  \end{sol}
\end{problem}


\section{Method of Moments and maximum likelihood}

\begin{problem}
  The variables $X_1$, \ldots, $X_n$ are independent identically distributed with density 
  \[
  f(x) = \begin{cases}
    \lambda \exp(-\lambda (x - \theta)), \text{ if } x\geq \theta \\
    0, \text{ otherwise}.
  \end{cases}  
  \]
  \begin{enumerate}
    \item {[5]} Find the method of moments estimator of $\lambda$ for known value $\theta = 1$ using the first moment. 
    \item {[5]} Find the method of moments estimator of $\lambda$ for unknown value $\theta$ using the first two moments. 
  \end{enumerate}  
  \begin{sol}
    Let's observe that we may decompose $X_i$ as a sum $X_i = Y_i + \theta$,
    where $Y_i \sim \dExpo(\lambda)$. 
    
    Hence, $\E(X_i) = 1/\lambda + \theta$, $\Var(X_i) = \Var(Y_i) = 1/\lambda^2$
    and $\E(X_i^2) = 1/\lambda^2 + (1/\lambda + \theta)^2$.
    
    There is an alternative solution with direct integration:
    \[
    \E(X_i) = \int_{\theta}^{+\infty} x f(x) \; dx, \quad \E(X_i^2) = \int_{\theta}^{+\infty} x^2 f(x) \; dx.
    \]
    
    \begin{enumerate}
        \item Solving $1/\hat\lambda + 1 = \bar X$ we obtain $\hat\lambda = 1/ (\bar X - 1)$.
        \item Solving for $\hat\lambda$ and $\hat\theta$ the system
    \[
    \begin{cases}
        1/\hat\lambda + \hat \theta = \bar X \\
        1/\hat\lambda^2 + (1/\hat\lambda + \hat\theta)^2 = M_2 \text{ with } M_2 = \sum X_i^2/n
    \end{cases}    
    \]
    we obtain 
    \[
    \hat \lambda = \frac{1}{\sqrt{M_2 - \bar X^2}} , \quad \hat \theta =  \bar X - \sqrt{M_2 - \bar X^2}   
    \]
    \end{enumerate}
  \end{sol}
\end{problem}

\begin{problem}
  The variables $X_1$, \ldots, $X_n$ are independent and normally distributed $\cN(a, 2a)$.
  
  Find the maximum likelihood estimator of $a$.

  Hint: $f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp(-(x-\mu)^2/2\sigma^2)$.  
  \begin{sol}
    The log-likelihood function is equal to
    \[
    \ell(a) = \sum_{i=1}^n \left( (-0.5)\ln(4\pi) - 0.5 \ln a - (x_i - a)^2 / 4a \right).
    \]
    The equation $\ell'(a) = 0$ may be simplified to
    \[
        n\hat a^2 + 2n\hat a - \sum X_i^2  = 0    
    \]
    Hence, 
    \[
    \hat a = \frac{-2n \pm \sqrt{4 n^2 + 4n \sum X_i^2}}{2n}
    \]
    We choose the root $\hat a > 0$ as $\Var(X_i) = 2a > 0$.
    \[
    \hat a = \sqrt{ 1  + \sum X_i^2/n} - 1
    \]
    
    Just for fun. 
    In the case $X_i \sim \cN(a, ka)$ the equation would be
    \[
        n\hat a^2 + k n\hat a - \sum X_i^2  = 0    
    \]
    And 
    \[
    \hat a = \frac{-nk + \sqrt{k^2 n^2  + 4n \sum X_i^2}}{2n}.    
    \]    
  \end{sol}
\end{problem}


\begin{problem}
  The weight of a fish $Y_i$ is a discrete random variables with 
  distribution and observed frequencies given in the table 
  
  \begin{tabular}{cccc}
      \toprule
      Weight [kg] & 1 & 2 & $a$ \\
      Probability & $0.2 + 0.1a$ & $0.3 - 0.1a$ & $0.5$ \\
      Observed frequency & $N_1$ & $N_2$ & $N_a$ \\
      \bottomrule
  \end{tabular}
  
  Fish weights $Y_i$ are independent, $a > 10$ is unknown. 
  \begin{enumerate}
      \item Find the method of moments estimator of the parameter $a$. 
      \item Find the maximum likelihood estimator of the parameter $a$. 
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  The weight of a fish $Y_i$ is a discrete random variables with 
  distribution and observed frequencies given in the table 
  
  \begin{tabular}{cccc}
      \toprule
      Weight [kg] & 1 & 2 & 4 \\
      Probability & $0.2 + a$ & $0.3 - a$ & $0.5$ \\
      Observed frequency & $N_1$ & $N_2$ & $N_4$ \\
      \bottomrule
  \end{tabular}
  
  Fish weights $Y_i$ are independent. 
  
  \begin{enumerate}
      \item {[5]} Find the maximum likelihood estimator of the parameter $a$. 
      \item {[5]} Find the method of moments estimator of the parameter $a$. 
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item 
      \[
      L = const (0.2 + a)^{N_1} (0.3 - a)^{N_2} 0.5^{N_3}    
      \]
      \[
      \ell = const + N_1 \ln (0.2 +a) + N_2\ln(0.3 - a) + N_3 \ln 0.5    
      \]
      \[
      \frac{\partial \ell}{\partial a} = \frac{N_1}{0.2 + a} - \frac{N_2}{0.3 - a} 
      \]
      \[
      \hat{a}_{ML} = \frac{0.3 N_1 - 0.2 N_2}{N_1 + N_2}    
      \]
      We see that $\partial \ell /\partial a$ decreases as $a$ increases, so 
      $\hat{a}_{ML}$ is indeed the point of maximum. 
      \item 
      \[
      \E(Y_i) = (0.2 + a) + 2(0.3 - a) + 4\cdot 0.5 = 2.8 -a    
      \]
      \[
      \bar Y = \frac{N_1 + 2N_2  + 4N_4}{N_1 + N_2  + N_4}    
      \]
      \[
      \hat a_{MM} = 2.8 - \frac{N_1 + 2N_2  + 4N_4}{N_1 + N_2  + N_4}
      \]
  \end{enumerate}
  
  \end{sol}
\end{problem}


\begin{problem}
To go to the mountain top I use a gondola lift in the morning. 
I go back from the top using the same gondola lift in the evening. 
Cabins are numbered from $1$ to $a$. 

I have noticed that the absolute difference of cabin numbers of my two trips was $10$. 

\begin{enumerate}
    \item Estimate $a$ using maximum likelihood. 
    \item Estimate $a$ using method of moments. 
\end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
Random variables $X_1$, $X_2$, \ldots,  $X_n$ are independent identically distributed with density 
\[
f(x_i \mid \lambda, a) = \frac{\lambda}{2} \exp(-\lambda \abs{x_i - a}).    
\]

Observed values for $n=3$ are $-3$, $1$, $11$.

\begin{enumerate}
    \item Estimate $\lambda$ using method of moments for fixed $a = 1$. 
    \item Estimate $\lambda$ and $a$ using maximum likelihood.
\end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}




\section{LR, LM and Wald tests}

\begin{problem}
  We have two independent random samples $X_1$, $X_2$, \ldots, $X_{n_x}$ and $Y_1$, $Y_2$, \ldots, $Y_{n_y}$.
  The random variables $X_i$ follow Poisson distribution with intensity rate $\lambda_x$, 
  random variables $Y_i$ follow Poisson distribution with intensity rate $\lambda_y$.

  We would like to test $H_0$: $\lambda_x = \lambda_y$ against $H_1$: $\lambda_x \neq \lambda_y$.

  \begin{enumerate}
    \item {[3]} Find the maximal value of log-likelihood under $H_0$.
    \item {[3]} Find the maximal value of log-likelihood under unrestricted model.
    \item {[2]} Construct the likelihood ratio test. 
    \item {[2]} Do you reject $H_0$ if $n_x = 100$, $n_y = 200$, $\sum x_i = 500$, $\sum y_i = 900$ at
    significance level $5\%$?
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item Under $H_0$ we have $X_i \sim \dPois(\lambda)$, $Y_i \sim \dPois(\lambda)$.
      \[
          \ell(\lambda) = \sum_{i=1}^{n_x} \left( -\lambda + X_i \ln \lambda - \ln (X_i!)\right) + 
          \sum_{i=1}^{n_y} \left( -\lambda + Y_i \ln \lambda - \ln (Y_i!)\right)       
      \]
      The score function is 
      \[
      \score(\lambda) = \ell'(\lambda) = \sum_{i=1}^{n_x} \left( -1 + X_i / \lambda\right) + \sum_{i=1}^{n_y} \left( -1 + Y_i / \lambda\right).
      \]
      The estimator is $\hat \lambda = (\sum X_i + \sum Y_i) / (n_x + n_y)$.
      \[
      \max \ell_R = - \hat \lambda (n_x + n_y) + \left(\sum X_i + \sum Y_i \right)\ln \hat\lambda - \sum \ln X_i! - \sum \ln Y_i!    
      \]
      
      \item In unrestricted model we have two independent estimators, 
      \[
      \hat \lambda_x = \bar X, \quad \hat \lambda_y = \bar Y    
      \]
      \[
      \max \ell_{UR} = - \hat \lambda_x n_x + \sum X_i \ln \hat\lambda_x + \sum Y_i \ln \hat\lambda_y - \sum \ln X_i! - \sum \ln Y_i!    
      \]
  
      \item 
      \[
      LR = 2(\max \ell_{UR} - \max \ell_R) = 2 \sum X_i (\ln \hat \lambda_x - \ln \hat\lambda) + 2 \sum Y_i (\ln \hat \lambda_y - \ln \hat\lambda)
      \]
      \item Unrestricted model has two parameters, restricted model has one parameter, 
      hence we use chi-squared disribution with $2 - 1 = 1$ degree of freedom, $LR_{\crit} = 3.84$.
      We calculate estimates, $\hat \lambda_x = 5$, $\hat\lambda_y = 4.5$, $\hat\lambda = 14/3$.
      
      \[
      LR = 1000 (\ln 5 - \ln (14/3)) + 1800 (\ln 4.5 - \ln (14/3)) \approx 3.5
      \]
      We do not reject $H_0$.  
  \end{enumerate}
  \end{sol}
\end{problem}

\begin{problem}
  You observe $X_1$, \ldots, $X_{400}$ and $Y_1$, \ldots, $Y_{400}$, $\bar X = 5$, $\bar Y = 6$. 
  All variables are independent. 
  
  Consider the null hypothesis that all random variables are exponentially distributed with common parameter $\lambda$ against alternative
  that parameter is $\lambda_X$ for every $X_i$ and $\lambda_Y$ for every $Y_j$. 
  
  \begin{enumerate}
    \item Estimate common $\lambda$ using maximum likelihood for the restricted model. 
    \item Estimate both $\lambda_X$ and $\lambda_Y$ using maximum likelihood in the unrestricted model. 
    \item Use LR-test to test the null hyphotesis at 5\% significance level. 
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  Random variables $X_1$, $X_2$, \ldots,  $X_n$ are independent identically distributed with density 
  \[
  f(x_i \mid \lambda) = \frac{\lambda}{2} \exp(-\lambda \abs{x_i}).    
  \]

  For $n=100$ I have 40 negative values with sum equal to $-300$ and 60 positive values with sum equal to $500$. 

  \begin{enumerate}
      \item Test the hypothesis $\lambda = 1$ using LR approach at significance level $\alpha=0.01$.
      \item Test the hypothesis $\lambda = 1$ using LM approach at significance level $\alpha=0.01$.
  \end{enumerate}

  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}



\section{Properties of estimators}


\begin{problem}
  The variables $X_1$, \ldots, $X_n$ are independent and uniformly distributed $\dUnif[0;a]$ with $a>1$.
  We do not observe $X_i$ directly but we know whether each $X_i$ is larger than 1. 
  Hence we observe the indicators $Y_i = I(X_i > 1)$.

  Consider the estimator $\hat a = 1 / (1 - \bar Y)$.

  \begin{enumerate}
    \item {[5]} Is $\hat a$ consistent?
    \item {[5]} Is $\hat a$ unbiased for $n=2$?
  \end{enumerate}
  \begin{sol}
$\E(Y_i) = \P(X_i > 1) = (a-1) / a = p$.
\begin{enumerate}
    \item The estimator is consistent as
\[
\plim \hat a = \frac {1}{1- \plim \bar Y } = \frac{1}{1 - \frac{a-1}{a}} = a
\]
\item 
For $n=2$ we have the positive probability $p^2$ that $\bar Y = 1$.
Hence with positive probability $\hat a$ is not defined.
The value $\E(\hat a)$ does not exist for $n=2$.
\end{enumerate}
  \end{sol}
\end{problem}


\begin{problem}
  You observe time between taxi arrivals on a stop, $Y_1$, $Y_2$, \ldots, $Y_n$.
  Assume that $Y_i$ are independent and exponentially distributed with $\E(Y_i) = \theta$, 
  that means the density of each $Y_i$ is $f(y) = \exp(-y/\theta)/\theta$ for $y\geq 0$. 
  Consider the following estimator of expected value
  \[
  \hat \theta = n \cdot \min\{Y_1, Y_2, \dots, Y_n \}    
  \]
  \begin{enumerate}
      \item {[6]} Find the probability density function of $\hat \theta$. 
      \item {[2]} Is $\hat \theta$ unbiased?
      \item {[2]} Is $\hat \theta$ consistent?
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item 
      \[
      \P(\hat \theta > y) = \P(Y_1 > y/n)^n = \left( \exp(-y/n\theta) \right)^n = \exp(-y/\theta)
      \]
      Hence $\hat\theta$ has exponential distribution with rate $1/\theta$ and 
      probability density function
      \[
      f(t) = \begin{cases}
          \exp(-t/\theta)/\theta, \text{ if } t\geq 0, \\
          0, \text{ otherwise}.
      \end{cases}
      \]
      \item The estimator is unbiased as
      \[
      \E(\hat\theta) = 1/(1/\theta) = \theta.    
      \]
      \item The estimator is non consistent as its distribution does not depend on $n$.
  \end{enumerate}  
  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\section{Fisher information and Cramer~— Rao}

\begin{problem}
  The variables $X_1$, \ldots, $X_n$ are independent and have Poisson distribution with intensity rate $\lambda$.
  In other words the probability mass function is given by $\P(X_i = k) = \exp(-\lambda) \lambda^k / k!$.
  
  \begin{enumerate}
    \item {[5]} Find theoretical Fisher information for $\lambda$ contained in the sample. 
    \item {[2]} Derive the maximum likelihood estimator for $\lambda$.
    \item {[3]} Does the maximum likelihood estimator attain the Cramer-Rao lower bound for variance? 
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item The log-likelihood function is equal to
      \[
      \ell(\lambda) = \sum_{i=1}^n \left( -\lambda + X_i \ln \lambda - \ln (X_i!)\right)    
      \]
      The score function is 
      \[
      \score(\lambda) = \ell'(\lambda) = \sum_{i=1}^n \left( -1 + X_i / \lambda\right).
      \]
      And
      \[
      \ell''(\lambda) = \sum_{i=1}^n \left(- X_i / \lambda^2 \right).
      \]
      Fisher information is 
      \[
      I_F = -\E(\ell''(\lambda)) = \sum \E(X_i) / \lambda^2 = n\lambda / \lambda^2 = n/\lambda.    
      \]
      
      \item Solving $\ell' = 0$ we obtain 
      \[
      \hat \lambda = \bar X    
      \] 
      
      \item Rewrite $\ell'(\lambda)$ using $\hat\lambda$. 
      Be careful! Do not confound $\lambda$ and $\hat\lambda$. 
      \[
      \score(\lambda) = \ell'(\lambda) = -n + n \hat \lambda / \lambda.
      \]
      Hence the score function is linear function of $\hat\lambda$, $\Corr(\score(\lambda), \hat\lambda) = 1$
      and the Cramer-Rao bound is attained. 
      
      One may also  find $\E \hat \lambda = \lambda$, $\Var(\hat \lambda) = \lambda / n$ and
      explicitly check that the general bound
      \[
      \Var(\hat \lambda) \geq 1/ I_F 
      \]
      is attained as equality in our case
      \[
          \lambda / n = 1/(n/\lambda). 
      \]
      \end{enumerate}      
  \end{sol}
\end{problem}


\begin{problem}
  Consider an estimator $\hat a$ with $\E(\hat a) = 0.5a + 3$. For the given sample size the Fisher information is $I_F(a) = 400/a^2$.
  \begin{enumerate}
    \item What is the theoretical minimal variance of $\hat a$?
    \item Assume that $\hat a$ attains the minimal variance boundary and is asymptotically normal. Given that $\hat a = 2022$ provide 95\% CI for $a$.
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  Consider iid sample from bivariate normal distribution, 
  $
  \begin{pmatrix}
    X_i \\
    Y_i \\
  \end{pmatrix}	 \sim \cN \left(     
  \begin{pmatrix}
    \theta \\
    2\theta \\
  \end{pmatrix}; 
  \begin{pmatrix}
    4 & 1 \\
    1 & 9 \\
  \end{pmatrix}
  \right).
  $
  
  Calculate Fischer information for the following cases: 
  \begin{enumerate}
    \item You observe $X_1$ only. 
    \item You observe $X_1$, \ldots, $X_n$.
    \item You observe $X_1$, \ldots, $X_n$, $Y_1$, \ldots, $Y_n$.
  \end{enumerate}
  
  Hint: the multivariate normal density is 
  $
  f(u) = \frac{1}{\sqrt{\det(2\pi \Sigma)}} \exp( -\frac{1}{2}(u-\mu)^T \Sigma^{-1}(u-\mu)).
  $
  
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Random variables $X_1$, \ldots, $X_n$ are independent with density 
  $
  f(x) = \begin{cases}
    -\ln(a) \cdot a^x, \text{ if } x\geq 0, \\
    0, \text{ otherwise.}
  \end{cases}	
  $
  \begin{enumerate}
    \item Estimate $a$ using maximum likelihood. 
    \item Check whether the estimator is unbiased and consistent. 
    \item Check whether the corresponding Cramer-Rao lower bound is attained. 
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  Random variables $X_1$, \ldots, $X_n$ are independent and normally distributed $\cN(1, 1/b)$. 
    
  \begin{enumerate}
      \item Estimate $b$ using maximum likelihood.
      \item Does the estimator achive the Cramer-Rao lower bound?
      \item Is the estimator consistent?
      \item Is the estimator unbiased?
  \end{enumerate}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}



\section{Sufficiency}

\begin{problem}
  The variables $X_1$, \ldots, $X_n$ are independent and gamma distributed with density
  \[
  f(x) = \begin{cases}
    \lambda^\alpha x^{\alpha - 1} \exp(-\lambda x) / \Gamma(\alpha), \text{ if } x\geq 0 \\
    0, \text{ otherwise}.
  \end{cases}  
  \]
  \begin{enumerate}
    \item {[5]} Find a sufficient statistic for $\alpha$ if we know that $\lambda = 1$. 
    \item {[5]} Find a two dimensional sufficient statistic for unknown $\alpha$ and $\lambda$. 
  \end{enumerate}
  \begin{sol}
    We do not need the formula for $\Gamma(\alpha)$ here. 
    \begin{enumerate}
        \item For known $\lambda = 1$ the likelihood is 
        \[
        L = \left(\prod X_i \right)^{\alpha - 1} \frac{1}{\Gamma(\alpha)} \cdot \exp( - \sum X_i).
        \]
        If we optimize this function for $\alpha$ the optimal $\hat\alpha$ will depend only on $\prod X_i$.
        Hence $\prod X_i$ is a sufficient statistic for $\alpha$. 
        There are many other sufficient statistics, $\sum \ln X_i$ is another example. 
        \item Now the likelihood is 
        \[
        L = \left(\prod X_i \right)^{\alpha - 1} \frac{1}{\Gamma(\alpha)} \lambda^{\alpha }\exp( - \lambda \sum X_i).
        \]
        If we optimize this function for $\alpha$ and $\lambda$ the optimal point will depend only on $\prod X_i$ and $\sum X_i$.
        Hence $\begin{pmatrix}
            \prod X_i & \sum X_i 
        \end{pmatrix}$ is a two dimensional sufficient statistic for $(\alpha, \lambda)$.
        
    \end{enumerate}
  \end{sol}
\end{problem}

\begin{problem}
  The variables $X_1$, \ldots, $X_n$ are independent and uniformly distributed on $[0; 2a]$ for some positive $a$. 

  \begin{enumerate}
    \item Find any sufficient statistic for $a$. 
    \item How the answer will change if $X_i \sim U[-a; 2a]$?
  \end{enumerate}  
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}

\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}


\begin{problem}
  \begin{sol}

  \end{sol}
\end{problem}





\Closesolutionfile{solution_file}


% для гиперссылок на условия
% http://tex.stackexchange.com/questions/45415
\renewenvironment{solution}[1]{%
         % add some glue
         \vskip .5cm plus 2cm minus 0.1cm%
         {\bfseries \hyperlink{problem:#1}{#1.}}%
}%
{%
}%

\section{Solutions}
\input{all_solutions}


\printindex


\section{Sources of wisdom}

\nocite{buzun2015stochastic}

\nocite{buzun2015stochastic}

\printbibliography[heading=none]


\end{document}
